{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c9iIwPU-2eG"
   },
   "source": [
    "# Notebook to extract variables out of NHD database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ee env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moOhZ19C_Alv"
   },
   "source": [
    "Originally 2021.02.23_ExportImagestoGCS_MR1_Copy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f36_nSgAAOP"
   },
   "source": [
    "# Authentications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "SOhZOzOln96G"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import ee\n",
    "import pickle\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 500 # 100 or 1250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggwS5nwCFiHw"
   },
   "source": [
    "# Set up bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "JSFWBtG4Fv9X"
   },
   "outputs": [],
   "source": [
    "def square(lat, lon, size):\n",
    "  crs_proj = \"EPSG:4326\"  \n",
    "  return ee.Geometry.Point([lon, lat], proj=crs_proj).buffer(size).bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "\n",
    "no_data_states = [\"LA\", \"MD\", \"SC\"]\n",
    "\n",
    "states = list(set(full_states) - set(no_data_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NWI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = ee.FeatureCollection([None])\n",
    "\n",
    "state_dict = {\"AK\" : [\"North\", \"Central\", \"South\"],\n",
    "             \"CA\" : [\"North\", \"NorthCentral\", \"South\", \"SouthCentral\"],\n",
    "             \"WI\" : [\"North\", \"South\"],\n",
    "             \"CO\" : [\"East\", \"West\"],\n",
    "             \"IA\" : [\"East\", \"West\"],\n",
    "             \"KS\" : [\"East\", \"West\"],\n",
    "             \"MN\" : [\"Central_West\", \"East\", \"North_Central\", \"North_East\", \"North_West\", \"South\"],\n",
    "             \"MT\" : [\"East\", \"West\"],\n",
    "             \"ND\" : [\"East\", \"West\"],\n",
    "             \"NE\" : [\"East\", \"West\"],\n",
    "             \"NV\" : [\"North\", \"South\"],\n",
    "             \"OK\" : [\"East\", \"West\"],\n",
    "             \"OR\" : [\"East\", \"West\"],\n",
    "             \"SD\" : [\"East\", \"West\"],\n",
    "             \"TX\" : [\"East\", \"West\", \"Central\"],\n",
    "             \"WI\" : [\"North\", \"South\"],\n",
    "             \"WY\" : [\"East\", \"West\"]}\n",
    "\n",
    "for state in states:\n",
    "    directions_list = state_dict.get(state)\n",
    "    if directions_list == None:\n",
    "        merged = merged.merge(ee.FeatureCollection(\"users/madhukarreddy/NWI_\" + state +\"_Wetlands\"))\n",
    "    else:\n",
    "        for direction in directions_list:\n",
    "            merged = (merged.merge(ee.FeatureCollection(\"users/madhukarreddy/NWI_\" + \n",
    "                                                        state +  \n",
    "                                                        \"_Wetlands\" + \"_\" + \n",
    "                                                        direction )))\n",
    "fc = merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_getInfo = fc.filterBounds(square(lat=45.475649, lon=-69.471018 , size=PATCH_SIZE)).getInfo()\n",
    "# fc_getInfo = fc_getInfo.get('features')\n",
    "# num_of_features = len(fc_getInfo)\n",
    "# print(\"number of features =\", num_of_features)\n",
    "\n",
    "# attr_list = []\n",
    "# wetland_list = []\n",
    "# for feature in range(num_of_features):\n",
    "#     # some features are described as \"PSS3/EM1E\"\n",
    "#     # split them out so each attritube can be joined to the wetland defns csv\n",
    "#     attr_element = fc_getInfo[feature].get('properties').get('ATTRIBUTE').split(\"/\")\n",
    "#     [attr_list.append(element) for element in attr_element]\n",
    "# #     attr_list.append(fc_getInfo[feature].get('properties').get('ATTRIBUTE'))\n",
    "\n",
    "#     wetland_element = fc_getInfo[feature].get('properties').get('WETLAND_TY').split(\"/\")\n",
    "#     [wetland_list.append(element) for element in wetland_element]\n",
    "# #     wetland_list.append(fc_getInfo[feature].get('properties').get('WETLAND_TY'))\n",
    "\n",
    "\n",
    "# print(attr_list)\n",
    "# print(wetland_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features = 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7,\n",
       " ['R5UBH', 'R5UBH', 'PUBH', 'PEM1E', 'PFO4E', 'PFO4E', 'PSS3', 'EM1E'],\n",
       " ['Riverine',\n",
       "  'Riverine',\n",
       "  'Freshwater Pond',\n",
       "  'Freshwater Emergent Wetland',\n",
       "  'Freshwater Forested',\n",
       "  'Shrub Wetland',\n",
       "  'Freshwater Forested',\n",
       "  'Shrub Wetland',\n",
       "  'Freshwater Forested',\n",
       "  'Shrub Wetland'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nwi_bbox_features(lat=45.475649, lon=-69.471018 , size=PATCH_SIZE):\n",
    "    \"\"\"\n",
    "    input: lat, lon, and size=half of bbox side\n",
    "    output:\n",
    "        - num_of_features: number of NWI features intersected by bbox\n",
    "        - attr_list: list of NWI attributes for wetland types\n",
    "        - wetland_list: WETLAND_TY for each ATTRIBUTE\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fc_getInfo = fc.filterBounds(square(lat, lon, size)).getInfo()\n",
    "        fc_getInfo = fc_getInfo.get('features')\n",
    "        num_of_features = len(fc_getInfo)\n",
    "        print(\"number of features =\", num_of_features)\n",
    "\n",
    "        attr_list = []\n",
    "        wetland_list = []\n",
    "        for feature in range(num_of_features):\n",
    "\n",
    "            # some features are described as \"PSS3/EM1E\"\n",
    "            # split them out so each attritube can be joined to the wetland defns csv\n",
    "            attr_element = fc_getInfo[feature].get('properties').get('ATTRIBUTE').split(\"/\")\n",
    "            [attr_list.append(element) for element in attr_element]\n",
    "\n",
    "            wetland_element = fc_getInfo[feature].get('properties').get('WETLAND_TY').split(\"/\")\n",
    "            [wetland_list.append(element) for element in wetland_element]\n",
    "        return num_of_features, attr_list, wetland_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Issue with at lat={}, lon={}\".format(lat, lon))\n",
    "        return np.nan\n",
    "    \n",
    "\n",
    "nwi_bbox_features()                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "2bcRIItG4bu7"
   },
   "outputs": [],
   "source": [
    "# read in csv file with SSURGO variables\n",
    "# df_m = pd.read_csv(\"combined_regular_clean_with_ssurgo_variables.csv\")\n",
    "df_m = pd.read_pickle(\"cwr_nwpr_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nwi_bbox_variables(df_m_, i, size):\n",
    "\n",
    "  df_m_[\"nwi_bbox_vars_\" + str(2 * PATCH_SIZE) + \"m\"] = (df_m_.apply(lambda x: \n",
    "                                                     nwi_bbox_features(lat=x.latitude, \n",
    "                                                                       lon=x.longitude, \n",
    "                                                                       size=size), axis=1))\n",
    "\n",
    "\n",
    "#   pickle.dump(df_m_, open(\"NWI_extracted_vars_200mX200m/200mX200m_nwi_variables_part\" + str(i),\"wb\"), protocol=3)        \n",
    "  (pickle.dump(df_m_, open(\"NWI_extracted_vars_cwr_nwpr_\" + str(2 * PATCH_SIZE) + \"m/nwi_extraction_\" \n",
    "                           + str(2 * PATCH_SIZE) + \"X\" + str(2 * PATCH_SIZE) + \"m\"\n",
    "                           + \"_part\" + str(i),\"wb\"), \n",
    "               protocol=3))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-02 21:44:37.920197\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_nwi_bbox_variables(batch_df, (START + batch_size * batch) + 1, size = PATCH_SIZE)\n",
    "\n",
    "# df_m[3151:3219].apply(lambda x: nwi_bbox_features(lat=x.latitude, lon=x.longitude, size=PATCH_SIZE), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJ7imBRFtzdI",
    "outputId": "17f4a2d4-a6bd-41cf-efed-d43001734eec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 64 of 10 started\n",
      "number of features = 8\n",
      "number of features = 2\n",
      "number of features = 28\n",
      "number of features = 8\n",
      "number of features = 40\n",
      "number of features = 40\n",
      "number of features = 44\n",
      "number of features = 29\n",
      "number of features = 21\n",
      "number of features = 7\n",
      "number of features = 27\n",
      "number of features = 16\n",
      "number of features = 36\n",
      "number of features = 6\n",
      "number of features = 25\n",
      "number of features = 10\n",
      "number of features = 7\n",
      "number of features = 12\n",
      "number of features = 9\n",
      "number of features = 8\n"
     ]
    }
   ],
   "source": [
    "# pass in batches of 500\n",
    "# MADHUKAR: 0 - 5000 \n",
    "# SHOBHA: 5000 - 10000\n",
    "# RADHIKA: 10000 - 15000\n",
    "\n",
    "batch_size = 50\n",
    "MY_NAME = \"MADHUKAR\"\n",
    "START = 0 + 5000 * (MY_NAME == \"SHOBHA\") + 10000 * (MY_NAME == \"RADHIKA\")\n",
    "batch = 65\n",
    "# for batch in range(10, df_m.shape[0]//50):\n",
    "for batch in [63]:\n",
    "  print(\"batch {} of 10 started\".format(batch + 1))\n",
    "  batch_df = df_m[START + batch_size * batch : START + batch_size * (batch + 1)].copy()\n",
    "  extract_nwi_bbox_variables(batch_df, (START + batch_size * batch) + 1, size = PATCH_SIZE)\n",
    "  print(\"batch {} of 10 done\".format(batch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pass in batches of 500\n",
    "# # MADHUKAR: 0 - 5000 \n",
    "# # SHOBHA: 5000 - 10000\n",
    "# # RADHIKA: 10000 - 15000\n",
    "\n",
    "# batch_size = 500\n",
    "# MY_NAME = \"SHOBHA\"\n",
    "# START = 0 + 5000 * (MY_NAME == \"SHOBHA\") + 10000 * (MY_NAME == \"RADHIKA\")\n",
    "\n",
    "# for batch in range(10):\n",
    "#   print(\"batch {} of 10 started\".format(batch + 1))\n",
    "#   batch_df = df_m[START + batch_size * batch : START + batch_size * (batch + 1)].copy()\n",
    "#   extract_nwi_bbox_variables(batch_df, (START + batch_size * batch) + 1, size = PATCH_SIZE)\n",
    "#   print(\"batch {} of 10 done\".format(batch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pass in batches of 500\n",
    "# # MADHUKAR: 0 - 5000 \n",
    "# # SHOBHA: 5000 - 10000\n",
    "# # RADHIKA: 10000 - 15000\n",
    "\n",
    "# batch_size = 500\n",
    "# MY_NAME = \"RADHIKA\"\n",
    "# START = 0 + 5000 * (MY_NAME == \"SHOBHA\") + 10000 * (MY_NAME == \"RADHIKA\")\n",
    "\n",
    "# for batch in range(10):\n",
    "#   print(\"batch {} of 10 started\".format(batch + 1))\n",
    "#   batch_df = df_m[START + batch_size * batch : START + batch_size * (batch + 1)].copy()\n",
    "#   extract_nwi_bbox_variables(batch_df, (START + batch_size * batch) + 1, size = PATCH_SIZE)\n",
    "#   print(\"batch {} of 10 done\".format(batch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "Wyr-kwm5PDnq",
    "outputId": "26ad02d0-dbbe-468b-c923-d5837819a20f"
   },
   "outputs": [],
   "source": [
    "#read the file\n",
    "# df_m_test = pd.read_csv('NHD_extracted_vars/combined_regular_clean_with_ssurgo_nhd_variables_part1.csv')\n",
    "df_m_test = pd.read_pickle(\"NWI_extracted_vars_cwr_nwpr_\" + str(2 * PATCH_SIZE) + \"m/nwi_extraction_\" \n",
    "                           + str(2 * PATCH_SIZE) + \"X\" + str(2 * PATCH_SIZE) + \"m\"\n",
    "                           + \"_part1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m_test[\"nwi_bbox_vars_\" + str(2 * PATCH_SIZE) + \"m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2021.02.27_Extract_NHD_Variables_Copy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
