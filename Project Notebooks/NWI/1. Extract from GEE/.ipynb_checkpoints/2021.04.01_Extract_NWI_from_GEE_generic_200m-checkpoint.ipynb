{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c9iIwPU-2eG"
   },
   "source": [
    "# Notebook to extract variables out of NHD database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ee env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moOhZ19C_Alv"
   },
   "source": [
    "Originally 2021.02.23_ExportImagestoGCS_MR1_Copy.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f36_nSgAAOP"
   },
   "source": [
    "# Authentications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "SOhZOzOln96G"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import ee\n",
    "import pickle\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 100 # 100 or 1250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggwS5nwCFiHw"
   },
   "source": [
    "# Set up bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "JSFWBtG4Fv9X"
   },
   "outputs": [],
   "source": [
    "def square(lat, lon, size):\n",
    "  crs_proj = \"EPSG:4326\"  \n",
    "  return ee.Geometry.Point([lon, lat], proj=crs_proj).buffer(size).bounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "\n",
    "no_data_states = [\"LA\", \"MD\", \"SC\"]\n",
    "\n",
    "states = list(set(full_states) - set(no_data_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NWI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = ee.FeatureCollection([None])\n",
    "\n",
    "state_dict = {\"AK\" : [\"North\", \"Central\", \"South\"],\n",
    "             \"CA\" : [\"North\", \"NorthCentral\", \"South\", \"SouthCentral\"],\n",
    "             \"WI\" : [\"North\", \"South\"],\n",
    "             \"CO\" : [\"East\", \"West\"],\n",
    "             \"IA\" : [\"East\", \"West\"],\n",
    "             \"KS\" : [\"East\", \"West\"],\n",
    "             \"MN\" : [\"Central_West\", \"East\", \"North_Central\", \"North_East\", \"North_West\", \"South\"],\n",
    "             \"MT\" : [\"East\", \"West\"],\n",
    "             \"ND\" : [\"East\", \"West\"],\n",
    "             \"NE\" : [\"East\", \"West\"],\n",
    "             \"NV\" : [\"North\", \"South\"],\n",
    "             \"OK\" : [\"East\", \"West\"],\n",
    "             \"OR\" : [\"East\", \"West\"],\n",
    "             \"SD\" : [\"East\", \"West\"],\n",
    "             \"TX\" : [\"East\", \"West\", \"Central\"],\n",
    "             \"WI\" : [\"North\", \"South\"],\n",
    "             \"WY\" : [\"East\", \"West\"]}\n",
    "\n",
    "for state in states:\n",
    "    directions_list = state_dict.get(state)\n",
    "    if directions_list == None:\n",
    "        merged = merged.merge(ee.FeatureCollection(\"users/madhukarreddy/NWI_\" + state +\"_Wetlands\"))\n",
    "    else:\n",
    "        for direction in directions_list:\n",
    "            merged = (merged.merge(ee.FeatureCollection(\"users/madhukarreddy/NWI_\" + \n",
    "                                                        state +  \n",
    "                                                        \"_Wetlands\" + \"_\" + \n",
    "                                                        direction )))\n",
    "fc = merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc.getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc_getInfo = fc.filterBounds(square(lat=45.475649, lon=-69.471018 , size=PATCH_SIZE)).getInfo()\n",
    "# fc_getInfo = fc_getInfo.get('features')\n",
    "# num_of_features = len(fc_getInfo)\n",
    "# print(\"number of features =\", num_of_features)\n",
    "\n",
    "# attr_list = []\n",
    "# wetland_list = []\n",
    "# for feature in range(num_of_features):\n",
    "#     # some features are described as \"PSS3/EM1E\"\n",
    "#     # split them out so each attritube can be joined to the wetland defns csv\n",
    "#     attr_element = fc_getInfo[feature].get('properties').get('ATTRIBUTE').split(\"/\")\n",
    "#     [attr_list.append(element) for element in attr_element]\n",
    "# #     attr_list.append(fc_getInfo[feature].get('properties').get('ATTRIBUTE'))\n",
    "\n",
    "#     wetland_element = fc_getInfo[feature].get('properties').get('WETLAND_TY').split(\"/\")\n",
    "#     [wetland_list.append(element) for element in wetland_element]\n",
    "# #     wetland_list.append(fc_getInfo[feature].get('properties').get('WETLAND_TY'))\n",
    "\n",
    "\n",
    "# print(attr_list)\n",
    "# print(wetland_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,\n",
       " ['R5UBH', 'PUBH', 'PEM1E', 'PSS3', 'EM1E'],\n",
       " ['Riverine',\n",
       "  'Freshwater Pond',\n",
       "  'Freshwater Emergent Wetland',\n",
       "  'Freshwater Forested',\n",
       "  'Shrub Wetland'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nwi_bbox_features(lat=45.475649, lon=-69.471018 , size=PATCH_SIZE):\n",
    "    \"\"\"\n",
    "    input: lat, lon, and size=half of bbox side\n",
    "    output:\n",
    "        - num_of_features: number of NWI features intersected by bbox\n",
    "        - attr_list: list of NWI attributes for wetland types\n",
    "        - wetland_list: WETLAND_TY for each ATTRIBUTE\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fc_getInfo = fc.filterBounds(square(lat, lon, size)).getInfo()\n",
    "        fc_getInfo = fc_getInfo.get('features')\n",
    "        num_of_features = len(fc_getInfo)\n",
    "        print(\"number of features =\", num_of_features)\n",
    "\n",
    "        attr_list = []\n",
    "        wetland_list = []\n",
    "        for feature in range(num_of_features):\n",
    "\n",
    "            # some features are described as \"PSS3/EM1E\"\n",
    "            # split them out so each attritube can be joined to the wetland defns csv\n",
    "            attr_element = fc_getInfo[feature].get('properties').get('ATTRIBUTE').split(\"/\")\n",
    "            [attr_list.append(element) for element in attr_element]\n",
    "\n",
    "            wetland_element = fc_getInfo[feature].get('properties').get('WETLAND_TY').split(\"/\")\n",
    "            [wetland_list.append(element) for element in wetland_element]\n",
    "        return num_of_features, attr_list, wetland_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Issue with at lat={}, lon={}\".format(lat, lon))\n",
    "        return np.nan\n",
    "    \n",
    "\n",
    "nwi_bbox_features()                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2bcRIItG4bu7"
   },
   "outputs": [],
   "source": [
    "# read in csv file with SSURGO variables\n",
    "# df_m = pd.read_csv(\"combined_regular_clean_with_ssurgo_variables.csv\")\n",
    "df_m = pd.read_pickle(\"cwr_nwpr_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nwi_bbox_variables(df_m_, i, size):\n",
    "\n",
    "  df_m_[\"nwi_bbox_vars_\" + str(2 * PATCH_SIZE) + \"m\"] = (df_m_.apply(lambda x: \n",
    "                                                     nwi_bbox_features(lat=x.latitude, \n",
    "                                                                       lon=x.longitude, \n",
    "                                                                       size=size), axis=1))\n",
    "\n",
    "\n",
    "#   pickle.dump(df_m_, open(\"NWI_extracted_vars_200mX200m/200mX200m_nwi_variables_part\" + str(i),\"wb\"), protocol=3)        \n",
    "  (pickle.dump(df_m_, open(\"NWI_extracted_vars_cwr_nwpr_\" + str(2 * PATCH_SIZE) + \"m/nwi_extraction_\" \n",
    "                           + str(2 * PATCH_SIZE) + \"X\" + str(2 * PATCH_SIZE) + \"m\"\n",
    "                           + \"_part\" + str(i),\"wb\"), \n",
    "               protocol=3))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-02 22:14:30.272683\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m.shape[0]//50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OJ7imBRFtzdI",
    "outputId": "17f4a2d4-a6bd-41cf-efed-d43001734eec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 65 of 10 started\n",
      "number of features = 2\n",
      "number of features = 3\n",
      "number of features = 0\n",
      "number of features = 3\n",
      "number of features = 0\n",
      "number of features = 1\n",
      "number of features = 2\n",
      "number of features = 2\n",
      "number of features = 1\n",
      "number of features = 0\n",
      "number of features = 0\n",
      "number of features = 2\n",
      "number of features = 1\n",
      "number of features = 2\n",
      "number of features = 2\n",
      "number of features = 1\n",
      "number of features = 4\n",
      "number of features = 3\n",
      "batch 65 of 10 done\n"
     ]
    }
   ],
   "source": [
    "# pass in batches of 500\n",
    "# MADHUKAR: 0 - 5000 \n",
    "# SHOBHA: 5000 - 10000\n",
    "# RADHIKA: 10000 - 15000\n",
    "\n",
    "batch_size = 50\n",
    "MY_NAME = \"MADHUKAR\"\n",
    "START = 0 + 5000 * (MY_NAME == \"SHOBHA\") + 10000 * (MY_NAME == \"RADHIKA\")\n",
    "\n",
    "# for batch in range(10, df_m.shape[0]//50):\n",
    "for batch in [64]:\n",
    "  print(\"batch {} of 10 started\".format(batch + 1))\n",
    "  batch_df = df_m[START + batch_size * batch : START + batch_size * (batch + 1)].copy()\n",
    "  extract_nwi_bbox_variables(batch_df, (START + batch_size * batch) + 1, size = PATCH_SIZE)\n",
    "  print(\"batch {} of 10 done\".format(batch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pass in batches of 500\n",
    "# # MADHUKAR: 0 - 5000 \n",
    "# # SHOBHA: 5000 - 10000\n",
    "# # RADHIKA: 10000 - 15000\n",
    "\n",
    "# batch_size = 500\n",
    "# MY_NAME = \"SHOBHA\"\n",
    "# START = 0 + 5000 * (MY_NAME == \"SHOBHA\") + 10000 * (MY_NAME == \"RADHIKA\")\n",
    "\n",
    "# for batch in range(10):\n",
    "#   print(\"batch {} of 10 started\".format(batch + 1))\n",
    "#   batch_df = df_m[START + batch_size * batch : START + batch_size * (batch + 1)].copy()\n",
    "#   extract_nwi_bbox_variables(batch_df, (START + batch_size * batch) + 1, size = PATCH_SIZE)\n",
    "#   print(\"batch {} of 10 done\".format(batch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pass in batches of 500\n",
    "# # MADHUKAR: 0 - 5000 \n",
    "# # SHOBHA: 5000 - 10000\n",
    "# # RADHIKA: 10000 - 15000\n",
    "\n",
    "# batch_size = 500\n",
    "# MY_NAME = \"RADHIKA\"\n",
    "# START = 0 + 5000 * (MY_NAME == \"SHOBHA\") + 10000 * (MY_NAME == \"RADHIKA\")\n",
    "\n",
    "# for batch in range(10):\n",
    "#   print(\"batch {} of 10 started\".format(batch + 1))\n",
    "#   batch_df = df_m[START + batch_size * batch : START + batch_size * (batch + 1)].copy()\n",
    "#   extract_nwi_bbox_variables(batch_df, (START + batch_size * batch) + 1, size = PATCH_SIZE)\n",
    "#   print(\"batch {} of 10 done\".format(batch + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-02 21:02:12.285928\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "Wyr-kwm5PDnq",
    "outputId": "26ad02d0-dbbe-468b-c923-d5837819a20f"
   },
   "outputs": [],
   "source": [
    "#read the file\n",
    "# df_m_test = pd.read_csv('NHD_extracted_vars/combined_regular_clean_with_ssurgo_nhd_variables_part1.csv')\n",
    "df_m_test = pd.read_pickle(\"NWI_extracted_vars_cwr_nwpr_\" + str(2 * PATCH_SIZE) + \"m/nwi_extraction_\" \n",
    "                           + str(2 * PATCH_SIZE) + \"X\" + str(2 * PATCH_SIZE) + \"m\"\n",
    "                           + \"_part1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14324                                          (0, [], [])\n",
       "14339                                          (0, [], [])\n",
       "14346    (16, [R1UBV, R1UBV, R3UBH, R5UBH, PEM1R, PEM1R...\n",
       "14348                                          (0, [], [])\n",
       "14378    (2, [PSS3Dg, PSS3Dg], [Freshwater Forested, Sh...\n",
       "14395    (2, [PUBHx, PUBHx], [Freshwater Pond, Freshwat...\n",
       "14540                             (1, [R4SBC], [Riverine])\n",
       "14604                                          (0, [], [])\n",
       "14609                                          (0, [], [])\n",
       "14659    (4, [R4SBC, R5UBH, PUBH, PEM1Ad], [Riverine, R...\n",
       "14668                                          (0, [], [])\n",
       "14680    (10, [PUB, ABFx, PUB, ABFx, PUB, ABFx, PUB, AB...\n",
       "14691         (1, [PEM1Cd], [Freshwater Emergent Wetland])\n",
       "14693                                          (0, [], [])\n",
       "14696    (3, [R5UBFx, PABHx, PEM1A], [Riverine, Freshwa...\n",
       "14723                                          (0, [], [])\n",
       "14726                                          (0, [], [])\n",
       "14727    (4, [R4SBC, PUBHx, PUBHx, PEM1Ad], [Riverine, ...\n",
       "14732                             (1, [R4SBC], [Riverine])\n",
       "14733          (1, [PEM1F], [Freshwater Emergent Wetland])\n",
       "14745                             (1, [R4SBC], [Riverine])\n",
       "14746                                          (0, [], [])\n",
       "14747    (4, [R2UB3Hx, PUBHx, PUBHx, PFO1C], [Riverine,...\n",
       "14750                                          (0, [], [])\n",
       "14755                             (1, [R4SBC], [Riverine])\n",
       "14760    (3, [PEM1C, PEM1C, PEM1Fx], [Freshwater Emerge...\n",
       "14781                                          (0, [], [])\n",
       "14784    (3, [R2UBH, PEM1A, PFO1A], [Riverine, Freshwat...\n",
       "14787                             (1, [R4SBC], [Riverine])\n",
       "14796    (5, [R4SBC, PEM1A, PEM1B, PEM1Cx, PFO1B], [Riv...\n",
       "14801    (4, [E2USM, E2EM1N, E2EM1Nd, E1UBL], [Estuarin...\n",
       "14802    (1, [PFO1C], [Freshwater Forested, Shrub Wetla...\n",
       "14803                                          (0, [], [])\n",
       "14810                                          (0, [], [])\n",
       "14818                                          (0, [], [])\n",
       "14820                       (1, [PUBH], [Freshwater Pond])\n",
       "14827    (2, [R5UBH, PEM1C], [Riverine, Freshwater Emer...\n",
       "14830                            (1, [R2UBHx], [Riverine])\n",
       "14832                                          (0, [], [])\n",
       "14835                             (1, [R4SBC], [Riverine])\n",
       "14837    (5, [R2UBGx, PEM1C, PEM1F, PEM1Ad, PEM1Ad], [R...\n",
       "14871                      (1, [PUBHh], [Freshwater Pond])\n",
       "14874                             (1, [R2UBH], [Riverine])\n",
       "14878    (4, [PUBHx, PEM1A, PEM1C, PEM1Af], [Freshwater...\n",
       "14894                                          (0, [], [])\n",
       "14895    (2, [PUBF, PUBF], [Freshwater Pond, Freshwater...\n",
       "14902    (6, [R4SBC, R5UBH, R5UBH, PUBG, PEM1C, PFO1A],...\n",
       "14906                      (1, [PUBGx], [Freshwater Pond])\n",
       "14977                                          (0, [], [])\n",
       "14993          (2, [R5UBFx, R5UBFx], [Riverine, Riverine])\n",
       "Name: nwi_bbox_vars_200m, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m_test[\"nwi_bbox_vars_\" + str(2 * PATCH_SIZE) + \"m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['jurisdiction_type', 'da_number', 'latitude', 'longitude',\n",
       "       'cwa_determination', 'Index', 'nwi_bbox_vars_200m'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2021.02.27_Extract_NHD_Variables_Copy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
