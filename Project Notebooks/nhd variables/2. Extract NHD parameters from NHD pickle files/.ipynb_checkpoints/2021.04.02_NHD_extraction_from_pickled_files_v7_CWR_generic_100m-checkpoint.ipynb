{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "published-belgium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Madhukar/opt/miniconda3/envs/ee_skmr/bin/python\n",
      "3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 05:02:20) \n",
      "[Clang 11.0.1 ]\n",
      "sys.version_info(major=3, minor=9, micro=2, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "technical-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "crude-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "mental-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the nhd addendum file\n",
    "nhd_stats = pd.read_csv(\"nhd_stats_AI.csv\")\n",
    "\n",
    "# read in csv file with SSURGO variables\n",
    "# df_m = pd.read_csv(\"combined_regular_clean_with_ssurgo_variables.csv\")\n",
    "df_m = pd.read_pickle(\"cwr_nwpr_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "assisted-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle('../1. Extract NHD information from GEE/NHD_extracted_vars_2.5kmX2.5km_with_fcode_ftype/2.5kmX2.5km_nhd_variables_part1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "liable-poker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3218\n"
     ]
    }
   ],
   "source": [
    "# comid's from GEE are extracted into several pickled files\n",
    "# pickling is needed to be able to share easily across team members \n",
    "# join these pickled files into one full dataset\n",
    "\n",
    "df_merged_full = []\n",
    "count = 0\n",
    "for i in range(df_m.shape[0] // 50 + 1):\n",
    "    try:\n",
    "        df_temp = pd.read_pickle('../1. Extract NHD information from GEE/NHD_extracted_vars_cwr_nwpr_' + str(2 * PATCH_SIZE) + 'mX' + str(2 * PATCH_SIZE) + 'm_with_fcode_ftype/' + str(2 * PATCH_SIZE) + 'mX' + str(2 * PATCH_SIZE) + 'm_nhd_variables_part' + str(50 * i + 1))\n",
    "#         print(df_temp.columns)\n",
    "#         print('../1. Extract NHD information from GEE/NHD_extracted_vars_cwr_nwpr_' + str(2 * PATCH_SIZE) + 'mX' + str(2 * PATCH_SIZE) + 'm_with_fcode_ftype/' + str(2 * PATCH_SIZE) + 'mX' + str(2 * PATCH_SIZE) + 'm_nhd_variables_part' + str(50 * i + 1))\n",
    "        df_merged_full.append(df_temp)\n",
    "        count += df_temp.shape[0]\n",
    "    except:\n",
    "        break\n",
    "print(count)\n",
    "df_merged_full = pd.concat(df_merged_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "reflected-picture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../1. Extract NHD information from GEE/NHD_extracted_vars_cwr_nwpr_2500mX2500m_with_fcode_ftype/2500mX1000m_nhd_variables_part1'"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "'../1. Extract NHD information from GEE/NHD_extracted_vars_cwr_nwpr_' + str(2 * PATCH_SIZE) + 'mX' + str(2 * PATCH_SIZE) + 'm_with_fcode_ftype/' + str(2 * PATCH_SIZE) + 'mX' + '1000m_nhd_variables_part' + str(50 * i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "heard-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# pd.read_pickle('../1. Extract NHD information from GEE/NHD_extracted_vars_200mX200m_with_fcode_ftype/200mX200m_nhd_variables_part' + str(500 * i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "extraordinary-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert to protocol 3 so that can read in aws\n",
    "# df_merged_full = []\n",
    "\n",
    "# for i in range(df_m.shape[0] // 500 + 1):\n",
    "#     try:\n",
    "#         df_temp = pd.read_pickle(('NHD_extracted_vars_2.5kmX2.5km/combined_regular_clean_with_ssurgo_nhd_variables_part' + \n",
    "#                               str(500 * i + 1)))\n",
    "#         print(df_temp.shape)\n",
    "#         pickle.dump(df_temp, open(\"NHD_extracted_vars_2.5kmX2.5km/combined_regular_clean_with_ssurgo_nhd_variables_part\" + str(500 * i + 1) + \"_pkl3.pkl\",\"wb\"), protocol=3)\n",
    "#         df_temp2 = pd.read_pickle(('NHD_extracted_vars_2.5kmX2.5km/combined_regular_clean_with_ssurgo_nhd_variables_part' + \n",
    "#                               str(500 * i + 1)))\n",
    "\n",
    "#     except:\n",
    "#         break\n",
    "#     df_merged_full.append(df_temp2)\n",
    "# df_merged_full = pd.concat(df_merged_full)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "light-advocacy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3218, 8)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "infrared-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this df into pickle\n",
    "import pickle\n",
    "pickle.dump(df_merged_full, open('cwr_nwpr_' + str(2 * PATCH_SIZE) + 'm_nhd_variables_partsmerged',\"wb\"), protocol=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-theater",
   "metadata": {},
   "source": [
    "# Read in merged pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "considered-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_full = pd.read_pickle('cwr_nwpr_' + str(2 * PATCH_SIZE) + 'm_nhd_variables_partsmerged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "adult-revelation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3218, 8)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_full.columns\n",
    "df_merged_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-title",
   "metadata": {},
   "source": [
    "# A word on 'nhd_vars_wb' and 'nhd_vars_fl' columns (see last two columns above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-gamma",
   "metadata": {},
   "source": [
    "## nhd_vars_wb: this is a list of lists\n",
    "- for each record, the following six features from GEE are extracted as lists and stored into a list\n",
    "- [comid_list, ftype_str, gnis_id, wb_area, fl_length, fcode]\n",
    "- the column is labeled nhd_vars_wb for waterbodies\n",
    "\n",
    "In a similar fashion, there is another column for flowlines labeled nhd_vars_fl\n",
    "\n",
    "Note: fl_length is NaN's for waterbodies and wb_area ae NaN's for flowlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-investor",
   "metadata": {},
   "source": [
    "## In the following, features are extracted from the above columns and feature engineered as discussed in meetings. Pls use your judgement to help devise any new features you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "prescribed-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wb for waterbodies\n",
    "# extract the individual lists\n",
    "df_merged_full[\"wb_comid_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_wb[0], axis=1)\n",
    "df_merged_full[\"wb_ftype_str_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_wb[1], axis=1)\n",
    "df_merged_full[\"wb_gnis_id_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_wb[2], axis=1)\n",
    "df_merged_full[\"wb_area_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_wb[3], axis=1)\n",
    "\n",
    "# fl for flowlines\n",
    "# extract the individual lists\n",
    "df_merged_full[\"fl_comid_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_fl[0], axis=1)\n",
    "df_merged_full[\"fl_ftype_str_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_fl[1], axis=1)\n",
    "df_merged_full[\"fl_gnis_id_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_fl[2], axis=1)\n",
    "df_merged_full[\"fl_length_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_fl[4], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-continent",
   "metadata": {},
   "source": [
    "# Lets look at columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "bridal-tunnel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14659</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14668</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14680</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14691</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14693</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14696</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [14659, 14668, 14680, 14691, 14693, 14696]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_full[df_merged_full.columns[29:39]][9:15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-privilege",
   "metadata": {},
   "source": [
    "## Lets look at one row in individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "extra-gamma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14324                             ([], [], [], [], [], [])\n",
       "14339                             ([], [], [], [], [], [])\n",
       "14346                             ([], [], [], [], [], [])\n",
       "14348                             ([], [], [], [], [], [])\n",
       "14378    ([4833730, 4833766, 4833678, 22324621, 2232597...\n",
       "                               ...                        \n",
       "7258     ([14784241, 14784243, 14784245, 14784251], [La...\n",
       "7262     ([14783701], [SwampMarsh], [], [0.578], [nan],...\n",
       "7266     ([904140248], [LakePond], [1623080], [57516.64...\n",
       "7272                              ([], [], [], [], [], [])\n",
       "7273                              ([], [], [], [], [], [])\n",
       "Name: nhd_vars_wb, Length: 3218, dtype: object"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at waterbodies list\n",
    "df_merged_full.nhd_vars_wb#[9]\n",
    "# you can see there are 6 items in the list [comid_list, ftype_str, gnis_id, wb_area, fl_length, fcode]\n",
    "# note that fcode is going to be null due to coding lapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "peripheral-advancement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14324    ([20331722, 20324433, 20324437], [StreamRiver,...\n",
       "14339    ([22338541, 22338501, 22338455], [StreamRiver,...\n",
       "14346    ([22338859, 22340313, 22338847], [StreamRiver,...\n",
       "14348    ([14784645, 14785825, 14784643, 14785817, 1478...\n",
       "14378    ([4835432, 4836616, 4837010], [StreamRiver, Ar...\n",
       "                               ...                        \n",
       "7258     ([14783695, 14783697, 14784417, 14784415], [St...\n",
       "7262     ([14783655, 14783715, 14783667], [CanalDitch, ...\n",
       "7266     ([12169339, 12169341, 12169343, 12169095, 1216...\n",
       "7272     ([14768198, 14768180], [StreamRiver, StreamRiv...\n",
       "7273     ([4142418], [StreamRiver], [], [nan], [2.08], ...\n",
       "Name: nhd_vars_fl, Length: 3218, dtype: object"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at flowlines list\n",
    "df_merged_full.nhd_vars_fl#[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "activated-wallet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14324                                                   []\n",
       "14339                                                   []\n",
       "14346                                                   []\n",
       "14348                                                   []\n",
       "14378    [4833730, 4833766, 4833678, 22324621, 22325977...\n",
       "                               ...                        \n",
       "7258              [14784241, 14784243, 14784245, 14784251]\n",
       "7262                                            [14783701]\n",
       "7266                                           [904140248]\n",
       "7272                                                    []\n",
       "7273                                                    []\n",
       "Name: wb_comid_list, Length: 3218, dtype: object"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of comids in waterbodies\n",
    "df_merged_full.wb_comid_list#[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "altered-mandate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14324                       [20331722, 20324433, 20324437]\n",
       "14339                       [22338541, 22338501, 22338455]\n",
       "14346                       [22338859, 22340313, 22338847]\n",
       "14348    [14784645, 14785825, 14784643, 14785817, 14784...\n",
       "14378                          [4835432, 4836616, 4837010]\n",
       "                               ...                        \n",
       "7258              [14783695, 14783697, 14784417, 14784415]\n",
       "7262                        [14783655, 14783715, 14783667]\n",
       "7266     [12169339, 12169341, 12169343, 12169095, 12169...\n",
       "7272                                  [14768198, 14768180]\n",
       "7273                                             [4142418]\n",
       "Name: fl_comid_list, Length: 3218, dtype: object"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of comids in flowlines\n",
    "df_merged_full.fl_comid_list#[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-future",
   "metadata": {},
   "source": [
    "### .... and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "continued-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Filter out invalid comids (although not used in this notebook)\n",
    "# # \"invalid\" = present in GEE but not present in nhd_stats\n",
    "\n",
    "# df_merged[\"wb_comid_list_filtered\"] = df_merged.apply(lambda x: [comid for comid in x.nhd_vars_wb[0] if comid in np.array(nhd_stats.comid)\n",
    "#                                                                 ], axis=1)\n",
    "\n",
    "# df_merged[\"fl_comid_list_filtered\"] = df_merged.apply(lambda x: [comid for comid in x.nhd_vars_fl[0] if comid in np.array(nhd_stats.comid)\n",
    "#                                                                 ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "immune-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigned begin and end of records for each person\n",
    "# MADHUKAR: records 1 - 5000\n",
    "# SHOBHA: records 5000 - 10000\n",
    "# RADHIKA: records 10000 - 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "naked-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features present in nhd_stats for corresponding comid\n",
    "# read in fl_comid_list, pull out matching variable values in nhd_stats\n",
    "\n",
    "df_merged = df_merged_full\n",
    "\n",
    "def extract_feature(comid, feature):\n",
    "    \"\"\"\n",
    "    Extract features present in nhd_stats for corresponding comid\n",
    "    \"\"\"\n",
    "    if comid == None:\n",
    "        return np.nan # if no comid's in GEE\n",
    "    extracted_feature = nhd_stats[nhd_stats[\"comid\"] == comid][str(feature)]\n",
    "    try:\n",
    "        extracted_feature = np.array(extracted_feature).item() \n",
    "    except Exception as e:\n",
    "        return np.nan # if comid in GEE but not in nhd database\n",
    "    return extracted_feature\n",
    "\n",
    "\n",
    "def extract_sum(feature):\n",
    "    \"\"\"\n",
    "    feature engineering per excel sheet\n",
    "    \"\"\"\n",
    "    return (df_merged.apply(lambda x: np.sum(np.array([extract_feature(comid, str(feature))\n",
    "                                                                 for comid in x.fl_comid_list])\n",
    "                                                       [~np.isnan(np.array([extract_feature(comid, str(feature))\n",
    "                                                                            for comid in x.fl_comid_list]))]), \n",
    "                                                axis=1))\n",
    "def extract_count(feature):\n",
    "    \"\"\"\n",
    "    feature engineering per excel sheet\n",
    "    \"\"\"\n",
    "    return (df_merged.apply(lambda x: len(np.array([extract_feature(comid, str(feature))\n",
    "                                                                 for comid in x.fl_comid_list])\n",
    "                                                       [~np.isnan(np.array([extract_feature(comid, str(feature))\n",
    "                                                                            for comid in x.fl_comid_list]))]), \n",
    "                                                  axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "departmental-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flowline variables\n",
    "\n",
    "# areasqkm\n",
    "df_merged[\"fl_areasqkm_sum\"] = extract_sum(\"areasqkm\")\n",
    "df_merged[\"fl_areasqkm_count\"] = extract_count(\"areasqkm\")\n",
    "df_merged[\"fl_areasqkm_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_areasqkm_sum/x.fl_areasqkm_count) \n",
    "                                                 if x.fl_areasqkm_count != 0 \n",
    "#                                                  else np.nan, axis=1)) # here you want to return 0\n",
    "                                                 else 0, axis=1))\n",
    "# gnis_name_ind\n",
    "df_merged[\"fl_gnis_name_ind_sum\"] = extract_sum(\"gnis_name_ind\")\n",
    "df_merged[\"fl_gnis_name_ind_count\"] = extract_count(\"gnis_name_ind\")\n",
    "df_merged[\"fl_gnis_name_ind_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_gnis_name_ind_sum/x.fl_gnis_name_ind_count) \n",
    "                                                 if x.fl_gnis_name_ind_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))\n",
    "\n",
    "# totdasqkm\n",
    "df_merged[\"fl_totdasqkm_sum\"] = extract_sum(\"totdasqkm\")\n",
    "df_merged[\"fl_totdasqkm_count\"] = extract_count(\"totdasqkm\")\n",
    "df_merged[\"fl_totdasqkm_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_totdasqkm_sum/x.fl_totdasqkm_count) \n",
    "                                                 if x.fl_totdasqkm_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))\n",
    "\n",
    "# flow_type\n",
    "df_merged[\"fl_flow_type_sum\"] = extract_sum(\"flow_type\")\n",
    "df_merged[\"fl_flow_type_count\"] = extract_count(\"flow_type\")\n",
    "df_merged[\"fl_flow_type_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_flow_type_sum/x.fl_flow_type_count) \n",
    "                                                 if x.fl_flow_type_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))\n",
    "\n",
    "                                                  \n",
    "# streamorde\n",
    "df_merged[\"fl_streamorde_sum\"] = extract_sum(\"streamorde\")\n",
    "df_merged[\"fl_streamorde_count\"] = extract_count(\"streamorde\")\n",
    "df_merged[\"fl_streamorde_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_streamorde_sum/x.fl_streamorde_count) \n",
    "                                                 if x.fl_streamorde_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))                                                   \n",
    "\n",
    "# intephem\n",
    "df_merged[\"fl_intephem_sum\"] = extract_sum(\"intephem\")\n",
    "df_merged[\"fl_intephem_count\"] = extract_count(\"intephem\")\n",
    "df_merged[\"fl_intephem_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_intephem_sum/x.fl_intephem_count) \n",
    "                                                 if x.fl_intephem_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))                                                 \n",
    "\n",
    "# startflag\n",
    "df_merged[\"fl_startflag_sum\"] = extract_sum(\"startflag\")\n",
    "df_merged[\"fl_startflag_count\"] = extract_count(\"startflag\")\n",
    "df_merged[\"fl_startflag_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_startflag_sum/x.fl_startflag_count) \n",
    "                                                 if x.fl_startflag_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))                                                  \n",
    "\n",
    "# divergence\n",
    "df_merged[\"fl_divergence_sum\"] = extract_sum(\"divergence\")\n",
    "df_merged[\"fl_divergence_count\"] = extract_count(\"divergence\")\n",
    "df_merged[\"fl_divergence_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_divergence_sum/x.fl_divergence_count) \n",
    "                                                 if x.fl_divergence_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-malta",
   "metadata": {},
   "source": [
    "# In a similar fasion, you can feature engineer the waterbodies (I will get that later today/tomorrow and push the updated dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "historical-mortality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>Index</th>\n",
       "      <th>fl_areasqkm_sum</th>\n",
       "      <th>fl_areasqkm_count</th>\n",
       "      <th>fl_areasqkm_mean</th>\n",
       "      <th>fl_gnis_name_ind_sum</th>\n",
       "      <th>fl_gnis_name_ind_count</th>\n",
       "      <th>fl_gnis_name_ind_mean</th>\n",
       "      <th>fl_totdasqkm_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>fl_streamorde_mean</th>\n",
       "      <th>fl_intephem_sum</th>\n",
       "      <th>fl_intephem_count</th>\n",
       "      <th>fl_intephem_mean</th>\n",
       "      <th>fl_startflag_sum</th>\n",
       "      <th>fl_startflag_count</th>\n",
       "      <th>fl_startflag_mean</th>\n",
       "      <th>fl_divergence_sum</th>\n",
       "      <th>fl_divergence_count</th>\n",
       "      <th>fl_divergence_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3.218000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.645566</td>\n",
       "      <td>-91.903265</td>\n",
       "      <td>1609.500000</td>\n",
       "      <td>14.947745</td>\n",
       "      <td>4.160348</td>\n",
       "      <td>5.158235</td>\n",
       "      <td>1.860783</td>\n",
       "      <td>4.878807</td>\n",
       "      <td>0.368007</td>\n",
       "      <td>1.997859e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>1.568769</td>\n",
       "      <td>1.425420</td>\n",
       "      <td>4.878807</td>\n",
       "      <td>0.328276</td>\n",
       "      <td>1.493785</td>\n",
       "      <td>4.160348</td>\n",
       "      <td>0.373625</td>\n",
       "      <td>0.244873</td>\n",
       "      <td>4.160348</td>\n",
       "      <td>0.027611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.438762</td>\n",
       "      <td>15.926433</td>\n",
       "      <td>929.100909</td>\n",
       "      <td>17.333822</td>\n",
       "      <td>4.021981</td>\n",
       "      <td>9.669960</td>\n",
       "      <td>2.329257</td>\n",
       "      <td>7.951726</td>\n",
       "      <td>0.360386</td>\n",
       "      <td>3.294527e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.205041</td>\n",
       "      <td>1.871252</td>\n",
       "      <td>7.951726</td>\n",
       "      <td>0.372553</td>\n",
       "      <td>1.453921</td>\n",
       "      <td>4.021981</td>\n",
       "      <td>0.353971</td>\n",
       "      <td>1.288509</td>\n",
       "      <td>4.021981</td>\n",
       "      <td>0.130694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.447080</td>\n",
       "      <td>-158.125820</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.998713e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>35.616210</td>\n",
       "      <td>-93.991342</td>\n",
       "      <td>805.250000</td>\n",
       "      <td>7.143300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.512172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.561150e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.781325</td>\n",
       "      <td>-88.034425</td>\n",
       "      <td>1609.500000</td>\n",
       "      <td>12.900150</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.884762</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.350385e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.303846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>43.316587</td>\n",
       "      <td>-81.846850</td>\n",
       "      <td>2413.750000</td>\n",
       "      <td>19.067175</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.609587</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.180284e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>70.296720</td>\n",
       "      <td>144.765240</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>564.331500</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>283.535100</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.382164e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          latitude    longitude        Index  fl_areasqkm_sum  \\\n",
       "count  3218.000000  3218.000000  3218.000000      3218.000000   \n",
       "mean     39.645566   -91.903265  1609.500000        14.947745   \n",
       "std       5.438762    15.926433   929.100909        17.333822   \n",
       "min      13.447080  -158.125820     1.000000         0.000000   \n",
       "25%      35.616210   -93.991342   805.250000         7.143300   \n",
       "50%      39.781325   -88.034425  1609.500000        12.900150   \n",
       "75%      43.316587   -81.846850  2413.750000        19.067175   \n",
       "max      70.296720   144.765240  3218.000000       564.331500   \n",
       "\n",
       "       fl_areasqkm_count  fl_areasqkm_mean  fl_gnis_name_ind_sum  \\\n",
       "count        3218.000000       3218.000000           3218.000000   \n",
       "mean            4.160348          5.158235              1.860783   \n",
       "std             4.021981          9.669960              2.329257   \n",
       "min             0.000000          0.000000              0.000000   \n",
       "25%             1.000000          1.512172              0.000000   \n",
       "50%             3.000000          2.884762              1.000000   \n",
       "75%             6.000000          5.609587              3.000000   \n",
       "max            56.000000        283.535100             25.000000   \n",
       "\n",
       "       fl_gnis_name_ind_count  fl_gnis_name_ind_mean  fl_totdasqkm_sum  ...  \\\n",
       "count             3218.000000            3218.000000      3.218000e+03  ...   \n",
       "mean                 4.878807               0.368007      1.997859e+04  ...   \n",
       "std                  7.951726               0.360386      3.294527e+05  ...   \n",
       "min                  0.000000               0.000000     -5.998713e+04  ...   \n",
       "25%                  2.000000               0.000000      9.561150e+00  ...   \n",
       "50%                  4.000000               0.333333      3.350385e+01  ...   \n",
       "75%                  6.000000               0.666667      2.180284e+02  ...   \n",
       "max                235.000000               1.000000      1.382164e+07  ...   \n",
       "\n",
       "       fl_streamorde_mean  fl_intephem_sum  fl_intephem_count  \\\n",
       "count         3218.000000      3218.000000        3218.000000   \n",
       "mean             1.568769         1.425420           4.878807   \n",
       "std              1.205041         1.871252           7.951726   \n",
       "min              0.000000         0.000000           0.000000   \n",
       "25%              1.000000         0.000000           2.000000   \n",
       "50%              1.303846         1.000000           4.000000   \n",
       "75%              2.000000         2.000000           6.000000   \n",
       "max             10.000000        14.000000         235.000000   \n",
       "\n",
       "       fl_intephem_mean  fl_startflag_sum  fl_startflag_count  \\\n",
       "count       3218.000000       3218.000000         3218.000000   \n",
       "mean           0.328276          1.493785            4.160348   \n",
       "std            0.372553          1.453921            4.021981   \n",
       "min            0.000000          0.000000            0.000000   \n",
       "25%            0.000000          0.000000            1.000000   \n",
       "50%            0.200000          1.000000            3.000000   \n",
       "75%            0.600000          2.000000            6.000000   \n",
       "max            1.000000          8.000000           56.000000   \n",
       "\n",
       "       fl_startflag_mean  fl_divergence_sum  fl_divergence_count  \\\n",
       "count        3218.000000        3218.000000          3218.000000   \n",
       "mean            0.373625           0.244873             4.160348   \n",
       "std             0.353971           1.288509             4.021981   \n",
       "min             0.000000           0.000000             0.000000   \n",
       "25%             0.000000           0.000000             1.000000   \n",
       "50%             0.333333           0.000000             3.000000   \n",
       "75%             0.600000           0.000000             6.000000   \n",
       "max             1.000000          28.000000            56.000000   \n",
       "\n",
       "       fl_divergence_mean  \n",
       "count         3218.000000  \n",
       "mean             0.027611  \n",
       "std              0.130694  \n",
       "min              0.000000  \n",
       "25%              0.000000  \n",
       "50%              0.000000  \n",
       "75%              0.000000  \n",
       "max              2.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "medieval-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the variables that needs to be made accessible to manipulation via csv files\n",
    "# wb_ftype_str_list\n",
    "# wb_area_list\n",
    "# fl_ftype_str_list\n",
    "# fl_length_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "adaptive-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged\n",
    "\n",
    "# wb for waterbodies\n",
    "# extract the individual lists\n",
    "df[\"wb_comid_str\"] = df.apply(lambda x: \"+\".join([str(comid) for comid in x.nhd_vars_wb[0]]), axis=1)\n",
    "df[\"wb_ftype_str\"] = df.apply(lambda x: \"+\".join([str(ftype) for ftype in x.nhd_vars_wb[1] if ftype != None]), axis=1)\n",
    "df[\"wb_gnis_id_str\"] = df.apply(lambda x: \"+\".join([str(gnis) for gnis in x.nhd_vars_wb[2]]), axis=1)\n",
    "\n",
    "# # sum and mean\n",
    "df[\"wb_area_sum\"] = df.apply(lambda x: np.sum(np.array([area for area in x.nhd_vars_wb[3] if area != None])), axis=1)\n",
    "df[\"wb_area_count\"] = df.apply(lambda x: len([area for area in x.nhd_vars_wb[3] if area != None]), axis=1)\n",
    "# df[\"wb_area_mean\"] = df.apply(lambda x: (x.wb_area_sum / x.wb_area_count) if x.wb_area_count != 0 else np.nan, axis=1)\n",
    "df[\"wb_area_mean\"] = df.apply(lambda x: (x.wb_area_sum / x.wb_area_count) if x.wb_area_count != 0 else 0, axis=1)\n",
    "\n",
    "df[\"wb_gnis_name_ind_sum\"] = df.apply(lambda x: np.sum(np.array([int(gnis) for gnis in x.nhd_vars_wb[2] if gnis not in [\"\", None]])), axis=1)\n",
    "df[\"wb_gnis_name_ind_count\"] = df.apply(lambda x: len([gnis for gnis in x.nhd_vars_wb[2] if gnis not in [\"\", None]]), axis=1)\n",
    "# df[\"wb_gnis_name_ind_mean\"] = df.apply(lambda x: (x.wb_gnis_name_ind_sum / x.wb_gnis_name_ind_count) if x.wb_gnis_name_ind_count != 0 else np.nan, axis=1)\n",
    "df[\"wb_gnis_name_ind_mean\"] = df.apply(lambda x: (x.wb_gnis_name_ind_sum / x.wb_gnis_name_ind_count) if x.wb_gnis_name_ind_count != 0 else 0, axis=1)\n",
    "\n",
    "\n",
    "# # fl for flowlines\n",
    "# # extract the individual lists\n",
    "df[\"fl_comid_str\"] = df.apply(lambda x: \"+\".join([str(comid) for comid in x.nhd_vars_fl[0]]), axis=1)\n",
    "df[\"fl_ftype_str\"] = df.apply(lambda x: \"+\".join([str(ftype) for ftype in x.nhd_vars_fl[1]]), axis=1)\n",
    "df[\"fl_gnis_id_str\"] = df.apply(lambda x: \"+\".join(x.nhd_vars_fl[2]), axis=1)\n",
    "\n",
    "# # sum and mean\n",
    "df[\"fl_length_sum\"] = df.apply(lambda x: np.sum(np.array([length for length in x.nhd_vars_wb[4] if length != None])), axis=1)\n",
    "df[\"fl_length_count\"] = df.apply(lambda x: len([length for length in x.nhd_vars_wb[4] if length != None]), axis=1)\n",
    "# df[\"fl_length_mean\"] = df.apply(lambda x: (x.fl_length_sum / x.fl_length_count) if x.fl_length_count != 0 else np.nan, axis=1)\n",
    "df[\"fl_length_mean\"] = df.apply(lambda x: (x.fl_length_sum / x.fl_length_count) if x.fl_length_count != 0 else 0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "formed-austin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['jurisdiction_type', 'da_number', 'latitude', 'longitude',\n",
       "       'cwa_determination', 'Index', 'nhd_vars_wb', 'nhd_vars_fl',\n",
       "       'wb_comid_list', 'wb_ftype_str_list', 'wb_gnis_id_list', 'wb_area_list',\n",
       "       'fl_comid_list', 'fl_ftype_str_list', 'fl_gnis_id_list',\n",
       "       'fl_length_list', 'fl_areasqkm_sum', 'fl_areasqkm_count',\n",
       "       'fl_areasqkm_mean', 'fl_gnis_name_ind_sum', 'fl_gnis_name_ind_count',\n",
       "       'fl_gnis_name_ind_mean', 'fl_totdasqkm_sum', 'fl_totdasqkm_count',\n",
       "       'fl_totdasqkm_mean', 'fl_flow_type_sum', 'fl_flow_type_count',\n",
       "       'fl_flow_type_mean', 'fl_streamorde_sum', 'fl_streamorde_count',\n",
       "       'fl_streamorde_mean', 'fl_intephem_sum', 'fl_intephem_count',\n",
       "       'fl_intephem_mean', 'fl_startflag_sum', 'fl_startflag_count',\n",
       "       'fl_startflag_mean', 'fl_divergence_sum', 'fl_divergence_count',\n",
       "       'fl_divergence_mean', 'wb_comid_str', 'wb_ftype_str', 'wb_gnis_id_str',\n",
       "       'wb_area_sum', 'wb_area_count', 'wb_area_mean', 'wb_gnis_name_ind_sum',\n",
       "       'wb_gnis_name_ind_count', 'wb_gnis_name_ind_mean', 'fl_comid_str',\n",
       "       'fl_ftype_str', 'fl_gnis_id_str', 'fl_length_sum', 'fl_length_count',\n",
       "       'fl_length_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "found-division",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cwr_nwpr_2500m_nhd_variables_partsmerged'"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'cwr_nwpr_' + str(2 * PATCH_SIZE) + 'm_nhd_variables_partsmerged'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "personal-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# use protocol 3 for backwards compatibility with Python 3.6 on AWS\n",
    "pickle.dump(df, open('cwr_nwpr_' + str(2 * PATCH_SIZE) + 'm_nhd_variables_extracted',\"wb\"), protocol=3)\n",
    "# df.to_csv(\"1000mX1000m_nhd_variables_extracted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "false-karma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cwr_nwpr_2500m_nhd_variables_extracted'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'cwr_nwpr_' + str(2 * PATCH_SIZE) + 'm_nhd_variables_extracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "necessary-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged.to_pickle(\"combined_regular_with_ssurgo_nhd_2.5kmX2.5km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "ethical-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"combined_regular_with_ssurgo_nhd_2.5kmX2.5km.csv\")\n",
    "# df.to_pickle(\"combined_regular_with_ssurgo_nhd_2.5kmX2.5km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "loaded-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"combined_regular_with_ssurgo_nhd_200mX200m.csv\")\n",
    "# df.to_pickle(\"combined_regular_with_ssurgo_nhd_200mX200m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-rental",
   "metadata": {},
   "source": [
    "# Strip ftype list into individual columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-insert",
   "metadata": {},
   "source": [
    "## replace 200m by 2.5km and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "atmospheric-median",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>Index</th>\n",
       "      <th>fl_areasqkm_sum</th>\n",
       "      <th>fl_areasqkm_count</th>\n",
       "      <th>fl_areasqkm_mean</th>\n",
       "      <th>fl_gnis_name_ind_sum</th>\n",
       "      <th>fl_gnis_name_ind_count</th>\n",
       "      <th>fl_gnis_name_ind_mean</th>\n",
       "      <th>fl_totdasqkm_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>fl_divergence_mean</th>\n",
       "      <th>wb_area_sum</th>\n",
       "      <th>wb_area_count</th>\n",
       "      <th>wb_area_mean</th>\n",
       "      <th>wb_gnis_name_ind_sum</th>\n",
       "      <th>wb_gnis_name_ind_count</th>\n",
       "      <th>wb_gnis_name_ind_mean</th>\n",
       "      <th>fl_length_sum</th>\n",
       "      <th>fl_length_count</th>\n",
       "      <th>fl_length_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3.218000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3.218000e+03</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>3.218000e+03</td>\n",
       "      <td>1758.0</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>1758.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.645566</td>\n",
       "      <td>-91.903265</td>\n",
       "      <td>1609.500000</td>\n",
       "      <td>14.947745</td>\n",
       "      <td>4.160348</td>\n",
       "      <td>5.158235</td>\n",
       "      <td>1.860783</td>\n",
       "      <td>4.878807</td>\n",
       "      <td>0.368007</td>\n",
       "      <td>1.997859e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027611</td>\n",
       "      <td>409.104406</td>\n",
       "      <td>0.830019</td>\n",
       "      <td>296.881952</td>\n",
       "      <td>2.081034e+05</td>\n",
       "      <td>0.227160</td>\n",
       "      <td>1.610609e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.052517</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.438762</td>\n",
       "      <td>15.926433</td>\n",
       "      <td>929.100909</td>\n",
       "      <td>17.333822</td>\n",
       "      <td>4.021981</td>\n",
       "      <td>9.669960</td>\n",
       "      <td>2.329257</td>\n",
       "      <td>7.951726</td>\n",
       "      <td>0.360386</td>\n",
       "      <td>3.294527e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130694</td>\n",
       "      <td>4410.133505</td>\n",
       "      <td>1.665352</td>\n",
       "      <td>3495.852199</td>\n",
       "      <td>5.703504e+05</td>\n",
       "      <td>0.604857</td>\n",
       "      <td>4.120467e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.771193</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>13.447080</td>\n",
       "      <td>-158.125820</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.998713e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>35.616210</td>\n",
       "      <td>-93.991342</td>\n",
       "      <td>805.250000</td>\n",
       "      <td>7.143300</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.512172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.561150e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.781325</td>\n",
       "      <td>-88.034425</td>\n",
       "      <td>1609.500000</td>\n",
       "      <td>12.900150</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.884762</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.350385e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>43.316587</td>\n",
       "      <td>-81.846850</td>\n",
       "      <td>2413.750000</td>\n",
       "      <td>19.067175</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.609587</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.180284e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033625</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>70.296720</td>\n",
       "      <td>144.765240</td>\n",
       "      <td>3218.000000</td>\n",
       "      <td>564.331500</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>283.535100</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>235.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.382164e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>57527.747000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>57516.647000</td>\n",
       "      <td>8.770948e+06</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.026720e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          latitude    longitude        Index  fl_areasqkm_sum  \\\n",
       "count  3218.000000  3218.000000  3218.000000      3218.000000   \n",
       "mean     39.645566   -91.903265  1609.500000        14.947745   \n",
       "std       5.438762    15.926433   929.100909        17.333822   \n",
       "min      13.447080  -158.125820     1.000000         0.000000   \n",
       "25%      35.616210   -93.991342   805.250000         7.143300   \n",
       "50%      39.781325   -88.034425  1609.500000        12.900150   \n",
       "75%      43.316587   -81.846850  2413.750000        19.067175   \n",
       "max      70.296720   144.765240  3218.000000       564.331500   \n",
       "\n",
       "       fl_areasqkm_count  fl_areasqkm_mean  fl_gnis_name_ind_sum  \\\n",
       "count        3218.000000       3218.000000           3218.000000   \n",
       "mean            4.160348          5.158235              1.860783   \n",
       "std             4.021981          9.669960              2.329257   \n",
       "min             0.000000          0.000000              0.000000   \n",
       "25%             1.000000          1.512172              0.000000   \n",
       "50%             3.000000          2.884762              1.000000   \n",
       "75%             6.000000          5.609587              3.000000   \n",
       "max            56.000000        283.535100             25.000000   \n",
       "\n",
       "       fl_gnis_name_ind_count  fl_gnis_name_ind_mean  fl_totdasqkm_sum  ...  \\\n",
       "count             3218.000000            3218.000000      3.218000e+03  ...   \n",
       "mean                 4.878807               0.368007      1.997859e+04  ...   \n",
       "std                  7.951726               0.360386      3.294527e+05  ...   \n",
       "min                  0.000000               0.000000     -5.998713e+04  ...   \n",
       "25%                  2.000000               0.000000      9.561150e+00  ...   \n",
       "50%                  4.000000               0.333333      3.350385e+01  ...   \n",
       "75%                  6.000000               0.666667      2.180284e+02  ...   \n",
       "max                235.000000               1.000000      1.382164e+07  ...   \n",
       "\n",
       "       fl_divergence_mean   wb_area_sum  wb_area_count  wb_area_mean  \\\n",
       "count         3218.000000   3218.000000    3218.000000   3218.000000   \n",
       "mean             0.027611    409.104406       0.830019    296.881952   \n",
       "std              0.130694   4410.133505       1.665352   3495.852199   \n",
       "min              0.000000      0.000000       0.000000      0.000000   \n",
       "25%              0.000000      0.000000       0.000000      0.000000   \n",
       "50%              0.000000      0.000000       0.000000      0.000000   \n",
       "75%              0.000000      0.056000       1.000000      0.033625   \n",
       "max              2.000000  57527.747000      25.000000  57516.647000   \n",
       "\n",
       "       wb_gnis_name_ind_sum  wb_gnis_name_ind_count  wb_gnis_name_ind_mean  \\\n",
       "count          3.218000e+03             3218.000000           3.218000e+03   \n",
       "mean           2.081034e+05                0.227160           1.610609e+05   \n",
       "std            5.703504e+05                0.604857           4.120467e+05   \n",
       "min            0.000000e+00                0.000000           0.000000e+00   \n",
       "25%            0.000000e+00                0.000000           0.000000e+00   \n",
       "50%            0.000000e+00                0.000000           0.000000e+00   \n",
       "75%            0.000000e+00                0.000000           0.000000e+00   \n",
       "max            8.770948e+06                6.000000           2.026720e+06   \n",
       "\n",
       "       fl_length_sum  fl_length_count  fl_length_mean  \n",
       "count         1758.0      3218.000000          1758.0  \n",
       "mean             0.0         1.052517             0.0  \n",
       "std              0.0         1.771193             0.0  \n",
       "min              0.0         0.000000             0.0  \n",
       "25%              0.0         0.000000             0.0  \n",
       "50%              0.0         0.000000             0.0  \n",
       "75%              0.0         1.000000             0.0  \n",
       "max              0.0        25.000000             0.0  \n",
       "\n",
       "[8 rows x 36 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"combined_regular_with_ssurgo_nhd_2.5kmX2.5km_pkl3.pkl\", \"rb\") as f:\n",
    "#     df_readfrompkl = pickle.load(f)\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# df = pd.read_pickle(\"2.5kmX2.5km_nhd_variables_extracted\")\n",
    "df = pd.read_pickle('cwr_nwpr_' + str(2 * PATCH_SIZE) + 'm_nhd_variables_extracted')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "reported-mattress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['jurisdiction_type', 'da_number', 'latitude', 'longitude',\n",
       "       'cwa_determination', 'Index', 'nhd_vars_wb', 'nhd_vars_fl',\n",
       "       'wb_comid_list', 'wb_ftype_str_list', 'wb_gnis_id_list', 'wb_area_list',\n",
       "       'fl_comid_list', 'fl_ftype_str_list', 'fl_gnis_id_list',\n",
       "       'fl_length_list', 'fl_areasqkm_sum', 'fl_areasqkm_count',\n",
       "       'fl_areasqkm_mean', 'fl_gnis_name_ind_sum', 'fl_gnis_name_ind_count',\n",
       "       'fl_gnis_name_ind_mean', 'fl_totdasqkm_sum', 'fl_totdasqkm_count',\n",
       "       'fl_totdasqkm_mean', 'fl_flow_type_sum', 'fl_flow_type_count',\n",
       "       'fl_flow_type_mean', 'fl_streamorde_sum', 'fl_streamorde_count',\n",
       "       'fl_streamorde_mean', 'fl_intephem_sum', 'fl_intephem_count',\n",
       "       'fl_intephem_mean', 'fl_startflag_sum', 'fl_startflag_count',\n",
       "       'fl_startflag_mean', 'fl_divergence_sum', 'fl_divergence_count',\n",
       "       'fl_divergence_mean', 'wb_comid_str', 'wb_ftype_str', 'wb_gnis_id_str',\n",
       "       'wb_area_sum', 'wb_area_count', 'wb_area_mean', 'wb_gnis_name_ind_sum',\n",
       "       'wb_gnis_name_ind_count', 'wb_gnis_name_ind_mean', 'fl_comid_str',\n",
       "       'fl_ftype_str', 'fl_gnis_id_str', 'fl_length_sum', 'fl_length_count',\n",
       "       'fl_length_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "retained-execution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['jurisdiction_type', 'da_number', 'latitude', 'longitude',\n",
       "       'cwa_determination', 'Index', 'nhd_vars_wb_2500m', 'nhd_vars_fl_2500m',\n",
       "       'wb_comid_list_2500m', 'wb_ftype_str_list_2500m',\n",
       "       'wb_gnis_id_list_2500m', 'wb_area_list_2500m', 'fl_comid_list_2500m',\n",
       "       'fl_ftype_str_list_2500m', 'fl_gnis_id_list_2500m',\n",
       "       'fl_length_list_2500m', 'fl_areasqkm_sum_2500m',\n",
       "       'fl_areasqkm_count_2500m', 'fl_areasqkm_mean_2500m',\n",
       "       'fl_gnis_name_ind_sum_2500m', 'fl_gnis_name_ind_count_2500m',\n",
       "       'fl_gnis_name_ind_mean_2500m', 'fl_totdasqkm_sum_2500m',\n",
       "       'fl_totdasqkm_count_2500m', 'fl_totdasqkm_mean_2500m',\n",
       "       'fl_flow_type_sum_2500m', 'fl_flow_type_count_2500m',\n",
       "       'fl_flow_type_mean_2500m', 'fl_streamorde_sum_2500m',\n",
       "       'fl_streamorde_count_2500m', 'fl_streamorde_mean_2500m',\n",
       "       'fl_intephem_sum_2500m', 'fl_intephem_count_2500m',\n",
       "       'fl_intephem_mean_2500m', 'fl_startflag_sum_2500m',\n",
       "       'fl_startflag_count_2500m', 'fl_startflag_mean_2500m',\n",
       "       'fl_divergence_sum_2500m', 'fl_divergence_count_2500m',\n",
       "       'fl_divergence_mean_2500m', 'wb_comid_str_2500m', 'wb_ftype_str_2500m',\n",
       "       'wb_gnis_id_str_2500m', 'wb_area_sum_2500m', 'wb_area_count_2500m',\n",
       "       'wb_area_mean_2500m', 'wb_gnis_name_ind_sum_2500m',\n",
       "       'wb_gnis_name_ind_count_2500m', 'wb_gnis_name_ind_mean_2500m',\n",
       "       'fl_comid_str_2500m', 'fl_ftype_str_2500m', 'fl_gnis_id_str_2500m',\n",
       "       'fl_length_sum_2500m', 'fl_length_count_2500m', 'fl_length_mean_2500m'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_columns = (['nhd_vars_wb', 'nhd_vars_fl',\n",
    "       'wb_comid_list', 'wb_ftype_str_list', 'wb_gnis_id_list', 'wb_area_list',\n",
    "       'fl_comid_list', 'fl_ftype_str_list', 'fl_gnis_id_list',\n",
    "       'fl_length_list', 'fl_areasqkm_sum', 'fl_areasqkm_count',\n",
    "       'fl_areasqkm_mean', 'fl_gnis_name_ind_sum', 'fl_gnis_name_ind_count',\n",
    "       'fl_gnis_name_ind_mean', 'fl_totdasqkm_sum', 'fl_totdasqkm_count',\n",
    "       'fl_totdasqkm_mean', 'fl_flow_type_sum', 'fl_flow_type_count',\n",
    "       'fl_flow_type_mean', 'fl_streamorde_sum', 'fl_streamorde_count',\n",
    "       'fl_streamorde_mean', 'fl_intephem_sum', 'fl_intephem_count',\n",
    "       'fl_intephem_mean', 'fl_startflag_sum', 'fl_startflag_count',\n",
    "       'fl_startflag_mean', 'fl_divergence_sum', 'fl_divergence_count',\n",
    "       'fl_divergence_mean', 'wb_comid_str', 'wb_ftype_str', 'wb_gnis_id_str',\n",
    "       'wb_area_sum', 'wb_area_count', 'wb_area_mean', 'wb_gnis_name_ind_sum',\n",
    "       'wb_gnis_name_ind_count', 'wb_gnis_name_ind_mean', 'fl_comid_str',\n",
    "       'fl_ftype_str', 'fl_gnis_id_str', 'fl_length_sum', 'fl_length_count',\n",
    "       'fl_length_mean'])\n",
    "\n",
    "\n",
    "new_columns = [column + \"_\" + str(2 * PATCH_SIZE) + \"m\" for column in old_columns]\n",
    "new_columns\n",
    "\n",
    "col_name_dict = dict(zip(old_columns, new_columns))\n",
    "\n",
    "\n",
    "df.rename(columns=col_name_dict, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "different-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.fl_ftype_str_list_1000m#[9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "grateful-watson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comid', 'long_comid', 'lat_comid', 'startflag', 'intephem',\n",
       "       'divergence', 'streamorde', 'lengthkm', 'gnis_name_ind', 'areasqkm',\n",
       "       'totdasqkm', 'flow_type', 'distup_max', 'distdown_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nhd_stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "based-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attribute_features = ([\"ATTRIBUTE\",\"SYSTEM_NAME\"])\n",
    "\n",
    "# def feature_vector(df_attr=df, feature=\"fl_ftype_str_list_2500m\"):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     Returns the vector for each feature of an attribute\n",
    "    \n",
    "#     input: NWI Code Definition as df_attr to find unique values for a featurE & feature name \n",
    "#     output: feature vector for that (attribute, feature) set\n",
    "    \n",
    "#     \"\"\"\n",
    "#     # find unique values for a give feature\n",
    "#     feature_list = list(df_attr[feature].unique())\n",
    "#     # sort\n",
    "#     feature_list.sort()\n",
    "#     # lower case and replace spaces by _\n",
    "#     feature_list = [feature.lower().replace(\" \", \"_\") for feature in feature_list]\n",
    "    \n",
    "#     return feature_list\n",
    "#     # create a dict where the order is maintained. Initialize values to 0\n",
    "#     feature_dict = OrderedDict(zip(feature_list, [0] * len(feature_list)))\n",
    "    \n",
    "#     # you would like to actually create a new column with this frozen feature_list\n",
    "#     df_attr[feature + \"_\" + \"uniques_list\"] = df_attr.apply(lambda x: feature_list, axis=1)\n",
    "#     return df_attr\n",
    "    \n",
    "# feature_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "placed-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"fl_ftype_str_list_200m\"].unique()\n",
    "# pd.DataFrame(df.fl_ftype_str_list_1000m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "guided-leader",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['jurisdiction_type', 'da_number', 'latitude', 'longitude',\n",
       "       'cwa_determination', 'Index', 'nhd_vars_wb_2500m', 'nhd_vars_fl_2500m',\n",
       "       'wb_comid_list_2500m', 'wb_ftype_str_list_2500m',\n",
       "       'wb_gnis_id_list_2500m', 'wb_area_list_2500m', 'fl_comid_list_2500m',\n",
       "       'fl_ftype_str_list_2500m', 'fl_gnis_id_list_2500m',\n",
       "       'fl_length_list_2500m', 'fl_areasqkm_sum_2500m',\n",
       "       'fl_areasqkm_count_2500m', 'fl_areasqkm_mean_2500m',\n",
       "       'fl_gnis_name_ind_sum_2500m', 'fl_gnis_name_ind_count_2500m',\n",
       "       'fl_gnis_name_ind_mean_2500m', 'fl_totdasqkm_sum_2500m',\n",
       "       'fl_totdasqkm_count_2500m', 'fl_totdasqkm_mean_2500m',\n",
       "       'fl_flow_type_sum_2500m', 'fl_flow_type_count_2500m',\n",
       "       'fl_flow_type_mean_2500m', 'fl_streamorde_sum_2500m',\n",
       "       'fl_streamorde_count_2500m', 'fl_streamorde_mean_2500m',\n",
       "       'fl_intephem_sum_2500m', 'fl_intephem_count_2500m',\n",
       "       'fl_intephem_mean_2500m', 'fl_startflag_sum_2500m',\n",
       "       'fl_startflag_count_2500m', 'fl_startflag_mean_2500m',\n",
       "       'fl_divergence_sum_2500m', 'fl_divergence_count_2500m',\n",
       "       'fl_divergence_mean_2500m', 'wb_comid_str_2500m', 'wb_ftype_str_2500m',\n",
       "       'wb_gnis_id_str_2500m', 'wb_area_sum_2500m', 'wb_area_count_2500m',\n",
       "       'wb_area_mean_2500m', 'wb_gnis_name_ind_sum_2500m',\n",
       "       'wb_gnis_name_ind_count_2500m', 'wb_gnis_name_ind_mean_2500m',\n",
       "       'fl_comid_str_2500m', 'fl_ftype_str_2500m', 'fl_gnis_id_str_2500m',\n",
       "       'fl_length_sum_2500m', 'fl_length_count_2500m', 'fl_length_mean_2500m'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "valuable-saint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('artificialpath', 0), ('canalditch', 0), ('coastline', 0), ('connector', 0), ('pipeline', 0), ('streamriver', 0)])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "\n",
    "unique_ftype_dict = defaultdict(int)\n",
    "\n",
    "for row in np.array(df.index):\n",
    "    try:\n",
    "        for ftype in df.loc[row,\"fl_ftype_str_list_\" + str(2 * PATCH_SIZE) + \"m\"]:\n",
    "            unique_ftype_dict[ftype.lower()] += 1\n",
    "    except Exception as e:\n",
    "        print(df.loc[row,\"fl_ftype_str_list_\" + str(2 * PATCH_SIZE) + \"m\"])\n",
    "#         print(row)\n",
    "        print(e)\n",
    "unique_ftype_dict.keys()\n",
    "sorted_unique_ftype_list = sorted(tuple(unique_ftype_dict.keys()))\n",
    "\n",
    "ordered_ftype_dict = OrderedDict()\n",
    "for key in sorted_unique_ftype_list:\n",
    "    ordered_ftype_dict[key] = 0\n",
    "    \n",
    "ordered_ftype_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "italic-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df.loc[0, \"fl_ftype_str_list_1000m\"]\n",
    "# # \"fl_ftype_str_list_1000m\" in df.columns\n",
    "# df.fl_ftype_str_list_1000m[0:3]\n",
    "# df.loc[14324, \"fl_ftype_str_list_1000m\"]\n",
    "# for i in np.array(df.index):\n",
    "#     print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "broadband-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_unique_ftype_list = sorted(tuple(unique_ftype_dict.keys()))\n",
    "\n",
    "def create_ftype_vector(ftype_list):\n",
    "    ftype_dict = OrderedDict(zip(sorted_unique_ftype_list, [0] * len(sorted_unique_ftype_list)))    \n",
    "    \n",
    "    for ftype in ftype_list:\n",
    "        ftype_dict[ftype.lower()] += 1\n",
    "\n",
    "    return list(ftype_dict.values())\n",
    "    \n",
    "    \n",
    "\n",
    "df[\"fl_ftype_str_vector_\" + str(2 * PATCH_SIZE) + \"m\"] = df.apply(lambda x: create_ftype_vector(x[\"fl_ftype_str_list_\" + str(2 * PATCH_SIZE) + \"m\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "contained-croatia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14324    [0, 0, 0, 0, 0, 3]\n",
       "14339    [0, 0, 0, 0, 0, 3]\n",
       "14346    [0, 0, 0, 0, 0, 3]\n",
       "14348    [0, 1, 0, 0, 0, 5]\n",
       "14378    [2, 0, 0, 0, 0, 1]\n",
       "                ...        \n",
       "7258     [0, 0, 0, 0, 0, 4]\n",
       "7262     [0, 1, 0, 0, 0, 2]\n",
       "7266     [0, 0, 3, 0, 0, 2]\n",
       "7272     [0, 0, 0, 0, 0, 2]\n",
       "7273     [0, 0, 0, 0, 0, 1]\n",
       "Name: fl_ftype_str_vector_2500m, Length: 3218, dtype: object"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"fl_ftype_str_vector_\" + str(2 * PATCH_SIZE) + \"m\"]#[14324]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "velvet-stack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['StreamRiver', 'StreamRiver', 'StreamRiver']"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[14324, \"fl_ftype_str_list_\" + str(2 * PATCH_SIZE) + \"m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "crucial-sense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"fl_ftype_str_vector_\" + str(2 * PATCH_SIZE) + \"m\"][14324][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "successful-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, ftype in enumerate(sorted_unique_ftype_list):\n",
    "    df[\"fl_ftype_\" + ftype + \"_1000m\"] = df.apply(lambda x: x[\"fl_ftype_str_vector_\" + str(2 * PATCH_SIZE) + \"m\"][count], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "portuguese-moore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['jurisdiction_type', 'da_number', 'latitude', 'longitude',\n",
       "       'cwa_determination', 'Index', 'nhd_vars_wb_2500m', 'nhd_vars_fl_2500m',\n",
       "       'wb_comid_list_2500m', 'wb_ftype_str_list_2500m',\n",
       "       'wb_gnis_id_list_2500m', 'wb_area_list_2500m', 'fl_comid_list_2500m',\n",
       "       'fl_ftype_str_list_2500m', 'fl_gnis_id_list_2500m',\n",
       "       'fl_length_list_2500m', 'fl_areasqkm_sum_2500m',\n",
       "       'fl_areasqkm_count_2500m', 'fl_areasqkm_mean_2500m',\n",
       "       'fl_gnis_name_ind_sum_2500m', 'fl_gnis_name_ind_count_2500m',\n",
       "       'fl_gnis_name_ind_mean_2500m', 'fl_totdasqkm_sum_2500m',\n",
       "       'fl_totdasqkm_count_2500m', 'fl_totdasqkm_mean_2500m',\n",
       "       'fl_flow_type_sum_2500m', 'fl_flow_type_count_2500m',\n",
       "       'fl_flow_type_mean_2500m', 'fl_streamorde_sum_2500m',\n",
       "       'fl_streamorde_count_2500m', 'fl_streamorde_mean_2500m',\n",
       "       'fl_intephem_sum_2500m', 'fl_intephem_count_2500m',\n",
       "       'fl_intephem_mean_2500m', 'fl_startflag_sum_2500m',\n",
       "       'fl_startflag_count_2500m', 'fl_startflag_mean_2500m',\n",
       "       'fl_divergence_sum_2500m', 'fl_divergence_count_2500m',\n",
       "       'fl_divergence_mean_2500m', 'wb_comid_str_2500m', 'wb_ftype_str_2500m',\n",
       "       'wb_gnis_id_str_2500m', 'wb_area_sum_2500m', 'wb_area_count_2500m',\n",
       "       'wb_area_mean_2500m', 'wb_gnis_name_ind_sum_2500m',\n",
       "       'wb_gnis_name_ind_count_2500m', 'wb_gnis_name_ind_mean_2500m',\n",
       "       'fl_comid_str_2500m', 'fl_ftype_str_2500m', 'fl_gnis_id_str_2500m',\n",
       "       'fl_length_sum_2500m', 'fl_length_count_2500m', 'fl_length_mean_2500m',\n",
       "       'fl_ftype_str_vector_2500m', 'fl_ftype_artificialpath_1000m',\n",
       "       'fl_ftype_canalditch_1000m', 'fl_ftype_coastline_1000m',\n",
       "       'fl_ftype_connector_1000m', 'fl_ftype_pipeline_1000m',\n",
       "       'fl_ftype_streamriver_1000m'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-berry",
   "metadata": {},
   "source": [
    "# Strip wb_ftype list into individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "equipped-hygiene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('estuary', 0), ('lakepond', 0), ('playa', 0), ('reservoir', 0), ('swampmarsh', 0)])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "\n",
    "wb_unique_ftype_dict = defaultdict(int)\n",
    "\n",
    "for row in np.array(df.index):\n",
    "    for ftype in df.loc[row,\"wb_ftype_str_list_\" + str(2 * PATCH_SIZE) + \"m\"]:\n",
    "        wb_unique_ftype_dict[ftype.lower()] += 1\n",
    "        \n",
    "wb_unique_ftype_dict.keys()\n",
    "wb_sorted_unique_ftype_list = sorted(tuple(wb_unique_ftype_dict.keys()))\n",
    "\n",
    "wb_ordered_ftype_dict = OrderedDict()\n",
    "for key in wb_sorted_unique_ftype_list:\n",
    "    wb_ordered_ftype_dict[key] = 0\n",
    "    \n",
    "wb_ordered_ftype_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "bulgarian-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_sorted_unique_ftype_list = sorted(tuple(wb_unique_ftype_dict.keys()))\n",
    "\n",
    "def wb_create_ftype_vector(ftype_list):\n",
    "    ftype_dict = OrderedDict(zip(wb_sorted_unique_ftype_list, [0] * len(wb_sorted_unique_ftype_list)))    \n",
    "    \n",
    "    for ftype in ftype_list:\n",
    "        ftype_dict[ftype.lower()] += 1\n",
    "\n",
    "    return list(ftype_dict.values())\n",
    "    \n",
    "    \n",
    "\n",
    "df[\"wb_ftype_str_vector_\" + str(2 * PATCH_SIZE) + \"m\"] = df.apply(lambda x: wb_create_ftype_vector(x[\"wb_ftype_str_list_\" + str(2 * PATCH_SIZE) + \"m\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "rural-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_vector(ftype_vector, count):\n",
    "    try:\n",
    "        return ftype_vector[count]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "for count, ftype in enumerate(sorted_unique_ftype_list):\n",
    "    df[\"wb_ftype_\" + ftype + \"_\" + str(2 * PATCH_SIZE) + \"m\"] = df.apply(lambda x: strip_vector(x[\"wb_ftype_str_vector_\" + str(2 * PATCH_SIZE) + \"m\"], count), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "comfortable-archive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['jurisdiction_type', 'da_number', 'latitude', 'longitude',\n",
       "       'cwa_determination', 'Index', 'nhd_vars_wb_2500m', 'nhd_vars_fl_2500m',\n",
       "       'wb_comid_list_2500m', 'wb_ftype_str_list_2500m',\n",
       "       'wb_gnis_id_list_2500m', 'wb_area_list_2500m', 'fl_comid_list_2500m',\n",
       "       'fl_ftype_str_list_2500m', 'fl_gnis_id_list_2500m',\n",
       "       'fl_length_list_2500m', 'fl_areasqkm_sum_2500m',\n",
       "       'fl_areasqkm_count_2500m', 'fl_areasqkm_mean_2500m',\n",
       "       'fl_gnis_name_ind_sum_2500m', 'fl_gnis_name_ind_count_2500m',\n",
       "       'fl_gnis_name_ind_mean_2500m', 'fl_totdasqkm_sum_2500m',\n",
       "       'fl_totdasqkm_count_2500m', 'fl_totdasqkm_mean_2500m',\n",
       "       'fl_flow_type_sum_2500m', 'fl_flow_type_count_2500m',\n",
       "       'fl_flow_type_mean_2500m', 'fl_streamorde_sum_2500m',\n",
       "       'fl_streamorde_count_2500m', 'fl_streamorde_mean_2500m',\n",
       "       'fl_intephem_sum_2500m', 'fl_intephem_count_2500m',\n",
       "       'fl_intephem_mean_2500m', 'fl_startflag_sum_2500m',\n",
       "       'fl_startflag_count_2500m', 'fl_startflag_mean_2500m',\n",
       "       'fl_divergence_sum_2500m', 'fl_divergence_count_2500m',\n",
       "       'fl_divergence_mean_2500m', 'wb_comid_str_2500m', 'wb_ftype_str_2500m',\n",
       "       'wb_gnis_id_str_2500m', 'wb_area_sum_2500m', 'wb_area_count_2500m',\n",
       "       'wb_area_mean_2500m', 'wb_gnis_name_ind_sum_2500m',\n",
       "       'wb_gnis_name_ind_count_2500m', 'wb_gnis_name_ind_mean_2500m',\n",
       "       'fl_comid_str_2500m', 'fl_ftype_str_2500m', 'fl_gnis_id_str_2500m',\n",
       "       'fl_length_sum_2500m', 'fl_length_count_2500m', 'fl_length_mean_2500m',\n",
       "       'fl_ftype_str_vector_2500m', 'fl_ftype_artificialpath_1000m',\n",
       "       'fl_ftype_canalditch_1000m', 'fl_ftype_coastline_1000m',\n",
       "       'fl_ftype_connector_1000m', 'fl_ftype_pipeline_1000m',\n",
       "       'fl_ftype_streamriver_1000m', 'wb_ftype_str_vector_2500m',\n",
       "       'wb_ftype_artificialpath_2500m', 'wb_ftype_canalditch_2500m',\n",
       "       'wb_ftype_coastline_2500m', 'wb_ftype_connector_2500m',\n",
       "       'wb_ftype_pipeline_2500m', 'wb_ftype_streamriver_2500m'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "twelve-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this df into pickle\n",
    "import pickle\n",
    "pickle.dump(df, open('cwr_nwpr_' + str(2 * PATCH_SIZE) + 'm_nhd_variables_extracted_stripped',\"wb\"), protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "written-procedure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['jurisdiction_type', 'da_number', 'latitude', 'longitude',\n",
       "       'cwa_determination', 'Index', 'nhd_vars_wb_2500m', 'nhd_vars_fl_2500m',\n",
       "       'wb_comid_list_2500m', 'wb_ftype_str_list_2500m',\n",
       "       'wb_gnis_id_list_2500m', 'wb_area_list_2500m', 'fl_comid_list_2500m',\n",
       "       'fl_ftype_str_list_2500m', 'fl_gnis_id_list_2500m',\n",
       "       'fl_length_list_2500m', 'fl_areasqkm_sum_2500m',\n",
       "       'fl_areasqkm_count_2500m', 'fl_areasqkm_mean_2500m',\n",
       "       'fl_gnis_name_ind_sum_2500m', 'fl_gnis_name_ind_count_2500m',\n",
       "       'fl_gnis_name_ind_mean_2500m', 'fl_totdasqkm_sum_2500m',\n",
       "       'fl_totdasqkm_count_2500m', 'fl_totdasqkm_mean_2500m',\n",
       "       'fl_flow_type_sum_2500m', 'fl_flow_type_count_2500m',\n",
       "       'fl_flow_type_mean_2500m', 'fl_streamorde_sum_2500m',\n",
       "       'fl_streamorde_count_2500m', 'fl_streamorde_mean_2500m',\n",
       "       'fl_intephem_sum_2500m', 'fl_intephem_count_2500m',\n",
       "       'fl_intephem_mean_2500m', 'fl_startflag_sum_2500m',\n",
       "       'fl_startflag_count_2500m', 'fl_startflag_mean_2500m',\n",
       "       'fl_divergence_sum_2500m', 'fl_divergence_count_2500m',\n",
       "       'fl_divergence_mean_2500m', 'wb_comid_str_2500m', 'wb_ftype_str_2500m',\n",
       "       'wb_gnis_id_str_2500m', 'wb_area_sum_2500m', 'wb_area_count_2500m',\n",
       "       'wb_area_mean_2500m', 'wb_gnis_name_ind_sum_2500m',\n",
       "       'wb_gnis_name_ind_count_2500m', 'wb_gnis_name_ind_mean_2500m',\n",
       "       'fl_comid_str_2500m', 'fl_ftype_str_2500m', 'fl_gnis_id_str_2500m',\n",
       "       'fl_length_sum_2500m', 'fl_length_count_2500m', 'fl_length_mean_2500m',\n",
       "       'fl_ftype_str_vector_2500m', 'fl_ftype_artificialpath_1000m',\n",
       "       'fl_ftype_canalditch_1000m', 'fl_ftype_coastline_1000m',\n",
       "       'fl_ftype_connector_1000m', 'fl_ftype_pipeline_1000m',\n",
       "       'fl_ftype_streamriver_1000m', 'wb_ftype_str_vector_2500m',\n",
       "       'wb_ftype_artificialpath_2500m', 'wb_ftype_canalditch_2500m',\n",
       "       'wb_ftype_coastline_2500m', 'wb_ftype_connector_2500m',\n",
       "       'wb_ftype_pipeline_2500m', 'wb_ftype_streamriver_2500m'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('cwr_nwpr_' + str(2 * PATCH_SIZE) + 'm_nhd_variables_extracted_stripped').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "informational-failing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3218, 69)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-daughter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-samoa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
