{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "published-belgium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Madhukar/opt/miniconda3/envs/gee/bin/python\n",
      "3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 05:02:20) \n",
      "[Clang 11.0.1 ]\n",
      "sys.version_info(major=3, minor=9, micro=2, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "technical-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "mental-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the nhd addendum file\n",
    "nhd_stats = pd.read_csv(\"nhd_stats_AI.csv\")\n",
    "\n",
    "# read in csv file with SSURGO variables\n",
    "df_m = pd.read_csv(\"combined_regular_clean_with_ssurgo_variables.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "assisted-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle('../1. Extract NHD information from GEE/NHD_extracted_vars_2.5kmX2.5km_with_fcode_ftype/2.5kmX2.5km_nhd_variables_part1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "liable-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comid's from GEE are extracted into several pickled files\n",
    "# pickling is needed to be able to share easily across team members \n",
    "# join these pickled files into one full dataset\n",
    "\n",
    "df_merged_full = []\n",
    "\n",
    "for i in range(df_m.shape[0] // 500 + 1):\n",
    "    try:\n",
    "        df_temp = pd.read_pickle('../1. Extract NHD information from GEE/NHD_extracted_vars_200mX200m_with_fcode_ftype/200mX200m_nhd_variables_part' + str(500 * i + 1))\n",
    "#         print(df_temp.columns)\n",
    "        df_merged_full.append(df_temp)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "df_merged_full = pd.concat(df_merged_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "heard-opening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# pd.read_pickle('../1. Extract NHD information from GEE/NHD_extracted_vars_200mX200m_with_fcode_ftype/200mX200m_nhd_variables_part' + str(500 * i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "extraordinary-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert to protocol 3 so that can read in aws\n",
    "# df_merged_full = []\n",
    "\n",
    "# for i in range(df_m.shape[0] // 500 + 1):\n",
    "#     try:\n",
    "#         df_temp = pd.read_pickle(('NHD_extracted_vars_2.5kmX2.5km/combined_regular_clean_with_ssurgo_nhd_variables_part' + \n",
    "#                               str(500 * i + 1)))\n",
    "#         print(df_temp.shape)\n",
    "#         pickle.dump(df_temp, open(\"NHD_extracted_vars_2.5kmX2.5km/combined_regular_clean_with_ssurgo_nhd_variables_part\" + str(500 * i + 1) + \"_pkl3.pkl\",\"wb\"), protocol=3)\n",
    "#         df_temp2 = pd.read_pickle(('NHD_extracted_vars_2.5kmX2.5km/combined_regular_clean_with_ssurgo_nhd_variables_part' + \n",
    "#                               str(500 * i + 1)))\n",
    "\n",
    "#     except:\n",
    "#         break\n",
    "#     df_merged_full.append(df_temp2)\n",
    "# df_merged_full = pd.concat(df_merged_full)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "infrared-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this df into pickle\n",
    "import pickle\n",
    "pickle.dump(df_merged_full, open(\"200mX200m_nhd_variables_partsmerged\",\"wb\"), protocol=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-theater",
   "metadata": {},
   "source": [
    "# Read in merged pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "considered-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_full = pd.read_pickle(\"200mX200m_nhd_variables_partsmerged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adult-revelation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'jurisdiction_type', 'da_number',\n",
       "       'district', 'project_name', 'longitude', 'latitude',\n",
       "       'date_issued_or_denied', 'rha_determination', 'cwa_determination',\n",
       "       'rha1', 'rha2', 'cwa1', 'cwa2', 'cwa3', 'cwa4', 'cwa5', 'cwa6', 'cwa7',\n",
       "       'cwa8', 'cwa9', 'potential_wetland', 'index', 'Index', 'mukey',\n",
       "       'hydclprs', 'aws025wta', 'drclassdcd', 'nhd_vars_wb', 'nhd_vars_fl'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_full.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-title",
   "metadata": {},
   "source": [
    "# A word on 'nhd_vars_wb' and 'nhd_vars_fl' columns (see last two columns above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-gamma",
   "metadata": {},
   "source": [
    "## nhd_vars_wb: this is a list of lists\n",
    "- for each record, the following six features from GEE are extracted as lists and stored into a list\n",
    "- [comid_list, ftype_str, gnis_id, wb_area, fl_length, fcode]\n",
    "- the column is labeled nhd_vars_wb for waterbodies\n",
    "\n",
    "In a similar fashion, there is another column for flowlines labeled nhd_vars_fl\n",
    "\n",
    "Note: fl_length is NaN's for waterbodies and wb_area ae NaN's for flowlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-investor",
   "metadata": {},
   "source": [
    "## In the following, features are extracted from the above columns and feature engineered as discussed in meetings. Pls use your judgement to help devise any new features you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "prescribed-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wb for waterbodies\n",
    "# extract the individual lists\n",
    "df_merged_full[\"wb_comid_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_wb[0], axis=1)\n",
    "df_merged_full[\"wb_ftype_str_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_wb[1], axis=1)\n",
    "df_merged_full[\"wb_gnis_id_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_wb[2], axis=1)\n",
    "df_merged_full[\"wb_area_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_wb[3], axis=1)\n",
    "\n",
    "# fl for flowlines\n",
    "# extract the individual lists\n",
    "df_merged_full[\"fl_comid_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_fl[0], axis=1)\n",
    "df_merged_full[\"fl_ftype_str_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_fl[1], axis=1)\n",
    "df_merged_full[\"fl_gnis_id_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_fl[2], axis=1)\n",
    "df_merged_full[\"fl_length_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_fl[4], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-continent",
   "metadata": {},
   "source": [
    "# Lets look at columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bridal-tunnel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nhd_vars_wb</th>\n",
       "      <th>nhd_vars_fl</th>\n",
       "      <th>wb_comid_list</th>\n",
       "      <th>wb_ftype_str_list</th>\n",
       "      <th>wb_gnis_id_list</th>\n",
       "      <th>wb_area_list</th>\n",
       "      <th>fl_comid_list</th>\n",
       "      <th>fl_ftype_str_list</th>\n",
       "      <th>fl_gnis_id_list</th>\n",
       "      <th>fl_length_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>([120052831], [LakePond], [974076], [171.202],...</td>\n",
       "      <td>([21980217, 21978365], [ArtificialPath, Stream...</td>\n",
       "      <td>[120052831]</td>\n",
       "      <td>[LakePond]</td>\n",
       "      <td>[974076]</td>\n",
       "      <td>[171.202]</td>\n",
       "      <td>[21980217, 21978365]</td>\n",
       "      <td>[ArtificialPath, StreamRiver]</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[3.135, 3.557]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>([], [], [], [], [], [])</td>\n",
       "      <td>([], [], [], [], [], [])</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>([904140246], [LakePond], [970427], [6693.837]...</td>\n",
       "      <td>([21632389, 21635913], [Coastline, Coastline],...</td>\n",
       "      <td>[904140246]</td>\n",
       "      <td>[LakePond]</td>\n",
       "      <td>[970427]</td>\n",
       "      <td>[6693.837]</td>\n",
       "      <td>[21632389, 21635913]</td>\n",
       "      <td>[Coastline, Coastline]</td>\n",
       "      <td>[, ]</td>\n",
       "      <td>[1.895, 10.109]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>([], [], [], [], [], [])</td>\n",
       "      <td>([], [], [], [], [], [])</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>([], [], [], [], [], [])</td>\n",
       "      <td>([15560261], [CanalDitch], [], [nan], [0.95], ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[15560261]</td>\n",
       "      <td>[CanalDitch]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0.95]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>([], [], [], [], [], [])</td>\n",
       "      <td>([], [], [], [], [], [])</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          nhd_vars_wb  \\\n",
       "9   ([120052831], [LakePond], [974076], [171.202],...   \n",
       "10                           ([], [], [], [], [], [])   \n",
       "11  ([904140246], [LakePond], [970427], [6693.837]...   \n",
       "12                           ([], [], [], [], [], [])   \n",
       "13                           ([], [], [], [], [], [])   \n",
       "14                           ([], [], [], [], [], [])   \n",
       "\n",
       "                                          nhd_vars_fl wb_comid_list  \\\n",
       "9   ([21980217, 21978365], [ArtificialPath, Stream...   [120052831]   \n",
       "10                           ([], [], [], [], [], [])            []   \n",
       "11  ([21632389, 21635913], [Coastline, Coastline],...   [904140246]   \n",
       "12                           ([], [], [], [], [], [])            []   \n",
       "13  ([15560261], [CanalDitch], [], [nan], [0.95], ...            []   \n",
       "14                           ([], [], [], [], [], [])            []   \n",
       "\n",
       "   wb_ftype_str_list wb_gnis_id_list wb_area_list         fl_comid_list  \\\n",
       "9         [LakePond]        [974076]    [171.202]  [21980217, 21978365]   \n",
       "10                []              []           []                    []   \n",
       "11        [LakePond]        [970427]   [6693.837]  [21632389, 21635913]   \n",
       "12                []              []           []                    []   \n",
       "13                []              []           []            [15560261]   \n",
       "14                []              []           []                    []   \n",
       "\n",
       "                fl_ftype_str_list fl_gnis_id_list   fl_length_list  \n",
       "9   [ArtificialPath, StreamRiver]            [, ]   [3.135, 3.557]  \n",
       "10                             []              []               []  \n",
       "11         [Coastline, Coastline]            [, ]  [1.895, 10.109]  \n",
       "12                             []              []               []  \n",
       "13                   [CanalDitch]              []           [0.95]  \n",
       "14                             []              []               []  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_full[df_merged_full.columns[29:39]][9:15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-privilege",
   "metadata": {},
   "source": [
    "## Lets look at one row in individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "extra-gamma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([120052831], ['LakePond'], ['974076'], [171.202], [nan], [39004])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at waterbodies list\n",
    "df_merged_full.nhd_vars_wb[9]\n",
    "# you can see there are 6 items in the list [comid_list, ftype_str, gnis_id, wb_area, fl_length, fcode]\n",
    "# note that fcode is going to be null due to coding lapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "peripheral-advancement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([21980217, 21978365],\n",
       " ['ArtificialPath', 'StreamRiver'],\n",
       " ['', ''],\n",
       " [nan, nan],\n",
       " [3.135, 3.557],\n",
       " [55800, 46006])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at flowlines list\n",
    "df_merged_full.nhd_vars_fl[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "activated-wallet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[120052831]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of comids in waterbodies\n",
    "df_merged_full.wb_comid_list[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "altered-mandate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21980217, 21978365]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of comids in flowlines\n",
    "df_merged_full.fl_comid_list[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-future",
   "metadata": {},
   "source": [
    "### .... and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "continued-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Filter out invalid comids (although not used in this notebook)\n",
    "# # \"invalid\" = present in GEE but not present in nhd_stats\n",
    "\n",
    "# df_merged[\"wb_comid_list_filtered\"] = df_merged.apply(lambda x: [comid for comid in x.nhd_vars_wb[0] if comid in np.array(nhd_stats.comid)\n",
    "#                                                                 ], axis=1)\n",
    "\n",
    "# df_merged[\"fl_comid_list_filtered\"] = df_merged.apply(lambda x: [comid for comid in x.nhd_vars_fl[0] if comid in np.array(nhd_stats.comid)\n",
    "#                                                                 ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "immune-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigned begin and end of records for each person\n",
    "# MADHUKAR: records 1 - 5000\n",
    "# SHOBHA: records 5000 - 10000\n",
    "# RADHIKA: records 10000 - 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "naked-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features present in nhd_stats for corresponding comid\n",
    "# read in fl_comid_list, pull out matching variable values in nhd_stats\n",
    "\n",
    "df_merged = df_merged_full\n",
    "\n",
    "def extract_feature(comid, feature):\n",
    "    \"\"\"\n",
    "    Extract features present in nhd_stats for corresponding comid\n",
    "    \"\"\"\n",
    "    if comid == None:\n",
    "        return np.nan # if no comid's in GEE\n",
    "    extracted_feature = nhd_stats[nhd_stats[\"comid\"] == comid][str(feature)]\n",
    "    try:\n",
    "        extracted_feature = np.array(extracted_feature).item() \n",
    "    except Exception as e:\n",
    "        return np.nan # if comid in GEE but not in nhd database\n",
    "    return extracted_feature\n",
    "\n",
    "\n",
    "def extract_sum(feature):\n",
    "    \"\"\"\n",
    "    feature engineering per excel sheet\n",
    "    \"\"\"\n",
    "    return (df_merged.apply(lambda x: np.sum(np.array([extract_feature(comid, str(feature))\n",
    "                                                                 for comid in x.fl_comid_list])\n",
    "                                                       [~np.isnan(np.array([extract_feature(comid, str(feature))\n",
    "                                                                            for comid in x.fl_comid_list]))]), \n",
    "                                                axis=1))\n",
    "def extract_count(feature):\n",
    "    \"\"\"\n",
    "    feature engineering per excel sheet\n",
    "    \"\"\"\n",
    "    return (df_merged.apply(lambda x: len(np.array([extract_feature(comid, str(feature))\n",
    "                                                                 for comid in x.fl_comid_list])\n",
    "                                                       [~np.isnan(np.array([extract_feature(comid, str(feature))\n",
    "                                                                            for comid in x.fl_comid_list]))]), \n",
    "                                                  axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "departmental-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flowline variables\n",
    "\n",
    "# areasqkm\n",
    "df_merged[\"fl_areasqkm_sum\"] = extract_sum(\"areasqkm\")\n",
    "df_merged[\"fl_areasqkm_count\"] = extract_count(\"areasqkm\")\n",
    "df_merged[\"fl_areasqkm_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_areasqkm_sum/x.fl_areasqkm_count) \n",
    "                                                 if x.fl_areasqkm_count != 0 \n",
    "#                                                  else np.nan, axis=1)) # here you want to return 0\n",
    "                                                 else 0, axis=1))\n",
    "# gnis_name_ind\n",
    "df_merged[\"fl_gnis_name_ind_sum\"] = extract_sum(\"gnis_name_ind\")\n",
    "df_merged[\"fl_gnis_name_ind_count\"] = extract_count(\"gnis_name_ind\")\n",
    "df_merged[\"fl_gnis_name_ind_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_gnis_name_ind_sum/x.fl_gnis_name_ind_count) \n",
    "                                                 if x.fl_gnis_name_ind_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))\n",
    "\n",
    "# totdasqkm\n",
    "df_merged[\"fl_totdasqkm_sum\"] = extract_sum(\"totdasqkm\")\n",
    "df_merged[\"fl_totdasqkm_count\"] = extract_count(\"totdasqkm\")\n",
    "df_merged[\"fl_totdasqkm_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_totdasqkm_sum/x.fl_totdasqkm_count) \n",
    "                                                 if x.fl_totdasqkm_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))\n",
    "\n",
    "# flow_type\n",
    "df_merged[\"fl_flow_type_sum\"] = extract_sum(\"flow_type\")\n",
    "df_merged[\"fl_flow_type_count\"] = extract_count(\"flow_type\")\n",
    "df_merged[\"fl_flow_type_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_flow_type_sum/x.fl_flow_type_count) \n",
    "                                                 if x.fl_flow_type_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))\n",
    "\n",
    "                                                  \n",
    "# streamorde\n",
    "df_merged[\"fl_streamorde_sum\"] = extract_sum(\"streamorde\")\n",
    "df_merged[\"fl_streamorde_count\"] = extract_count(\"streamorde\")\n",
    "df_merged[\"fl_streamorde_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_streamorde_sum/x.fl_streamorde_count) \n",
    "                                                 if x.fl_streamorde_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))                                                   \n",
    "\n",
    "# intephem\n",
    "df_merged[\"fl_intephem_sum\"] = extract_sum(\"intephem\")\n",
    "df_merged[\"fl_intephem_count\"] = extract_count(\"intephem\")\n",
    "df_merged[\"fl_intephem_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_intephem_sum/x.fl_intephem_count) \n",
    "                                                 if x.fl_intephem_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))                                                 \n",
    "\n",
    "# startflag\n",
    "df_merged[\"fl_startflag_sum\"] = extract_sum(\"startflag\")\n",
    "df_merged[\"fl_startflag_count\"] = extract_count(\"startflag\")\n",
    "df_merged[\"fl_startflag_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_startflag_sum/x.fl_startflag_count) \n",
    "                                                 if x.fl_startflag_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))                                                  \n",
    "\n",
    "# divergence\n",
    "df_merged[\"fl_divergence_sum\"] = extract_sum(\"divergence\")\n",
    "df_merged[\"fl_divergence_count\"] = extract_count(\"divergence\")\n",
    "df_merged[\"fl_divergence_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_divergence_sum/x.fl_divergence_count) \n",
    "                                                 if x.fl_divergence_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-malta",
   "metadata": {},
   "source": [
    "# In a similar fasion, you can feature engineer the waterbodies (I will get that later today/tomorrow and push the updated dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "historical-mortality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>rha_determination</th>\n",
       "      <th>cwa_determination</th>\n",
       "      <th>rha1</th>\n",
       "      <th>rha2</th>\n",
       "      <th>cwa1</th>\n",
       "      <th>cwa2</th>\n",
       "      <th>...</th>\n",
       "      <th>fl_streamorde_mean</th>\n",
       "      <th>fl_intephem_sum</th>\n",
       "      <th>fl_intephem_count</th>\n",
       "      <th>fl_intephem_mean</th>\n",
       "      <th>fl_startflag_sum</th>\n",
       "      <th>fl_startflag_count</th>\n",
       "      <th>fl_startflag_mean</th>\n",
       "      <th>fl_divergence_sum</th>\n",
       "      <th>fl_divergence_count</th>\n",
       "      <th>fl_divergence_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7309.000000</td>\n",
       "      <td>7309.000000</td>\n",
       "      <td>-90.491852</td>\n",
       "      <td>37.209405</td>\n",
       "      <td>0.104932</td>\n",
       "      <td>0.369588</td>\n",
       "      <td>0.070935</td>\n",
       "      <td>0.062727</td>\n",
       "      <td>0.093235</td>\n",
       "      <td>0.063889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329004</td>\n",
       "      <td>0.059101</td>\n",
       "      <td>0.232779</td>\n",
       "      <td>0.054015</td>\n",
       "      <td>0.066968</td>\n",
       "      <td>0.206033</td>\n",
       "      <td>0.062722</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>0.206033</td>\n",
       "      <td>0.006733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4220.286128</td>\n",
       "      <td>4220.286128</td>\n",
       "      <td>15.418083</td>\n",
       "      <td>6.766997</td>\n",
       "      <td>0.306476</td>\n",
       "      <td>0.482710</td>\n",
       "      <td>0.256725</td>\n",
       "      <td>0.242479</td>\n",
       "      <td>0.290771</td>\n",
       "      <td>0.244564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994656</td>\n",
       "      <td>0.250179</td>\n",
       "      <td>0.551299</td>\n",
       "      <td>0.223712</td>\n",
       "      <td>0.252696</td>\n",
       "      <td>0.504558</td>\n",
       "      <td>0.239481</td>\n",
       "      <td>0.163613</td>\n",
       "      <td>0.504558</td>\n",
       "      <td>0.098222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-174.197880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3654.500000</td>\n",
       "      <td>3654.500000</td>\n",
       "      <td>-93.638850</td>\n",
       "      <td>32.815575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7309.000000</td>\n",
       "      <td>7309.000000</td>\n",
       "      <td>-88.186500</td>\n",
       "      <td>35.478300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10963.500000</td>\n",
       "      <td>10963.500000</td>\n",
       "      <td>-80.226955</td>\n",
       "      <td>41.403750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14618.000000</td>\n",
       "      <td>14618.000000</td>\n",
       "      <td>144.828420</td>\n",
       "      <td>70.483790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  Unnamed: 0.1     longitude      latitude  \\\n",
       "count  14619.000000  14619.000000  14619.000000  14619.000000   \n",
       "mean    7309.000000   7309.000000    -90.491852     37.209405   \n",
       "std     4220.286128   4220.286128     15.418083      6.766997   \n",
       "min        0.000000      0.000000   -174.197880      0.000000   \n",
       "25%     3654.500000   3654.500000    -93.638850     32.815575   \n",
       "50%     7309.000000   7309.000000    -88.186500     35.478300   \n",
       "75%    10963.500000  10963.500000    -80.226955     41.403750   \n",
       "max    14618.000000  14618.000000    144.828420     70.483790   \n",
       "\n",
       "       rha_determination  cwa_determination          rha1          rha2  \\\n",
       "count       14619.000000       14619.000000  14619.000000  14619.000000   \n",
       "mean            0.104932           0.369588      0.070935      0.062727   \n",
       "std             0.306476           0.482710      0.256725      0.242479   \n",
       "min             0.000000           0.000000      0.000000      0.000000   \n",
       "25%             0.000000           0.000000      0.000000      0.000000   \n",
       "50%             0.000000           0.000000      0.000000      0.000000   \n",
       "75%             0.000000           1.000000      0.000000      0.000000   \n",
       "max             1.000000           1.000000      1.000000      1.000000   \n",
       "\n",
       "               cwa1          cwa2  ...  fl_streamorde_mean  fl_intephem_sum  \\\n",
       "count  14619.000000  14619.000000  ...        14619.000000     14619.000000   \n",
       "mean       0.093235      0.063889  ...            0.329004         0.059101   \n",
       "std        0.290771      0.244564  ...            0.994656         0.250179   \n",
       "min        0.000000      0.000000  ...            0.000000         0.000000   \n",
       "25%        0.000000      0.000000  ...            0.000000         0.000000   \n",
       "50%        0.000000      0.000000  ...            0.000000         0.000000   \n",
       "75%        0.000000      0.000000  ...            0.000000         0.000000   \n",
       "max        1.000000      1.000000  ...           10.000000         3.000000   \n",
       "\n",
       "       fl_intephem_count  fl_intephem_mean  fl_startflag_sum  \\\n",
       "count       14619.000000      14619.000000      14619.000000   \n",
       "mean            0.232779          0.054015          0.066968   \n",
       "std             0.551299          0.223712          0.252696   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             0.000000          0.000000          0.000000   \n",
       "50%             0.000000          0.000000          0.000000   \n",
       "75%             0.000000          0.000000          0.000000   \n",
       "max             7.000000          1.000000          2.000000   \n",
       "\n",
       "       fl_startflag_count  fl_startflag_mean  fl_divergence_sum  \\\n",
       "count        14619.000000       14619.000000       14619.000000   \n",
       "mean             0.206033           0.062722           0.010739   \n",
       "std              0.504558           0.239481           0.163613   \n",
       "min              0.000000           0.000000           0.000000   \n",
       "25%              0.000000           0.000000           0.000000   \n",
       "50%              0.000000           0.000000           0.000000   \n",
       "75%              0.000000           0.000000           0.000000   \n",
       "max              6.000000           1.000000           6.000000   \n",
       "\n",
       "       fl_divergence_count  fl_divergence_mean  \n",
       "count         14619.000000        14619.000000  \n",
       "mean              0.206033            0.006733  \n",
       "std               0.504558            0.098222  \n",
       "min               0.000000            0.000000  \n",
       "25%               0.000000            0.000000  \n",
       "50%               0.000000            0.000000  \n",
       "75%               0.000000            0.000000  \n",
       "max               6.000000            2.000000  \n",
       "\n",
       "[8 rows x 47 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "medieval-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the variables that needs to be made accessible to manipulation via csv files\n",
    "# wb_ftype_str_list\n",
    "# wb_area_list\n",
    "# fl_ftype_str_list\n",
    "# fl_length_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "adaptive-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged\n",
    "\n",
    "# wb for waterbodies\n",
    "# extract the individual lists\n",
    "df[\"wb_comid_str\"] = df.apply(lambda x: \"+\".join([str(comid) for comid in x.nhd_vars_wb[0]]), axis=1)\n",
    "df[\"wb_ftype_str\"] = df.apply(lambda x: \"+\".join([str(ftype) for ftype in x.nhd_vars_wb[1] if ftype != None]), axis=1)\n",
    "df[\"wb_gnis_id_str\"] = df.apply(lambda x: \"+\".join([str(gnis) for gnis in x.nhd_vars_wb[2]]), axis=1)\n",
    "\n",
    "# # sum and mean\n",
    "df[\"wb_area_sum\"] = df.apply(lambda x: np.sum(np.array([area for area in x.nhd_vars_wb[3] if area != None])), axis=1)\n",
    "df[\"wb_area_count\"] = df.apply(lambda x: len([area for area in x.nhd_vars_wb[3] if area != None]), axis=1)\n",
    "# df[\"wb_area_mean\"] = df.apply(lambda x: (x.wb_area_sum / x.wb_area_count) if x.wb_area_count != 0 else np.nan, axis=1)\n",
    "df[\"wb_area_mean\"] = df.apply(lambda x: (x.wb_area_sum / x.wb_area_count) if x.wb_area_count != 0 else 0, axis=1)\n",
    "\n",
    "df[\"wb_gnis_name_ind_sum\"] = df.apply(lambda x: np.sum(np.array([int(gnis) for gnis in x.nhd_vars_wb[2] if gnis not in [\"\", None]])), axis=1)\n",
    "df[\"wb_gnis_name_ind_count\"] = df.apply(lambda x: len([gnis for gnis in x.nhd_vars_wb[2] if gnis not in [\"\", None]]), axis=1)\n",
    "# df[\"wb_gnis_name_ind_mean\"] = df.apply(lambda x: (x.wb_gnis_name_ind_sum / x.wb_gnis_name_ind_count) if x.wb_gnis_name_ind_count != 0 else np.nan, axis=1)\n",
    "df[\"wb_gnis_name_ind_mean\"] = df.apply(lambda x: (x.wb_gnis_name_ind_sum / x.wb_gnis_name_ind_count) if x.wb_gnis_name_ind_count != 0 else 0, axis=1)\n",
    "\n",
    "\n",
    "# # fl for flowlines\n",
    "# # extract the individual lists\n",
    "df[\"fl_comid_str\"] = df.apply(lambda x: \"+\".join([str(comid) for comid in x.nhd_vars_fl[0]]), axis=1)\n",
    "df[\"fl_ftype_str\"] = df.apply(lambda x: \"+\".join([str(ftype) for ftype in x.nhd_vars_fl[1]]), axis=1)\n",
    "df[\"fl_gnis_id_str\"] = df.apply(lambda x: \"+\".join(x.nhd_vars_fl[2]), axis=1)\n",
    "\n",
    "# # sum and mean\n",
    "df[\"fl_length_sum\"] = df.apply(lambda x: np.sum(np.array([length for length in x.nhd_vars_wb[4] if length != None])), axis=1)\n",
    "df[\"fl_length_count\"] = df.apply(lambda x: len([length for length in x.nhd_vars_wb[4] if length != None]), axis=1)\n",
    "# df[\"fl_length_mean\"] = df.apply(lambda x: (x.fl_length_sum / x.fl_length_count) if x.fl_length_count != 0 else np.nan, axis=1)\n",
    "df[\"fl_length_mean\"] = df.apply(lambda x: (x.fl_length_sum / x.fl_length_count) if x.fl_length_count != 0 else 0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "formed-austin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'jurisdiction_type', 'da_number',\n",
       "       'district', 'project_name', 'longitude', 'latitude',\n",
       "       'date_issued_or_denied', 'rha_determination', 'cwa_determination',\n",
       "       'rha1', 'rha2', 'cwa1', 'cwa2', 'cwa3', 'cwa4', 'cwa5', 'cwa6', 'cwa7',\n",
       "       'cwa8', 'cwa9', 'potential_wetland', 'index', 'Index', 'mukey',\n",
       "       'hydclprs', 'aws025wta', 'drclassdcd', 'nhd_vars_wb', 'nhd_vars_fl',\n",
       "       'wb_comid_list', 'wb_ftype_str_list', 'wb_gnis_id_list', 'wb_area_list',\n",
       "       'fl_comid_list', 'fl_ftype_str_list', 'fl_gnis_id_list',\n",
       "       'fl_length_list', 'fl_areasqkm_sum', 'fl_areasqkm_count',\n",
       "       'fl_areasqkm_mean', 'fl_gnis_name_ind_sum', 'fl_gnis_name_ind_count',\n",
       "       'fl_gnis_name_ind_mean', 'fl_totdasqkm_sum', 'fl_totdasqkm_count',\n",
       "       'fl_totdasqkm_mean', 'fl_flow_type_sum', 'fl_flow_type_count',\n",
       "       'fl_flow_type_mean', 'fl_streamorde_sum', 'fl_streamorde_count',\n",
       "       'fl_streamorde_mean', 'fl_intephem_sum', 'fl_intephem_count',\n",
       "       'fl_intephem_mean', 'fl_startflag_sum', 'fl_startflag_count',\n",
       "       'fl_startflag_mean', 'fl_divergence_sum', 'fl_divergence_count',\n",
       "       'fl_divergence_mean', 'wb_comid_str', 'wb_ftype_str', 'wb_gnis_id_str',\n",
       "       'wb_area_sum', 'wb_area_count', 'wb_area_mean', 'wb_gnis_name_ind_sum',\n",
       "       'wb_gnis_name_ind_count', 'wb_gnis_name_ind_mean', 'fl_comid_str',\n",
       "       'fl_ftype_str', 'fl_gnis_id_str', 'fl_length_sum', 'fl_length_count',\n",
       "       'fl_length_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "personal-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# use protocol 3 for backwards compatibility with Python 3.6 on AWS\n",
    "pickle.dump(df, open(\"200mX200m_nhd_variables_extracted\",\"wb\"), protocol=3)\n",
    "df.to_csv(\"200mX200m_nhd_variables_extracted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "necessary-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged.to_pickle(\"combined_regular_with_ssurgo_nhd_2.5kmX2.5km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ethical-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"combined_regular_with_ssurgo_nhd_2.5kmX2.5km.csv\")\n",
    "# df.to_pickle(\"combined_regular_with_ssurgo_nhd_2.5kmX2.5km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "loaded-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"combined_regular_with_ssurgo_nhd_200mX200m.csv\")\n",
    "# df.to_pickle(\"combined_regular_with_ssurgo_nhd_200mX200m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-rental",
   "metadata": {},
   "source": [
    "# Strip ftype list into individual columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-insert",
   "metadata": {},
   "source": [
    "## replace 200m by 2.5km and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "atmospheric-median",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>rha_determination</th>\n",
       "      <th>cwa_determination</th>\n",
       "      <th>rha1</th>\n",
       "      <th>rha2</th>\n",
       "      <th>cwa1</th>\n",
       "      <th>cwa2</th>\n",
       "      <th>...</th>\n",
       "      <th>fl_divergence_mean</th>\n",
       "      <th>wb_area_sum</th>\n",
       "      <th>wb_area_count</th>\n",
       "      <th>wb_area_mean</th>\n",
       "      <th>wb_gnis_name_ind_sum</th>\n",
       "      <th>wb_gnis_name_ind_count</th>\n",
       "      <th>wb_gnis_name_ind_mean</th>\n",
       "      <th>fl_length_sum</th>\n",
       "      <th>fl_length_count</th>\n",
       "      <th>fl_length_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>1.461900e+04</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>1.461900e+04</td>\n",
       "      <td>13325.0</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>13325.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7309.000000</td>\n",
       "      <td>7309.000000</td>\n",
       "      <td>-90.491852</td>\n",
       "      <td>37.209405</td>\n",
       "      <td>0.104932</td>\n",
       "      <td>0.369588</td>\n",
       "      <td>0.070935</td>\n",
       "      <td>0.062727</td>\n",
       "      <td>0.093235</td>\n",
       "      <td>0.063889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006733</td>\n",
       "      <td>139.840426</td>\n",
       "      <td>0.065052</td>\n",
       "      <td>139.377298</td>\n",
       "      <td>3.175971e+04</td>\n",
       "      <td>0.030029</td>\n",
       "      <td>3.146162e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095219</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4220.286128</td>\n",
       "      <td>4220.286128</td>\n",
       "      <td>15.418083</td>\n",
       "      <td>6.766997</td>\n",
       "      <td>0.306476</td>\n",
       "      <td>0.482710</td>\n",
       "      <td>0.256725</td>\n",
       "      <td>0.242479</td>\n",
       "      <td>0.290771</td>\n",
       "      <td>0.244564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098222</td>\n",
       "      <td>2420.182459</td>\n",
       "      <td>0.268155</td>\n",
       "      <td>2420.056593</td>\n",
       "      <td>1.994140e+05</td>\n",
       "      <td>0.172667</td>\n",
       "      <td>1.969949e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317055</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-174.197880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3654.500000</td>\n",
       "      <td>3654.500000</td>\n",
       "      <td>-93.638850</td>\n",
       "      <td>32.815575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7309.000000</td>\n",
       "      <td>7309.000000</td>\n",
       "      <td>-88.186500</td>\n",
       "      <td>35.478300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10963.500000</td>\n",
       "      <td>10963.500000</td>\n",
       "      <td>-80.226955</td>\n",
       "      <td>41.403750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14618.000000</td>\n",
       "      <td>14618.000000</td>\n",
       "      <td>144.828420</td>\n",
       "      <td>70.483790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>57516.647000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>57516.647000</td>\n",
       "      <td>3.371068e+06</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.863850e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  Unnamed: 0.1     longitude      latitude  \\\n",
       "count  14619.000000  14619.000000  14619.000000  14619.000000   \n",
       "mean    7309.000000   7309.000000    -90.491852     37.209405   \n",
       "std     4220.286128   4220.286128     15.418083      6.766997   \n",
       "min        0.000000      0.000000   -174.197880      0.000000   \n",
       "25%     3654.500000   3654.500000    -93.638850     32.815575   \n",
       "50%     7309.000000   7309.000000    -88.186500     35.478300   \n",
       "75%    10963.500000  10963.500000    -80.226955     41.403750   \n",
       "max    14618.000000  14618.000000    144.828420     70.483790   \n",
       "\n",
       "       rha_determination  cwa_determination          rha1          rha2  \\\n",
       "count       14619.000000       14619.000000  14619.000000  14619.000000   \n",
       "mean            0.104932           0.369588      0.070935      0.062727   \n",
       "std             0.306476           0.482710      0.256725      0.242479   \n",
       "min             0.000000           0.000000      0.000000      0.000000   \n",
       "25%             0.000000           0.000000      0.000000      0.000000   \n",
       "50%             0.000000           0.000000      0.000000      0.000000   \n",
       "75%             0.000000           1.000000      0.000000      0.000000   \n",
       "max             1.000000           1.000000      1.000000      1.000000   \n",
       "\n",
       "               cwa1          cwa2  ...  fl_divergence_mean   wb_area_sum  \\\n",
       "count  14619.000000  14619.000000  ...        14619.000000  14619.000000   \n",
       "mean       0.093235      0.063889  ...            0.006733    139.840426   \n",
       "std        0.290771      0.244564  ...            0.098222   2420.182459   \n",
       "min        0.000000      0.000000  ...            0.000000      0.000000   \n",
       "25%        0.000000      0.000000  ...            0.000000      0.000000   \n",
       "50%        0.000000      0.000000  ...            0.000000      0.000000   \n",
       "75%        0.000000      0.000000  ...            0.000000      0.000000   \n",
       "max        1.000000      1.000000  ...            2.000000  57516.647000   \n",
       "\n",
       "       wb_area_count  wb_area_mean  wb_gnis_name_ind_sum  \\\n",
       "count   14619.000000  14619.000000          1.461900e+04   \n",
       "mean        0.065052    139.377298          3.175971e+04   \n",
       "std         0.268155   2420.056593          1.994140e+05   \n",
       "min         0.000000      0.000000          0.000000e+00   \n",
       "25%         0.000000      0.000000          0.000000e+00   \n",
       "50%         0.000000      0.000000          0.000000e+00   \n",
       "75%         0.000000      0.000000          0.000000e+00   \n",
       "max         3.000000  57516.647000          3.371068e+06   \n",
       "\n",
       "       wb_gnis_name_ind_count  wb_gnis_name_ind_mean  fl_length_sum  \\\n",
       "count            14619.000000           1.461900e+04        13325.0   \n",
       "mean                 0.030029           3.146162e+04            0.0   \n",
       "std                  0.172667           1.969949e+05            0.0   \n",
       "min                  0.000000           0.000000e+00            0.0   \n",
       "25%                  0.000000           0.000000e+00            0.0   \n",
       "50%                  0.000000           0.000000e+00            0.0   \n",
       "75%                  0.000000           0.000000e+00            0.0   \n",
       "max                  2.000000           1.863850e+06            0.0   \n",
       "\n",
       "       fl_length_count  fl_length_mean  \n",
       "count     14619.000000         13325.0  \n",
       "mean          0.095219             0.0  \n",
       "std           0.317055             0.0  \n",
       "min           0.000000             0.0  \n",
       "25%           0.000000             0.0  \n",
       "50%           0.000000             0.0  \n",
       "75%           0.000000             0.0  \n",
       "max           3.000000             0.0  \n",
       "\n",
       "[8 rows x 56 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"combined_regular_with_ssurgo_nhd_2.5kmX2.5km_pkl3.pkl\", \"rb\") as f:\n",
    "#     df_readfrompkl = pickle.load(f)\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# df = pd.read_pickle(\"2.5kmX2.5km_nhd_variables_extracted\")\n",
    "df = pd.read_pickle(\"200mX200m_nhd_variables_extracted\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "reported-mattress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'jurisdiction_type', 'da_number',\n",
       "       'district', 'project_name', 'longitude', 'latitude',\n",
       "       'date_issued_or_denied', 'rha_determination', 'cwa_determination',\n",
       "       'rha1', 'rha2', 'cwa1', 'cwa2', 'cwa3', 'cwa4', 'cwa5', 'cwa6', 'cwa7',\n",
       "       'cwa8', 'cwa9', 'potential_wetland', 'index', 'Index', 'mukey',\n",
       "       'hydclprs', 'aws025wta', 'drclassdcd', 'nhd_vars_wb', 'nhd_vars_fl',\n",
       "       'wb_comid_list', 'wb_ftype_str_list', 'wb_gnis_id_list', 'wb_area_list',\n",
       "       'fl_comid_list', 'fl_ftype_str_list', 'fl_gnis_id_list',\n",
       "       'fl_length_list', 'fl_areasqkm_sum', 'fl_areasqkm_count',\n",
       "       'fl_areasqkm_mean', 'fl_gnis_name_ind_sum', 'fl_gnis_name_ind_count',\n",
       "       'fl_gnis_name_ind_mean', 'fl_totdasqkm_sum', 'fl_totdasqkm_count',\n",
       "       'fl_totdasqkm_mean', 'fl_flow_type_sum', 'fl_flow_type_count',\n",
       "       'fl_flow_type_mean', 'fl_streamorde_sum', 'fl_streamorde_count',\n",
       "       'fl_streamorde_mean', 'fl_intephem_sum', 'fl_intephem_count',\n",
       "       'fl_intephem_mean', 'fl_startflag_sum', 'fl_startflag_count',\n",
       "       'fl_startflag_mean', 'fl_divergence_sum', 'fl_divergence_count',\n",
       "       'fl_divergence_mean', 'wb_comid_str', 'wb_ftype_str', 'wb_gnis_id_str',\n",
       "       'wb_area_sum', 'wb_area_count', 'wb_area_mean', 'wb_gnis_name_ind_sum',\n",
       "       'wb_gnis_name_ind_count', 'wb_gnis_name_ind_mean', 'fl_comid_str',\n",
       "       'fl_ftype_str', 'fl_gnis_id_str', 'fl_length_sum', 'fl_length_count',\n",
       "       'fl_length_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "retained-execution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'jurisdiction_type', 'da_number',\n",
       "       'district', 'project_name', 'longitude', 'latitude',\n",
       "       'date_issued_or_denied', 'rha_determination', 'cwa_determination',\n",
       "       'rha1', 'rha2', 'cwa1', 'cwa2', 'cwa3', 'cwa4', 'cwa5', 'cwa6', 'cwa7',\n",
       "       'cwa8', 'cwa9', 'potential_wetland', 'index', 'Index', 'mukey',\n",
       "       'hydclprs', 'aws025wta', 'drclassdcd', 'nhd_vars_wb_200m',\n",
       "       'nhd_vars_fl_200m', 'wb_comid_list_200m', 'wb_ftype_str_list_200m',\n",
       "       'wb_gnis_id_list_200m', 'wb_area_list_200m', 'fl_comid_list_200m',\n",
       "       'fl_ftype_str_list_200m', 'fl_gnis_id_list_200m', 'fl_length_list_200m',\n",
       "       'fl_areasqkm_sum_200m', 'fl_areasqkm_count_200m',\n",
       "       'fl_areasqkm_mean_200m', 'fl_gnis_name_ind_sum_200m',\n",
       "       'fl_gnis_name_ind_count_200m', 'fl_gnis_name_ind_mean_200m',\n",
       "       'fl_totdasqkm_sum_200m', 'fl_totdasqkm_count_200m',\n",
       "       'fl_totdasqkm_mean_200m', 'fl_flow_type_sum_200m',\n",
       "       'fl_flow_type_count_200m', 'fl_flow_type_mean_200m',\n",
       "       'fl_streamorde_sum_200m', 'fl_streamorde_count_200m',\n",
       "       'fl_streamorde_mean_200m', 'fl_intephem_sum_200m',\n",
       "       'fl_intephem_count_200m', 'fl_intephem_mean_200m',\n",
       "       'fl_startflag_sum_200m', 'fl_startflag_count_200m',\n",
       "       'fl_startflag_mean_200m', 'fl_divergence_sum_200m',\n",
       "       'fl_divergence_count_200m', 'fl_divergence_mean_200m',\n",
       "       'wb_comid_str_200m', 'wb_ftype_str_200m', 'wb_gnis_id_str_200m',\n",
       "       'wb_area_sum_200m', 'wb_area_count_200m', 'wb_area_mean_200m',\n",
       "       'wb_gnis_name_ind_sum_200m', 'wb_gnis_name_ind_count_200m',\n",
       "       'wb_gnis_name_ind_mean_200m', 'fl_comid_str_200m', 'fl_ftype_str_200m',\n",
       "       'fl_gnis_id_str_200m', 'fl_length_sum_200m', 'fl_length_count_200m',\n",
       "       'fl_length_mean_200m'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_columns = (['nhd_vars_wb', 'nhd_vars_fl',\n",
    "       'wb_comid_list', 'wb_ftype_str_list', 'wb_gnis_id_list', 'wb_area_list',\n",
    "       'fl_comid_list', 'fl_ftype_str_list', 'fl_gnis_id_list',\n",
    "       'fl_length_list', 'fl_areasqkm_sum', 'fl_areasqkm_count',\n",
    "       'fl_areasqkm_mean', 'fl_gnis_name_ind_sum', 'fl_gnis_name_ind_count',\n",
    "       'fl_gnis_name_ind_mean', 'fl_totdasqkm_sum', 'fl_totdasqkm_count',\n",
    "       'fl_totdasqkm_mean', 'fl_flow_type_sum', 'fl_flow_type_count',\n",
    "       'fl_flow_type_mean', 'fl_streamorde_sum', 'fl_streamorde_count',\n",
    "       'fl_streamorde_mean', 'fl_intephem_sum', 'fl_intephem_count',\n",
    "       'fl_intephem_mean', 'fl_startflag_sum', 'fl_startflag_count',\n",
    "       'fl_startflag_mean', 'fl_divergence_sum', 'fl_divergence_count',\n",
    "       'fl_divergence_mean', 'wb_comid_str', 'wb_ftype_str', 'wb_gnis_id_str',\n",
    "       'wb_area_sum', 'wb_area_count', 'wb_area_mean', 'wb_gnis_name_ind_sum',\n",
    "       'wb_gnis_name_ind_count', 'wb_gnis_name_ind_mean', 'fl_comid_str',\n",
    "       'fl_ftype_str', 'fl_gnis_id_str', 'fl_length_sum', 'fl_length_count',\n",
    "       'fl_length_mean'])\n",
    "\n",
    "\n",
    "new_columns = [column + \"_200m\" for column in old_columns]\n",
    "new_columns\n",
    "\n",
    "col_name_dict = dict(zip(old_columns, new_columns))\n",
    "\n",
    "\n",
    "df.rename(columns=col_name_dict, inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "different-pregnancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArtificialPath', 'StreamRiver']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fl_ftype_str_list_200m[9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "grateful-watson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comid', 'long_comid', 'lat_comid', 'startflag', 'intephem',\n",
       "       'divergence', 'streamorde', 'lengthkm', 'gnis_name_ind', 'areasqkm',\n",
       "       'totdasqkm', 'flow_type', 'distup_max', 'distdown_max'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nhd_stats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "based-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # attribute_features = ([\"ATTRIBUTE\",\"SYSTEM_NAME\"])\n",
    "\n",
    "# def feature_vector(df_attr=df, feature=\"fl_ftype_str_list_2500m\"):\n",
    "#     \"\"\"\n",
    "    \n",
    "#     Returns the vector for each feature of an attribute\n",
    "    \n",
    "#     input: NWI Code Definition as df_attr to find unique values for a featurE & feature name \n",
    "#     output: feature vector for that (attribute, feature) set\n",
    "    \n",
    "#     \"\"\"\n",
    "#     # find unique values for a give feature\n",
    "#     feature_list = list(df_attr[feature].unique())\n",
    "#     # sort\n",
    "#     feature_list.sort()\n",
    "#     # lower case and replace spaces by _\n",
    "#     feature_list = [feature.lower().replace(\" \", \"_\") for feature in feature_list]\n",
    "    \n",
    "#     return feature_list\n",
    "#     # create a dict where the order is maintained. Initialize values to 0\n",
    "#     feature_dict = OrderedDict(zip(feature_list, [0] * len(feature_list)))\n",
    "    \n",
    "#     # you would like to actually create a new column with this frozen feature_list\n",
    "#     df_attr[feature + \"_\" + \"uniques_list\"] = df_attr.apply(lambda x: feature_list, axis=1)\n",
    "#     return df_attr\n",
    "    \n",
    "# feature_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "placed-teach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fl_ftype_str_list_200m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14614</th>\n",
       "      <td>[StreamRiver]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14615</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14616</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14617</th>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14618</th>\n",
       "      <td>[StreamRiver]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14619 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fl_ftype_str_list_200m\n",
       "0                         []\n",
       "1                         []\n",
       "2                         []\n",
       "3                         []\n",
       "4                         []\n",
       "...                      ...\n",
       "14614          [StreamRiver]\n",
       "14615                     []\n",
       "14616                     []\n",
       "14617                     []\n",
       "14618          [StreamRiver]\n",
       "\n",
       "[14619 rows x 1 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[\"fl_ftype_str_list_200m\"].unique()\n",
    "pd.DataFrame(df.fl_ftype_str_list_200m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "valuable-saint",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('artificialpath', 0), ('canalditch', 0), ('coastline', 0), ('connector', 0), ('pipeline', 0), ('streamriver', 0)])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "\n",
    "unique_ftype_dict = defaultdict(int)\n",
    "\n",
    "for row in range(df.shape[0]):\n",
    "    for ftype in df.loc[row,\"fl_ftype_str_list_200m\"]:\n",
    "        unique_ftype_dict[ftype.lower()] += 1\n",
    "        \n",
    "unique_ftype_dict.keys()\n",
    "sorted_unique_ftype_list = sorted(tuple(unique_ftype_dict.keys()))\n",
    "\n",
    "ordered_ftype_dict = OrderedDict()\n",
    "for key in sorted_unique_ftype_list:\n",
    "    ordered_ftype_dict[key] = 0\n",
    "    \n",
    "ordered_ftype_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "broadband-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_unique_ftype_list = sorted(tuple(unique_ftype_dict.keys()))\n",
    "\n",
    "def create_ftype_vector(ftype_list):\n",
    "    ftype_dict = OrderedDict(zip(sorted_unique_ftype_list, [0] * len(sorted_unique_ftype_list)))    \n",
    "    \n",
    "    for ftype in ftype_list:\n",
    "        ftype_dict[ftype.lower()] += 1\n",
    "\n",
    "    return list(ftype_dict.values())\n",
    "    \n",
    "    \n",
    "\n",
    "df[\"fl_ftype_str_vector_200m\"] = df.apply(lambda x: create_ftype_vector(x.fl_ftype_str_list_200m), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "contained-croatia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fl_ftype_str_vector_200m[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "velvet-stack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ArtificialPath', 'StreamRiver']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[9, \"fl_ftype_str_list_200m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "crucial-sense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fl_ftype_str_vector_200m[9][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "successful-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, ftype in enumerate(sorted_unique_ftype_list):\n",
    "    df[\"fl_ftype_\" + ftype + \"_200m\"] = df.apply(lambda x: x.fl_ftype_str_vector_200m[count], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "portuguese-moore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'jurisdiction_type', 'da_number',\n",
       "       'district', 'project_name', 'longitude', 'latitude',\n",
       "       'date_issued_or_denied', 'rha_determination', 'cwa_determination',\n",
       "       'rha1', 'rha2', 'cwa1', 'cwa2', 'cwa3', 'cwa4', 'cwa5', 'cwa6', 'cwa7',\n",
       "       'cwa8', 'cwa9', 'potential_wetland', 'index', 'Index', 'mukey',\n",
       "       'hydclprs', 'aws025wta', 'drclassdcd', 'nhd_vars_wb_200m',\n",
       "       'nhd_vars_fl_200m', 'wb_comid_list_200m', 'wb_ftype_str_list_200m',\n",
       "       'wb_gnis_id_list_200m', 'wb_area_list_200m', 'fl_comid_list_200m',\n",
       "       'fl_ftype_str_list_200m', 'fl_gnis_id_list_200m', 'fl_length_list_200m',\n",
       "       'fl_areasqkm_sum_200m', 'fl_areasqkm_count_200m',\n",
       "       'fl_areasqkm_mean_200m', 'fl_gnis_name_ind_sum_200m',\n",
       "       'fl_gnis_name_ind_count_200m', 'fl_gnis_name_ind_mean_200m',\n",
       "       'fl_totdasqkm_sum_200m', 'fl_totdasqkm_count_200m',\n",
       "       'fl_totdasqkm_mean_200m', 'fl_flow_type_sum_200m',\n",
       "       'fl_flow_type_count_200m', 'fl_flow_type_mean_200m',\n",
       "       'fl_streamorde_sum_200m', 'fl_streamorde_count_200m',\n",
       "       'fl_streamorde_mean_200m', 'fl_intephem_sum_200m',\n",
       "       'fl_intephem_count_200m', 'fl_intephem_mean_200m',\n",
       "       'fl_startflag_sum_200m', 'fl_startflag_count_200m',\n",
       "       'fl_startflag_mean_200m', 'fl_divergence_sum_200m',\n",
       "       'fl_divergence_count_200m', 'fl_divergence_mean_200m',\n",
       "       'wb_comid_str_200m', 'wb_ftype_str_200m', 'wb_gnis_id_str_200m',\n",
       "       'wb_area_sum_200m', 'wb_area_count_200m', 'wb_area_mean_200m',\n",
       "       'wb_gnis_name_ind_sum_200m', 'wb_gnis_name_ind_count_200m',\n",
       "       'wb_gnis_name_ind_mean_200m', 'fl_comid_str_200m', 'fl_ftype_str_200m',\n",
       "       'fl_gnis_id_str_200m', 'fl_length_sum_200m', 'fl_length_count_200m',\n",
       "       'fl_length_mean_200m', 'fl_ftype_str_vector_200m',\n",
       "       'fl_ftype_artificialpath_200m', 'fl_ftype_canalditch_200m',\n",
       "       'fl_ftype_coastline_200m', 'fl_ftype_connector_200m',\n",
       "       'fl_ftype_pipeline_200m', 'fl_ftype_streamriver_200m'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-berry",
   "metadata": {},
   "source": [
    "# Strip wb_ftype list into individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "equipped-hygiene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('estuary', 0), ('lakepond', 0), ('playa', 0), ('reservoir', 0), ('swampmarsh', 0)])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import defaultdict\n",
    "\n",
    "wb_unique_ftype_dict = defaultdict(int)\n",
    "\n",
    "for row in range(df.shape[0]):\n",
    "    for ftype in df.loc[row,\"wb_ftype_str_list_200m\"]:\n",
    "        wb_unique_ftype_dict[ftype.lower()] += 1\n",
    "        \n",
    "wb_unique_ftype_dict.keys()\n",
    "wb_sorted_unique_ftype_list = sorted(tuple(wb_unique_ftype_dict.keys()))\n",
    "\n",
    "wb_ordered_ftype_dict = OrderedDict()\n",
    "for key in wb_sorted_unique_ftype_list:\n",
    "    wb_ordered_ftype_dict[key] = 0\n",
    "    \n",
    "wb_ordered_ftype_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bulgarian-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_sorted_unique_ftype_list = sorted(tuple(wb_unique_ftype_dict.keys()))\n",
    "\n",
    "def wb_create_ftype_vector(ftype_list):\n",
    "    ftype_dict = OrderedDict(zip(wb_sorted_unique_ftype_list, [0] * len(wb_sorted_unique_ftype_list)))    \n",
    "    \n",
    "    for ftype in ftype_list:\n",
    "        ftype_dict[ftype.lower()] += 1\n",
    "\n",
    "    return list(ftype_dict.values())\n",
    "    \n",
    "    \n",
    "\n",
    "df[\"wb_ftype_str_vector_200m\"] = df.apply(lambda x: wb_create_ftype_vector(x.wb_ftype_str_list_200m), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "rural-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_vector(ftype_vector, count):\n",
    "    try:\n",
    "        return ftype_vector[count]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "for count, ftype in enumerate(sorted_unique_ftype_list):\n",
    "    df[\"wb_ftype_\" + ftype + \"_200m\"] = df.apply(lambda x: strip_vector(x.wb_ftype_str_vector_200m, count), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "comfortable-archive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'jurisdiction_type', 'da_number',\n",
       "       'district', 'project_name', 'longitude', 'latitude',\n",
       "       'date_issued_or_denied', 'rha_determination', 'cwa_determination',\n",
       "       'rha1', 'rha2', 'cwa1', 'cwa2', 'cwa3', 'cwa4', 'cwa5', 'cwa6', 'cwa7',\n",
       "       'cwa8', 'cwa9', 'potential_wetland', 'index', 'Index', 'mukey',\n",
       "       'hydclprs', 'aws025wta', 'drclassdcd', 'nhd_vars_wb_200m',\n",
       "       'nhd_vars_fl_200m', 'wb_comid_list_200m', 'wb_ftype_str_list_200m',\n",
       "       'wb_gnis_id_list_200m', 'wb_area_list_200m', 'fl_comid_list_200m',\n",
       "       'fl_ftype_str_list_200m', 'fl_gnis_id_list_200m', 'fl_length_list_200m',\n",
       "       'fl_areasqkm_sum_200m', 'fl_areasqkm_count_200m',\n",
       "       'fl_areasqkm_mean_200m', 'fl_gnis_name_ind_sum_200m',\n",
       "       'fl_gnis_name_ind_count_200m', 'fl_gnis_name_ind_mean_200m',\n",
       "       'fl_totdasqkm_sum_200m', 'fl_totdasqkm_count_200m',\n",
       "       'fl_totdasqkm_mean_200m', 'fl_flow_type_sum_200m',\n",
       "       'fl_flow_type_count_200m', 'fl_flow_type_mean_200m',\n",
       "       'fl_streamorde_sum_200m', 'fl_streamorde_count_200m',\n",
       "       'fl_streamorde_mean_200m', 'fl_intephem_sum_200m',\n",
       "       'fl_intephem_count_200m', 'fl_intephem_mean_200m',\n",
       "       'fl_startflag_sum_200m', 'fl_startflag_count_200m',\n",
       "       'fl_startflag_mean_200m', 'fl_divergence_sum_200m',\n",
       "       'fl_divergence_count_200m', 'fl_divergence_mean_200m',\n",
       "       'wb_comid_str_200m', 'wb_ftype_str_200m', 'wb_gnis_id_str_200m',\n",
       "       'wb_area_sum_200m', 'wb_area_count_200m', 'wb_area_mean_200m',\n",
       "       'wb_gnis_name_ind_sum_200m', 'wb_gnis_name_ind_count_200m',\n",
       "       'wb_gnis_name_ind_mean_200m', 'fl_comid_str_200m', 'fl_ftype_str_200m',\n",
       "       'fl_gnis_id_str_200m', 'fl_length_sum_200m', 'fl_length_count_200m',\n",
       "       'fl_length_mean_200m', 'fl_ftype_str_vector_200m',\n",
       "       'fl_ftype_artificialpath_200m', 'fl_ftype_canalditch_200m',\n",
       "       'fl_ftype_coastline_200m', 'fl_ftype_connector_200m',\n",
       "       'fl_ftype_pipeline_200m', 'fl_ftype_streamriver_200m',\n",
       "       'wb_ftype_str_vector_200m', 'wb_ftype_artificialpath_200m',\n",
       "       'wb_ftype_canalditch_200m', 'wb_ftype_coastline_200m',\n",
       "       'wb_ftype_connector_200m', 'wb_ftype_pipeline_200m',\n",
       "       'wb_ftype_streamriver_200m'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "twelve-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this df into pickle\n",
    "import pickle\n",
    "pickle.dump(df, open(\"200mX200m_nhd_variables_extracted_stripped\",\"wb\"), protocol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "written-procedure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'jurisdiction_type', 'da_number',\n",
       "       'district', 'project_name', 'longitude', 'latitude',\n",
       "       'date_issued_or_denied', 'rha_determination', 'cwa_determination',\n",
       "       'rha1', 'rha2', 'cwa1', 'cwa2', 'cwa3', 'cwa4', 'cwa5', 'cwa6', 'cwa7',\n",
       "       'cwa8', 'cwa9', 'potential_wetland', 'index', 'Index', 'mukey',\n",
       "       'hydclprs', 'aws025wta', 'drclassdcd', 'nhd_vars_wb_200m',\n",
       "       'nhd_vars_fl_200m', 'wb_comid_list_200m', 'wb_ftype_str_list_200m',\n",
       "       'wb_gnis_id_list_200m', 'wb_area_list_200m', 'fl_comid_list_200m',\n",
       "       'fl_ftype_str_list_200m', 'fl_gnis_id_list_200m', 'fl_length_list_200m',\n",
       "       'fl_areasqkm_sum_200m', 'fl_areasqkm_count_200m',\n",
       "       'fl_areasqkm_mean_200m', 'fl_gnis_name_ind_sum_200m',\n",
       "       'fl_gnis_name_ind_count_200m', 'fl_gnis_name_ind_mean_200m',\n",
       "       'fl_totdasqkm_sum_200m', 'fl_totdasqkm_count_200m',\n",
       "       'fl_totdasqkm_mean_200m', 'fl_flow_type_sum_200m',\n",
       "       'fl_flow_type_count_200m', 'fl_flow_type_mean_200m',\n",
       "       'fl_streamorde_sum_200m', 'fl_streamorde_count_200m',\n",
       "       'fl_streamorde_mean_200m', 'fl_intephem_sum_200m',\n",
       "       'fl_intephem_count_200m', 'fl_intephem_mean_200m',\n",
       "       'fl_startflag_sum_200m', 'fl_startflag_count_200m',\n",
       "       'fl_startflag_mean_200m', 'fl_divergence_sum_200m',\n",
       "       'fl_divergence_count_200m', 'fl_divergence_mean_200m',\n",
       "       'wb_comid_str_200m', 'wb_ftype_str_200m', 'wb_gnis_id_str_200m',\n",
       "       'wb_area_sum_200m', 'wb_area_count_200m', 'wb_area_mean_200m',\n",
       "       'wb_gnis_name_ind_sum_200m', 'wb_gnis_name_ind_count_200m',\n",
       "       'wb_gnis_name_ind_mean_200m', 'fl_comid_str_200m', 'fl_ftype_str_200m',\n",
       "       'fl_gnis_id_str_200m', 'fl_length_sum_200m', 'fl_length_count_200m',\n",
       "       'fl_length_mean_200m', 'fl_ftype_str_vector_200m',\n",
       "       'fl_ftype_artificialpath_200m', 'fl_ftype_canalditch_200m',\n",
       "       'fl_ftype_coastline_200m', 'fl_ftype_connector_200m',\n",
       "       'fl_ftype_pipeline_200m', 'fl_ftype_streamriver_200m',\n",
       "       'wb_ftype_str_vector_200m', 'wb_ftype_artificialpath_200m',\n",
       "       'wb_ftype_canalditch_200m', 'wb_ftype_coastline_200m',\n",
       "       'wb_ftype_connector_200m', 'wb_ftype_pipeline_200m',\n",
       "       'wb_ftype_streamriver_200m'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle(\"200mX200m_nhd_variables_extracted_stripped\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-failing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
