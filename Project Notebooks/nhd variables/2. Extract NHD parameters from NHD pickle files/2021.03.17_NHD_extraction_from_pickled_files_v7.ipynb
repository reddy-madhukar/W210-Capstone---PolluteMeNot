{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "published-belgium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Madhukar/opt/miniconda3/envs/ee_skmr/bin/python\n",
      "3.9.2 | packaged by conda-forge | (default, Feb 21 2021, 05:02:20) \n",
      "[Clang 11.0.1 ]\n",
      "sys.version_info(major=3, minor=9, micro=2, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "technical-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mental-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the nhd addendum file\n",
    "nhd_stats = pd.read_csv(\"nhd_stats_AI.csv\")\n",
    "\n",
    "# read in csv file with SSURGO variables\n",
    "df_m = pd.read_csv(\"combined_regular_clean_with_ssurgo_variables.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "infectious-alexandria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comid</th>\n",
       "      <th>long_comid</th>\n",
       "      <th>lat_comid</th>\n",
       "      <th>startflag</th>\n",
       "      <th>intephem</th>\n",
       "      <th>divergence</th>\n",
       "      <th>streamorde</th>\n",
       "      <th>lengthkm</th>\n",
       "      <th>gnis_name_ind</th>\n",
       "      <th>areasqkm</th>\n",
       "      <th>totdasqkm</th>\n",
       "      <th>flow_type</th>\n",
       "      <th>distup_max</th>\n",
       "      <th>distdown_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>179</td>\n",
       "      <td>-67.986409</td>\n",
       "      <td>46.022164</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.412</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5550</td>\n",
       "      <td>3.5550</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>243.97200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>181</td>\n",
       "      <td>-67.998723</td>\n",
       "      <td>46.016490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2898</td>\n",
       "      <td>3.8448</td>\n",
       "      <td>1</td>\n",
       "      <td>1.364</td>\n",
       "      <td>242.60800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>183</td>\n",
       "      <td>-67.998835</td>\n",
       "      <td>46.020847</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.112</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2043</td>\n",
       "      <td>8.1954</td>\n",
       "      <td>1</td>\n",
       "      <td>3.278</td>\n",
       "      <td>243.29601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>185</td>\n",
       "      <td>-67.998621</td>\n",
       "      <td>46.019712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>8.2323</td>\n",
       "      <td>1</td>\n",
       "      <td>3.438</td>\n",
       "      <td>243.13600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>843</td>\n",
       "      <td>-68.378758</td>\n",
       "      <td>46.246067</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.889</td>\n",
       "      <td>0</td>\n",
       "      <td>2.7486</td>\n",
       "      <td>2.7486</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>285.85901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comid  long_comid  lat_comid  startflag  intephem  divergence  streamorde  \\\n",
       "0    179  -67.986409  46.022164        1.0         0         0.0         1.0   \n",
       "1    181  -67.998723  46.016490        0.0         0         0.0         1.0   \n",
       "2    183  -67.998835  46.020847        0.0         0         0.0         2.0   \n",
       "3    185  -67.998621  46.019712        0.0         0         0.0         2.0   \n",
       "4    843  -68.378758  46.246067        1.0         0         0.0         1.0   \n",
       "\n",
       "   lengthkm  gnis_name_ind  areasqkm  totdasqkm  flow_type  distup_max  \\\n",
       "0     2.412              0    3.5550     3.5550          1         NaN   \n",
       "1     0.442              0    0.2898     3.8448          1       1.364   \n",
       "2     0.112              1    0.2043     8.1954          1       3.278   \n",
       "3     0.170              1    0.0369     8.2323          1       3.438   \n",
       "4     1.889              0    2.7486     2.7486          1         NaN   \n",
       "\n",
       "   distdown_max  \n",
       "0     243.97200  \n",
       "1     242.60800  \n",
       "2     243.29601  \n",
       "3     243.13600  \n",
       "4     285.85901  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nhd_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "answering-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_pickle('../1. Extract NHD information from GEE/NHD_extracted_vars_2.5kmX2.5km_with_fcode_ftype/2.5kmX2.5km_nhd_variables_part1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "liable-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comid's from GEE are extracted into several pickled files\n",
    "# pickling is needed to be able to share easily across team members \n",
    "# join these pickled files into one full dataset\n",
    "\n",
    "df_merged_full = []\n",
    "\n",
    "for i in range(df_m.shape[0] // 500 + 1):\n",
    "    try:\n",
    "        df_temp = pd.read_pickle('../1. Extract NHD information from GEE/NHD_extracted_vars_2.5kmX2.5km_with_fcode_ftype/2.5kmX2.5km_nhd_variables_part' + str(500 * i + 1))\n",
    "#         print(df_temp.columns)\n",
    "        df_merged_full.append(df_temp)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "df_merged_full = pd.concat(df_merged_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "extraordinary-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert to protocol 3 so that can read in aws\n",
    "# df_merged_full = []\n",
    "\n",
    "# for i in range(df_m.shape[0] // 500 + 1):\n",
    "#     try:\n",
    "#         df_temp = pd.read_pickle(('NHD_extracted_vars_2.5kmX2.5km/combined_regular_clean_with_ssurgo_nhd_variables_part' + \n",
    "#                               str(500 * i + 1)))\n",
    "#         print(df_temp.shape)\n",
    "#         pickle.dump(df_temp, open(\"NHD_extracted_vars_2.5kmX2.5km/combined_regular_clean_with_ssurgo_nhd_variables_part\" + str(500 * i + 1) + \"_pkl3.pkl\",\"wb\"), protocol=3)\n",
    "#         df_temp2 = pd.read_pickle(('NHD_extracted_vars_2.5kmX2.5km/combined_regular_clean_with_ssurgo_nhd_variables_part' + \n",
    "#                               str(500 * i + 1)))\n",
    "\n",
    "#     except:\n",
    "#         break\n",
    "#     df_merged_full.append(df_temp2)\n",
    "# df_merged_full = pd.concat(df_merged_full)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "infrared-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write this df into pickle\n",
    "import pickle\n",
    "pickle.dump(df_merged_full, open(\"2.5kmX2.5km_nhd_variables_partsmerged\",\"wb\"), protocol=3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-theater",
   "metadata": {},
   "source": [
    "# Read in merged pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "considered-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_full = pd.read_pickle(\"2.5kmX2.5km_nhd_variables_partsmerged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adult-revelation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'jurisdiction_type', 'da_number',\n",
       "       'district', 'project_name', 'longitude', 'latitude',\n",
       "       'date_issued_or_denied', 'rha_determination', 'cwa_determination',\n",
       "       'rha1', 'rha2', 'cwa1', 'cwa2', 'cwa3', 'cwa4', 'cwa5', 'cwa6', 'cwa7',\n",
       "       'cwa8', 'cwa9', 'potential_wetland', 'index', 'Index', 'mukey',\n",
       "       'hydclprs', 'aws025wta', 'drclassdcd', 'nhd_vars_wb', 'nhd_vars_fl'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_full.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-title",
   "metadata": {},
   "source": [
    "# A word on 'nhd_vars_wb' and 'nhd_vars_fl' columns (see last two columns above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-gamma",
   "metadata": {},
   "source": [
    "## nhd_vars_wb: this is a list of lists\n",
    "- for each record, the following six features from GEE are extracted as lists and stored into a list\n",
    "- [comid_list, ftype_str, gnis_id, wb_area, fl_length, fcode]\n",
    "- the column is labeled nhd_vars_wb for waterbodies\n",
    "\n",
    "In a similar fashion, there is another column for flowlines labeled nhd_vars_fl\n",
    "\n",
    "Note: fl_length is NaN's for waterbodies and wb_area ae NaN's for flowlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-investor",
   "metadata": {},
   "source": [
    "## In the following, features are extracted from the above columns and feature engineered as discussed in meetings. Pls use your judgement to help devise any new features you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "prescribed-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wb for waterbodies\n",
    "# extract the individual lists\n",
    "df_merged_full[\"wb_comid_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_wb[0], axis=1)\n",
    "df_merged_full[\"wb_ftype_str_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_wb[1], axis=1)\n",
    "df_merged_full[\"wb_gnis_id_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_wb[2], axis=1)\n",
    "df_merged_full[\"wb_area_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_wb[3], axis=1)\n",
    "\n",
    "# fl for flowlines\n",
    "# extract the individual lists\n",
    "df_merged_full[\"fl_comid_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_fl[0], axis=1)\n",
    "df_merged_full[\"fl_ftype_str_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_fl[1], axis=1)\n",
    "df_merged_full[\"fl_gnis_id_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_fl[2], axis=1)\n",
    "df_merged_full[\"fl_length_list\"] = df_merged_full.apply(lambda x: x.nhd_vars_fl[4], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-continent",
   "metadata": {},
   "source": [
    "# Lets look at columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bridal-tunnel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nhd_vars_wb</th>\n",
       "      <th>nhd_vars_fl</th>\n",
       "      <th>wb_comid_list</th>\n",
       "      <th>wb_ftype_str_list</th>\n",
       "      <th>wb_gnis_id_list</th>\n",
       "      <th>wb_area_list</th>\n",
       "      <th>fl_comid_list</th>\n",
       "      <th>fl_ftype_str_list</th>\n",
       "      <th>fl_gnis_id_list</th>\n",
       "      <th>fl_length_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>([120052831, 22027044], [LakePond, LakePond], ...</td>\n",
       "      <td>([21980197, 21980245, 21980217, 21980207, 2198...</td>\n",
       "      <td>[120052831, 22027044]</td>\n",
       "      <td>[LakePond, LakePond]</td>\n",
       "      <td>[974076, ]</td>\n",
       "      <td>[171.202, 0.008]</td>\n",
       "      <td>[21980197, 21980245, 21980217, 21980207, 21980...</td>\n",
       "      <td>[ArtificialPath, ArtificialPath, ArtificialPat...</td>\n",
       "      <td>[, , , , , , , , 973757, , 974058]</td>\n",
       "      <td>[0.022, 1.975, 3.135, 0.725, 2.786, 0.764, 3.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>([904140245], [LakePond], [1075813], [12045.52...</td>\n",
       "      <td>([15594559, 15594563, 15594573, 15594565, 1559...</td>\n",
       "      <td>[904140245]</td>\n",
       "      <td>[LakePond]</td>\n",
       "      <td>[1075813]</td>\n",
       "      <td>[12045.529]</td>\n",
       "      <td>[15594559, 15594563, 15594573, 15594565, 15594...</td>\n",
       "      <td>[StreamRiver, StreamRiver, StreamRiver, Stream...</td>\n",
       "      <td>[, , 1066928, 1066928, 1066928, , , 1066928]</td>\n",
       "      <td>[0.602, 0.217, 1.254, 0.208, 7.518, 5.608, 12....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>([21631197, 904140246, 21631201], [LakePond, L...</td>\n",
       "      <td>([21632385, 21632387, 21632389, 21635915, 2163...</td>\n",
       "      <td>[21631197, 904140246, 21631201]</td>\n",
       "      <td>[LakePond, LakePond, LakePond]</td>\n",
       "      <td>[967326, 970427, ]</td>\n",
       "      <td>[0.425, 6693.837, 0.946]</td>\n",
       "      <td>[21632385, 21632387, 21632389, 21635915, 21635...</td>\n",
       "      <td>[Coastline, Coastline, Coastline, Coastline, C...</td>\n",
       "      <td>[, , , , , , , ]</td>\n",
       "      <td>[2.781, 0.287, 1.895, 0.871, 10.109, 0.15, 1.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>([15586156, 15586164], [LakePond, LakePond], [...</td>\n",
       "      <td>([15588640, 15588636, 15587674, 15587680, 1558...</td>\n",
       "      <td>[15586156, 15586164]</td>\n",
       "      <td>[LakePond, LakePond]</td>\n",
       "      <td>[1078482, 1079552]</td>\n",
       "      <td>[0.139, 0.051]</td>\n",
       "      <td>[15588640, 15588636, 15587674, 15587680, 15587...</td>\n",
       "      <td>[ArtificialPath, ArtificialPath, StreamRiver, ...</td>\n",
       "      <td>[, , , , 1066599, 1066599, 1066851]</td>\n",
       "      <td>[0.941, 0.658, 1.216, 0.505, 0.968, 3.143, 1.98]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>([], [], [], [], [], [])</td>\n",
       "      <td>([15560355, 15560297, 15560169, 15560311, 1556...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[15560355, 15560297, 15560169, 15560311, 15560...</td>\n",
       "      <td>[StreamRiver, StreamRiver, CanalDitch, CanalDi...</td>\n",
       "      <td>[968024, 968024, , , , , , , ]</td>\n",
       "      <td>[0.766, 4.526, 1.44, 1.941, 0.95, 0.641, 0.372...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>([], [], [], [], [], [])</td>\n",
       "      <td>([15594581, 15594569, 15594591], [StreamRiver,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[15594581, 15594569, 15594591]</td>\n",
       "      <td>[StreamRiver, StreamRiver, StreamRiver]</td>\n",
       "      <td>[, 1044708, 1044708]</td>\n",
       "      <td>[1.943, 2.539, 4.12]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          nhd_vars_wb  \\\n",
       "9   ([120052831, 22027044], [LakePond, LakePond], ...   \n",
       "10  ([904140245], [LakePond], [1075813], [12045.52...   \n",
       "11  ([21631197, 904140246, 21631201], [LakePond, L...   \n",
       "12  ([15586156, 15586164], [LakePond, LakePond], [...   \n",
       "13                           ([], [], [], [], [], [])   \n",
       "14                           ([], [], [], [], [], [])   \n",
       "\n",
       "                                          nhd_vars_fl  \\\n",
       "9   ([21980197, 21980245, 21980217, 21980207, 2198...   \n",
       "10  ([15594559, 15594563, 15594573, 15594565, 1559...   \n",
       "11  ([21632385, 21632387, 21632389, 21635915, 2163...   \n",
       "12  ([15588640, 15588636, 15587674, 15587680, 1558...   \n",
       "13  ([15560355, 15560297, 15560169, 15560311, 1556...   \n",
       "14  ([15594581, 15594569, 15594591], [StreamRiver,...   \n",
       "\n",
       "                      wb_comid_list               wb_ftype_str_list  \\\n",
       "9             [120052831, 22027044]            [LakePond, LakePond]   \n",
       "10                      [904140245]                      [LakePond]   \n",
       "11  [21631197, 904140246, 21631201]  [LakePond, LakePond, LakePond]   \n",
       "12             [15586156, 15586164]            [LakePond, LakePond]   \n",
       "13                               []                              []   \n",
       "14                               []                              []   \n",
       "\n",
       "       wb_gnis_id_list              wb_area_list  \\\n",
       "9           [974076, ]          [171.202, 0.008]   \n",
       "10           [1075813]               [12045.529]   \n",
       "11  [967326, 970427, ]  [0.425, 6693.837, 0.946]   \n",
       "12  [1078482, 1079552]            [0.139, 0.051]   \n",
       "13                  []                        []   \n",
       "14                  []                        []   \n",
       "\n",
       "                                        fl_comid_list  \\\n",
       "9   [21980197, 21980245, 21980217, 21980207, 21980...   \n",
       "10  [15594559, 15594563, 15594573, 15594565, 15594...   \n",
       "11  [21632385, 21632387, 21632389, 21635915, 21635...   \n",
       "12  [15588640, 15588636, 15587674, 15587680, 15587...   \n",
       "13  [15560355, 15560297, 15560169, 15560311, 15560...   \n",
       "14                     [15594581, 15594569, 15594591]   \n",
       "\n",
       "                                    fl_ftype_str_list  \\\n",
       "9   [ArtificialPath, ArtificialPath, ArtificialPat...   \n",
       "10  [StreamRiver, StreamRiver, StreamRiver, Stream...   \n",
       "11  [Coastline, Coastline, Coastline, Coastline, C...   \n",
       "12  [ArtificialPath, ArtificialPath, StreamRiver, ...   \n",
       "13  [StreamRiver, StreamRiver, CanalDitch, CanalDi...   \n",
       "14            [StreamRiver, StreamRiver, StreamRiver]   \n",
       "\n",
       "                                 fl_gnis_id_list  \\\n",
       "9             [, , , , , , , , 973757, , 974058]   \n",
       "10  [, , 1066928, 1066928, 1066928, , , 1066928]   \n",
       "11                              [, , , , , , , ]   \n",
       "12           [, , , , 1066599, 1066599, 1066851]   \n",
       "13                [968024, 968024, , , , , , , ]   \n",
       "14                          [, 1044708, 1044708]   \n",
       "\n",
       "                                       fl_length_list  \n",
       "9   [0.022, 1.975, 3.135, 0.725, 2.786, 0.764, 3.0...  \n",
       "10  [0.602, 0.217, 1.254, 0.208, 7.518, 5.608, 12....  \n",
       "11  [2.781, 0.287, 1.895, 0.871, 10.109, 0.15, 1.1...  \n",
       "12   [0.941, 0.658, 1.216, 0.505, 0.968, 3.143, 1.98]  \n",
       "13  [0.766, 4.526, 1.44, 1.941, 0.95, 0.641, 0.372...  \n",
       "14                               [1.943, 2.539, 4.12]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_full[df_merged_full.columns[29:39]][9:15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-privilege",
   "metadata": {},
   "source": [
    "## Lets look at one row in individual columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "extra-gamma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([120052831, 22027044],\n",
       " ['LakePond', 'LakePond'],\n",
       " ['974076', ''],\n",
       " [171.202, 0.008],\n",
       " [nan, nan],\n",
       " [39004, 39004])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at waterbodies list\n",
    "df_merged_full.nhd_vars_wb[9]\n",
    "# you can see there are 6 items in the list [comid_list, ftype_str, gnis_id, wb_area, fl_length, fcode]\n",
    "# note that fcode is going to be null due to coding lapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "peripheral-advancement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([21980197,\n",
       "  21980245,\n",
       "  21980217,\n",
       "  21980207,\n",
       "  21980203,\n",
       "  21980199,\n",
       "  21980195,\n",
       "  21980193,\n",
       "  21978319,\n",
       "  21978365,\n",
       "  21978323],\n",
       " ['ArtificialPath',\n",
       "  'ArtificialPath',\n",
       "  'ArtificialPath',\n",
       "  'ArtificialPath',\n",
       "  'ArtificialPath',\n",
       "  'ArtificialPath',\n",
       "  'ArtificialPath',\n",
       "  'ArtificialPath',\n",
       "  'StreamRiver',\n",
       "  'StreamRiver',\n",
       "  'StreamRiver'],\n",
       " ['', '', '', '', '', '', '', '', '973757', '', '974058'],\n",
       " [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       " [0.022, 1.975, 3.135, 0.725, 2.786, 0.764, 3.016, 1.652, 3.419, 3.557, 2.136],\n",
       " [55800, 55800, 55800, 55800, 55800, 55800, 55800, 55800, 46006, 46006, 46006])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at flowlines list\n",
    "df_merged_full.nhd_vars_fl[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "activated-wallet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[120052831, 22027044]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of comids in waterbodies\n",
    "df_merged_full.wb_comid_list[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "altered-mandate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21980197,\n",
       " 21980245,\n",
       " 21980217,\n",
       " 21980207,\n",
       " 21980203,\n",
       " 21980199,\n",
       " 21980195,\n",
       " 21980193,\n",
       " 21978319,\n",
       " 21978365,\n",
       " 21978323]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of comids in flowlines\n",
    "df_merged_full.fl_comid_list[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-future",
   "metadata": {},
   "source": [
    "### .... and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "continued-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Filter out invalid comids (although not used in this notebook)\n",
    "# # \"invalid\" = present in GEE but not present in nhd_stats\n",
    "\n",
    "# df_merged[\"wb_comid_list_filtered\"] = df_merged.apply(lambda x: [comid for comid in x.nhd_vars_wb[0] if comid in np.array(nhd_stats.comid)\n",
    "#                                                                 ], axis=1)\n",
    "\n",
    "# df_merged[\"fl_comid_list_filtered\"] = df_merged.apply(lambda x: [comid for comid in x.nhd_vars_fl[0] if comid in np.array(nhd_stats.comid)\n",
    "#                                                                 ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "immune-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigned begin and end of records for each person\n",
    "# MADHUKAR: records 1 - 5000\n",
    "# SHOBHA: records 5000 - 10000\n",
    "# RADHIKA: records 10000 - 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "naked-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features present in nhd_stats for corresponding comid\n",
    "# read in fl_comid_list, pull out matching variable values in nhd_stats\n",
    "\n",
    "df_merged = df_merged_full\n",
    "\n",
    "def extract_feature(comid, feature):\n",
    "    \"\"\"\n",
    "    Extract features present in nhd_stats for corresponding comid\n",
    "    \"\"\"\n",
    "    if comid == None:\n",
    "        return np.nan # if no comid's in GEE\n",
    "    extracted_feature = nhd_stats[nhd_stats[\"comid\"] == comid][str(feature)]\n",
    "    try:\n",
    "        extracted_feature = np.array(extracted_feature).item() \n",
    "    except Exception as e:\n",
    "        return np.nan # if comid in GEE but not in nhd database\n",
    "    return extracted_feature\n",
    "\n",
    "\n",
    "def extract_sum(feature):\n",
    "    \"\"\"\n",
    "    feature engineering per excel sheet\n",
    "    \"\"\"\n",
    "    return (df_merged.apply(lambda x: np.sum(np.array([extract_feature(comid, str(feature))\n",
    "                                                                 for comid in x.fl_comid_list])\n",
    "                                                       [~np.isnan(np.array([extract_feature(comid, str(feature))\n",
    "                                                                            for comid in x.fl_comid_list]))]), \n",
    "                                                axis=1))\n",
    "def extract_count(feature):\n",
    "    \"\"\"\n",
    "    feature engineering per excel sheet\n",
    "    \"\"\"\n",
    "    return (df_merged.apply(lambda x: len(np.array([extract_feature(comid, str(feature))\n",
    "                                                                 for comid in x.fl_comid_list])\n",
    "                                                       [~np.isnan(np.array([extract_feature(comid, str(feature))\n",
    "                                                                            for comid in x.fl_comid_list]))]), \n",
    "                                                  axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "departmental-filter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flowline variables\n",
    "\n",
    "# areasqkm\n",
    "df_merged[\"fl_areasqkm_sum\"] = extract_sum(\"areasqkm\")\n",
    "df_merged[\"fl_areasqkm_count\"] = extract_count(\"areasqkm\")\n",
    "df_merged[\"fl_areasqkm_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_areasqkm_sum/x.fl_areasqkm_count) \n",
    "                                                 if x.fl_areasqkm_count != 0 \n",
    "#                                                  else np.nan, axis=1)) # here you want to return 0\n",
    "                                                 else 0, axis=1))\n",
    "# gnis_name_ind\n",
    "df_merged[\"fl_gnis_name_ind_sum\"] = extract_sum(\"gnis_name_ind\")\n",
    "df_merged[\"fl_gnis_name_ind_count\"] = extract_count(\"gnis_name_ind\")\n",
    "df_merged[\"fl_gnis_name_ind_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_gnis_name_ind_sum/x.fl_gnis_name_ind_count) \n",
    "                                                 if x.fl_gnis_name_ind_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))\n",
    "\n",
    "# totdasqkm\n",
    "df_merged[\"fl_totdasqkm_sum\"] = extract_sum(\"totdasqkm\")\n",
    "df_merged[\"fl_totdasqkm_count\"] = extract_count(\"totdasqkm\")\n",
    "df_merged[\"fl_totdasqkm_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_totdasqkm_sum/x.fl_totdasqkm_count) \n",
    "                                                 if x.fl_totdasqkm_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))\n",
    "\n",
    "# flow_type\n",
    "df_merged[\"fl_flow_type_sum\"] = extract_sum(\"flow_type\")\n",
    "df_merged[\"fl_flow_type_count\"] = extract_count(\"flow_type\")\n",
    "df_merged[\"fl_flow_type_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_flow_type_sum/x.fl_flow_type_count) \n",
    "                                                 if x.fl_flow_type_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))\n",
    "\n",
    "                                                  \n",
    "# streamorde\n",
    "df_merged[\"fl_streamorde_sum\"] = extract_sum(\"streamorde\")\n",
    "df_merged[\"fl_streamorde_count\"] = extract_count(\"streamorde\")\n",
    "df_merged[\"fl_streamorde_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_streamorde_sum/x.fl_streamorde_count) \n",
    "                                                 if x.fl_streamorde_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))                                                   \n",
    "\n",
    "# intephem\n",
    "df_merged[\"fl_intephem_sum\"] = extract_sum(\"intephem\")\n",
    "df_merged[\"fl_intephem_count\"] = extract_count(\"intephem\")\n",
    "df_merged[\"fl_intephem_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_intephem_sum/x.fl_intephem_count) \n",
    "                                                 if x.fl_intephem_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))                                                 \n",
    "\n",
    "# startflag\n",
    "df_merged[\"fl_startflag_sum\"] = extract_sum(\"startflag\")\n",
    "df_merged[\"fl_startflag_count\"] = extract_count(\"startflag\")\n",
    "df_merged[\"fl_startflag_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_startflag_sum/x.fl_startflag_count) \n",
    "                                                 if x.fl_startflag_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))                                                  \n",
    "\n",
    "# divergence\n",
    "df_merged[\"fl_divergence_sum\"] = extract_sum(\"divergence\")\n",
    "df_merged[\"fl_divergence_count\"] = extract_count(\"divergence\")\n",
    "df_merged[\"fl_divergence_mean\"] = (df_merged.apply(lambda x: \n",
    "                                                 (x.fl_divergence_sum/x.fl_divergence_count) \n",
    "                                                 if x.fl_divergence_count != 0 \n",
    "#                                                  else np.nan, axis=1))\n",
    "                                                 else 0, axis=1))                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-malta",
   "metadata": {},
   "source": [
    "# In a similar fasion, you can feature engineer the waterbodies (I will get that later today/tomorrow and push the updated dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "historical-mortality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>rha_determination</th>\n",
       "      <th>cwa_determination</th>\n",
       "      <th>rha1</th>\n",
       "      <th>rha2</th>\n",
       "      <th>cwa1</th>\n",
       "      <th>cwa2</th>\n",
       "      <th>...</th>\n",
       "      <th>fl_streamorde_mean</th>\n",
       "      <th>fl_intephem_sum</th>\n",
       "      <th>fl_intephem_count</th>\n",
       "      <th>fl_intephem_mean</th>\n",
       "      <th>fl_startflag_sum</th>\n",
       "      <th>fl_startflag_count</th>\n",
       "      <th>fl_startflag_mean</th>\n",
       "      <th>fl_divergence_sum</th>\n",
       "      <th>fl_divergence_count</th>\n",
       "      <th>fl_divergence_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7309.000000</td>\n",
       "      <td>7309.000000</td>\n",
       "      <td>-90.491852</td>\n",
       "      <td>37.209405</td>\n",
       "      <td>0.104932</td>\n",
       "      <td>0.369588</td>\n",
       "      <td>0.070935</td>\n",
       "      <td>0.062727</td>\n",
       "      <td>0.093235</td>\n",
       "      <td>0.063889</td>\n",
       "      <td>...</td>\n",
       "      <td>1.615928</td>\n",
       "      <td>1.297490</td>\n",
       "      <td>5.167453</td>\n",
       "      <td>0.271995</td>\n",
       "      <td>1.410425</td>\n",
       "      <td>4.350708</td>\n",
       "      <td>0.337638</td>\n",
       "      <td>0.369861</td>\n",
       "      <td>4.350708</td>\n",
       "      <td>0.037571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4220.286128</td>\n",
       "      <td>4220.286128</td>\n",
       "      <td>15.418083</td>\n",
       "      <td>6.766997</td>\n",
       "      <td>0.306476</td>\n",
       "      <td>0.482710</td>\n",
       "      <td>0.256725</td>\n",
       "      <td>0.242479</td>\n",
       "      <td>0.290771</td>\n",
       "      <td>0.244564</td>\n",
       "      <td>...</td>\n",
       "      <td>1.455323</td>\n",
       "      <td>1.905433</td>\n",
       "      <td>6.459320</td>\n",
       "      <td>0.349860</td>\n",
       "      <td>1.494997</td>\n",
       "      <td>4.537728</td>\n",
       "      <td>0.348352</td>\n",
       "      <td>1.817414</td>\n",
       "      <td>4.537728</td>\n",
       "      <td>0.147239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-174.197880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3654.500000</td>\n",
       "      <td>3654.500000</td>\n",
       "      <td>-93.638850</td>\n",
       "      <td>32.815575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7309.000000</td>\n",
       "      <td>7309.000000</td>\n",
       "      <td>-88.186500</td>\n",
       "      <td>35.478300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10963.500000</td>\n",
       "      <td>10963.500000</td>\n",
       "      <td>-80.226955</td>\n",
       "      <td>41.403750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14618.000000</td>\n",
       "      <td>14618.000000</td>\n",
       "      <td>144.828420</td>\n",
       "      <td>70.483790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  Unnamed: 0.1     longitude      latitude  \\\n",
       "count  14619.000000  14619.000000  14619.000000  14619.000000   \n",
       "mean    7309.000000   7309.000000    -90.491852     37.209405   \n",
       "std     4220.286128   4220.286128     15.418083      6.766997   \n",
       "min        0.000000      0.000000   -174.197880      0.000000   \n",
       "25%     3654.500000   3654.500000    -93.638850     32.815575   \n",
       "50%     7309.000000   7309.000000    -88.186500     35.478300   \n",
       "75%    10963.500000  10963.500000    -80.226955     41.403750   \n",
       "max    14618.000000  14618.000000    144.828420     70.483790   \n",
       "\n",
       "       rha_determination  cwa_determination          rha1          rha2  \\\n",
       "count       14619.000000       14619.000000  14619.000000  14619.000000   \n",
       "mean            0.104932           0.369588      0.070935      0.062727   \n",
       "std             0.306476           0.482710      0.256725      0.242479   \n",
       "min             0.000000           0.000000      0.000000      0.000000   \n",
       "25%             0.000000           0.000000      0.000000      0.000000   \n",
       "50%             0.000000           0.000000      0.000000      0.000000   \n",
       "75%             0.000000           1.000000      0.000000      0.000000   \n",
       "max             1.000000           1.000000      1.000000      1.000000   \n",
       "\n",
       "               cwa1          cwa2  ...  fl_streamorde_mean  fl_intephem_sum  \\\n",
       "count  14619.000000  14619.000000  ...        14619.000000     14619.000000   \n",
       "mean       0.093235      0.063889  ...            1.615928         1.297490   \n",
       "std        0.290771      0.244564  ...            1.455323         1.905433   \n",
       "min        0.000000      0.000000  ...            0.000000         0.000000   \n",
       "25%        0.000000      0.000000  ...            1.000000         0.000000   \n",
       "50%        0.000000      0.000000  ...            1.250000         0.000000   \n",
       "75%        0.000000      0.000000  ...            2.000000         2.000000   \n",
       "max        1.000000      1.000000  ...           10.000000        30.000000   \n",
       "\n",
       "       fl_intephem_count  fl_intephem_mean  fl_startflag_sum  \\\n",
       "count       14619.000000      14619.000000      14619.000000   \n",
       "mean            5.167453          0.271995          1.410425   \n",
       "std             6.459320          0.349860          1.494997   \n",
       "min             0.000000          0.000000          0.000000   \n",
       "25%             2.000000          0.000000          0.000000   \n",
       "50%             4.000000          0.000000          1.000000   \n",
       "75%             7.000000          0.500000          2.000000   \n",
       "max           254.000000          1.000000         10.000000   \n",
       "\n",
       "       fl_startflag_count  fl_startflag_mean  fl_divergence_sum  \\\n",
       "count        14619.000000       14619.000000       14619.000000   \n",
       "mean             4.350708           0.337638           0.369861   \n",
       "std              4.537728           0.348352           1.817414   \n",
       "min              0.000000           0.000000           0.000000   \n",
       "25%              1.000000           0.000000           0.000000   \n",
       "50%              3.000000           0.250000           0.000000   \n",
       "75%              6.000000           0.500000           0.000000   \n",
       "max             58.000000           1.000000          57.000000   \n",
       "\n",
       "       fl_divergence_count  fl_divergence_mean  \n",
       "count         14619.000000        14619.000000  \n",
       "mean              4.350708            0.037571  \n",
       "std               4.537728            0.147239  \n",
       "min               0.000000            0.000000  \n",
       "25%               1.000000            0.000000  \n",
       "50%               3.000000            0.000000  \n",
       "75%               6.000000            0.000000  \n",
       "max              58.000000            2.000000  \n",
       "\n",
       "[8 rows x 47 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "medieval-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the variables that needs to be made accessible to manipulation via csv files\n",
    "# wb_ftype_str_list\n",
    "# wb_area_list\n",
    "# fl_ftype_str_list\n",
    "# fl_length_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "adaptive-direction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_merged\n",
    "\n",
    "# wb for waterbodies\n",
    "# extract the individual lists\n",
    "df[\"wb_comid_str\"] = df.apply(lambda x: \"+\".join([str(comid) for comid in x.nhd_vars_wb[0]]), axis=1)\n",
    "df[\"wb_ftype_str\"] = df.apply(lambda x: \"+\".join([str(ftype) for ftype in x.nhd_vars_wb[1] if ftype != None]), axis=1)\n",
    "df[\"wb_gnis_id_str\"] = df.apply(lambda x: \"+\".join([str(gnis) for gnis in x.nhd_vars_wb[2]]), axis=1)\n",
    "\n",
    "# # sum and mean\n",
    "df[\"wb_area_sum\"] = df.apply(lambda x: np.sum(np.array([area for area in x.nhd_vars_wb[3] if area != None])), axis=1)\n",
    "df[\"wb_area_count\"] = df.apply(lambda x: len([area for area in x.nhd_vars_wb[3] if area != None]), axis=1)\n",
    "# df[\"wb_area_mean\"] = df.apply(lambda x: (x.wb_area_sum / x.wb_area_count) if x.wb_area_count != 0 else np.nan, axis=1)\n",
    "df[\"wb_area_mean\"] = df.apply(lambda x: (x.wb_area_sum / x.wb_area_count) if x.wb_area_count != 0 else 0, axis=1)\n",
    "\n",
    "df[\"wb_gnis_name_ind_sum\"] = df.apply(lambda x: np.sum(np.array([int(gnis) for gnis in x.nhd_vars_wb[2] if gnis not in [\"\", None]])), axis=1)\n",
    "df[\"wb_gnis_name_ind_count\"] = df.apply(lambda x: len([gnis for gnis in x.nhd_vars_wb[2] if gnis not in [\"\", None]]), axis=1)\n",
    "# df[\"wb_gnis_name_ind_mean\"] = df.apply(lambda x: (x.wb_gnis_name_ind_sum / x.wb_gnis_name_ind_count) if x.wb_gnis_name_ind_count != 0 else np.nan, axis=1)\n",
    "df[\"wb_gnis_name_ind_mean\"] = df.apply(lambda x: (x.wb_gnis_name_ind_sum / x.wb_gnis_name_ind_count) if x.wb_gnis_name_ind_count != 0 else 0, axis=1)\n",
    "\n",
    "\n",
    "# # fl for flowlines\n",
    "# # extract the individual lists\n",
    "df[\"fl_comid_str\"] = df.apply(lambda x: \"+\".join([str(comid) for comid in x.nhd_vars_fl[0]]), axis=1)\n",
    "df[\"fl_ftype_str\"] = df.apply(lambda x: \"+\".join([str(ftype) for ftype in x.nhd_vars_fl[1]]), axis=1)\n",
    "df[\"fl_gnis_id_str\"] = df.apply(lambda x: \"+\".join(x.nhd_vars_fl[2]), axis=1)\n",
    "\n",
    "# # sum and mean\n",
    "df[\"fl_length_sum\"] = df.apply(lambda x: np.sum(np.array([length for length in x.nhd_vars_wb[4] if length != None])), axis=1)\n",
    "df[\"fl_length_count\"] = df.apply(lambda x: len([length for length in x.nhd_vars_wb[4] if length != None]), axis=1)\n",
    "# df[\"fl_length_mean\"] = df.apply(lambda x: (x.fl_length_sum / x.fl_length_count) if x.fl_length_count != 0 else np.nan, axis=1)\n",
    "df[\"fl_length_mean\"] = df.apply(lambda x: (x.fl_length_sum / x.fl_length_count) if x.fl_length_count != 0 else 0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "formed-austin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'jurisdiction_type', 'da_number',\n",
       "       'district', 'project_name', 'longitude', 'latitude',\n",
       "       'date_issued_or_denied', 'rha_determination', 'cwa_determination',\n",
       "       'rha1', 'rha2', 'cwa1', 'cwa2', 'cwa3', 'cwa4', 'cwa5', 'cwa6', 'cwa7',\n",
       "       'cwa8', 'cwa9', 'potential_wetland', 'index', 'Index', 'mukey',\n",
       "       'hydclprs', 'aws025wta', 'drclassdcd', 'nhd_vars_wb', 'nhd_vars_fl',\n",
       "       'wb_comid_list', 'wb_ftype_str_list', 'wb_gnis_id_list', 'wb_area_list',\n",
       "       'fl_comid_list', 'fl_ftype_str_list', 'fl_gnis_id_list',\n",
       "       'fl_length_list', 'fl_areasqkm_sum', 'fl_areasqkm_count',\n",
       "       'fl_areasqkm_mean', 'fl_gnis_name_ind_sum', 'fl_gnis_name_ind_count',\n",
       "       'fl_gnis_name_ind_mean', 'fl_totdasqkm_sum', 'fl_totdasqkm_count',\n",
       "       'fl_totdasqkm_mean', 'fl_flow_type_sum', 'fl_flow_type_count',\n",
       "       'fl_flow_type_mean', 'fl_streamorde_sum', 'fl_streamorde_count',\n",
       "       'fl_streamorde_mean', 'fl_intephem_sum', 'fl_intephem_count',\n",
       "       'fl_intephem_mean', 'fl_startflag_sum', 'fl_startflag_count',\n",
       "       'fl_startflag_mean', 'fl_divergence_sum', 'fl_divergence_count',\n",
       "       'fl_divergence_mean', 'wb_comid_str', 'wb_ftype_str', 'wb_gnis_id_str',\n",
       "       'wb_area_sum', 'wb_area_count', 'wb_area_mean', 'wb_gnis_name_ind_sum',\n",
       "       'wb_gnis_name_ind_count', 'wb_gnis_name_ind_mean', 'fl_comid_str',\n",
       "       'fl_ftype_str', 'fl_gnis_id_str', 'fl_length_sum', 'fl_length_count',\n",
       "       'fl_length_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "personal-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# use protocol 3 for backwards compatibility with Python 3.6 on AWS\n",
    "pickle.dump(df, open(\"2.5kmX2.5km_nhd_variables_extracted\",\"wb\"), protocol=3)\n",
    "df.to_csv(\"2.5kmX2.5km_nhd_variables_extracted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "necessary-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged.to_pickle(\"combined_regular_with_ssurgo_nhd_2.5kmX2.5km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ethical-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"combined_regular_with_ssurgo_nhd_2.5kmX2.5km.csv\")\n",
    "# df.to_pickle(\"combined_regular_with_ssurgo_nhd_2.5kmX2.5km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "loaded-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"combined_regular_with_ssurgo_nhd_200mX200m.csv\")\n",
    "# df.to_pickle(\"combined_regular_with_ssurgo_nhd_200mX200m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "atmospheric-median",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>rha_determination</th>\n",
       "      <th>cwa_determination</th>\n",
       "      <th>rha1</th>\n",
       "      <th>rha2</th>\n",
       "      <th>cwa1</th>\n",
       "      <th>cwa2</th>\n",
       "      <th>...</th>\n",
       "      <th>fl_divergence_mean</th>\n",
       "      <th>wb_area_sum</th>\n",
       "      <th>wb_area_count</th>\n",
       "      <th>wb_area_mean</th>\n",
       "      <th>wb_gnis_name_ind_sum</th>\n",
       "      <th>wb_gnis_name_ind_count</th>\n",
       "      <th>wb_gnis_name_ind_mean</th>\n",
       "      <th>fl_length_sum</th>\n",
       "      <th>fl_length_count</th>\n",
       "      <th>fl_length_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>1.461900e+04</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>1.461900e+04</td>\n",
       "      <td>6908.0</td>\n",
       "      <td>14619.000000</td>\n",
       "      <td>6908.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7309.000000</td>\n",
       "      <td>7309.000000</td>\n",
       "      <td>-90.491852</td>\n",
       "      <td>37.209405</td>\n",
       "      <td>0.104932</td>\n",
       "      <td>0.369588</td>\n",
       "      <td>0.070935</td>\n",
       "      <td>0.062727</td>\n",
       "      <td>0.093235</td>\n",
       "      <td>0.063889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037571</td>\n",
       "      <td>239.810967</td>\n",
       "      <td>0.865107</td>\n",
       "      <td>174.044895</td>\n",
       "      <td>1.890082e+05</td>\n",
       "      <td>0.196525</td>\n",
       "      <td>1.495429e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.343320</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4220.286128</td>\n",
       "      <td>4220.286128</td>\n",
       "      <td>15.418083</td>\n",
       "      <td>6.766997</td>\n",
       "      <td>0.306476</td>\n",
       "      <td>0.482710</td>\n",
       "      <td>0.256725</td>\n",
       "      <td>0.242479</td>\n",
       "      <td>0.290771</td>\n",
       "      <td>0.244564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147239</td>\n",
       "      <td>3184.600853</td>\n",
       "      <td>1.846926</td>\n",
       "      <td>2542.119172</td>\n",
       "      <td>5.657580e+05</td>\n",
       "      <td>0.537191</td>\n",
       "      <td>3.998410e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.038685</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-174.197880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3654.500000</td>\n",
       "      <td>3654.500000</td>\n",
       "      <td>-93.638850</td>\n",
       "      <td>32.815575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7309.000000</td>\n",
       "      <td>7309.000000</td>\n",
       "      <td>-88.186500</td>\n",
       "      <td>35.478300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10963.500000</td>\n",
       "      <td>10963.500000</td>\n",
       "      <td>-80.226955</td>\n",
       "      <td>41.403750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.030875</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14618.000000</td>\n",
       "      <td>14618.000000</td>\n",
       "      <td>144.828420</td>\n",
       "      <td>70.483790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>57535.316000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>57516.647000</td>\n",
       "      <td>1.148393e+07</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.036537e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  Unnamed: 0.1     longitude      latitude  \\\n",
       "count  14619.000000  14619.000000  14619.000000  14619.000000   \n",
       "mean    7309.000000   7309.000000    -90.491852     37.209405   \n",
       "std     4220.286128   4220.286128     15.418083      6.766997   \n",
       "min        0.000000      0.000000   -174.197880      0.000000   \n",
       "25%     3654.500000   3654.500000    -93.638850     32.815575   \n",
       "50%     7309.000000   7309.000000    -88.186500     35.478300   \n",
       "75%    10963.500000  10963.500000    -80.226955     41.403750   \n",
       "max    14618.000000  14618.000000    144.828420     70.483790   \n",
       "\n",
       "       rha_determination  cwa_determination          rha1          rha2  \\\n",
       "count       14619.000000       14619.000000  14619.000000  14619.000000   \n",
       "mean            0.104932           0.369588      0.070935      0.062727   \n",
       "std             0.306476           0.482710      0.256725      0.242479   \n",
       "min             0.000000           0.000000      0.000000      0.000000   \n",
       "25%             0.000000           0.000000      0.000000      0.000000   \n",
       "50%             0.000000           0.000000      0.000000      0.000000   \n",
       "75%             0.000000           1.000000      0.000000      0.000000   \n",
       "max             1.000000           1.000000      1.000000      1.000000   \n",
       "\n",
       "               cwa1          cwa2  ...  fl_divergence_mean   wb_area_sum  \\\n",
       "count  14619.000000  14619.000000  ...        14619.000000  14619.000000   \n",
       "mean       0.093235      0.063889  ...            0.037571    239.810967   \n",
       "std        0.290771      0.244564  ...            0.147239   3184.600853   \n",
       "min        0.000000      0.000000  ...            0.000000      0.000000   \n",
       "25%        0.000000      0.000000  ...            0.000000      0.000000   \n",
       "50%        0.000000      0.000000  ...            0.000000      0.000000   \n",
       "75%        0.000000      0.000000  ...            0.000000      0.051000   \n",
       "max        1.000000      1.000000  ...            2.000000  57535.316000   \n",
       "\n",
       "       wb_area_count  wb_area_mean  wb_gnis_name_ind_sum  \\\n",
       "count   14619.000000  14619.000000          1.461900e+04   \n",
       "mean        0.865107    174.044895          1.890082e+05   \n",
       "std         1.846926   2542.119172          5.657580e+05   \n",
       "min         0.000000      0.000000          0.000000e+00   \n",
       "25%         0.000000      0.000000          0.000000e+00   \n",
       "50%         0.000000      0.000000          0.000000e+00   \n",
       "75%         1.000000      0.030875          0.000000e+00   \n",
       "max        26.000000  57516.647000          1.148393e+07   \n",
       "\n",
       "       wb_gnis_name_ind_count  wb_gnis_name_ind_mean  fl_length_sum  \\\n",
       "count            14619.000000           1.461900e+04         6908.0   \n",
       "mean                 0.196525           1.495429e+05            0.0   \n",
       "std                  0.537191           3.998410e+05            0.0   \n",
       "min                  0.000000           0.000000e+00            0.0   \n",
       "25%                  0.000000           0.000000e+00            0.0   \n",
       "50%                  0.000000           0.000000e+00            0.0   \n",
       "75%                  0.000000           0.000000e+00            0.0   \n",
       "max                  7.000000           2.036537e+06            0.0   \n",
       "\n",
       "       fl_length_count  fl_length_mean  \n",
       "count     14619.000000          6908.0  \n",
       "mean          1.343320             0.0  \n",
       "std           2.038685             0.0  \n",
       "min           0.000000             0.0  \n",
       "25%           0.000000             0.0  \n",
       "50%           1.000000             0.0  \n",
       "75%           2.000000             0.0  \n",
       "max          26.000000             0.0  \n",
       "\n",
       "[8 rows x 56 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with open(\"combined_regular_with_ssurgo_nhd_2.5kmX2.5km_pkl3.pkl\", \"rb\") as f:\n",
    "#     df_readfrompkl = pickle.load(f)\n",
    "\n",
    "df = pd.read_pickle(\"2.5kmX2.5km_nhd_variables_extracted\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "reported-mattress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'jurisdiction_type', 'da_number',\n",
       "       'district', 'project_name', 'longitude', 'latitude',\n",
       "       'date_issued_or_denied', 'rha_determination', 'cwa_determination',\n",
       "       'rha1', 'rha2', 'cwa1', 'cwa2', 'cwa3', 'cwa4', 'cwa5', 'cwa6', 'cwa7',\n",
       "       'cwa8', 'cwa9', 'potential_wetland', 'index', 'Index', 'mukey',\n",
       "       'hydclprs', 'aws025wta', 'drclassdcd', 'nhd_vars_wb', 'nhd_vars_fl',\n",
       "       'wb_comid_list', 'wb_ftype_str_list', 'wb_gnis_id_list', 'wb_area_list',\n",
       "       'fl_comid_list', 'fl_ftype_str_list', 'fl_gnis_id_list',\n",
       "       'fl_length_list', 'fl_areasqkm_sum', 'fl_areasqkm_count',\n",
       "       'fl_areasqkm_mean', 'fl_gnis_name_ind_sum', 'fl_gnis_name_ind_count',\n",
       "       'fl_gnis_name_ind_mean', 'fl_totdasqkm_sum', 'fl_totdasqkm_count',\n",
       "       'fl_totdasqkm_mean', 'fl_flow_type_sum', 'fl_flow_type_count',\n",
       "       'fl_flow_type_mean', 'fl_streamorde_sum', 'fl_streamorde_count',\n",
       "       'fl_streamorde_mean', 'fl_intephem_sum', 'fl_intephem_count',\n",
       "       'fl_intephem_mean', 'fl_startflag_sum', 'fl_startflag_count',\n",
       "       'fl_startflag_mean', 'fl_divergence_sum', 'fl_divergence_count',\n",
       "       'fl_divergence_mean', 'wb_comid_str', 'wb_ftype_str', 'wb_gnis_id_str',\n",
       "       'wb_area_sum', 'wb_area_count', 'wb_area_mean', 'wb_gnis_name_ind_sum',\n",
       "       'wb_gnis_name_ind_count', 'wb_gnis_name_ind_mean', 'fl_comid_str',\n",
       "       'fl_ftype_str', 'fl_gnis_id_str', 'fl_length_sum', 'fl_length_count',\n",
       "       'fl_length_mean'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-vault",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
