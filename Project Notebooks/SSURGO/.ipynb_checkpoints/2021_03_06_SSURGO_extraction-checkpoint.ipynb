{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f36_nSgAAOP"
   },
   "source": [
    "# Authentications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGNyccn1n8ig",
    "outputId": "dd3f12d0-e1a7-45b9-ea0f-6c0f903a6d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Running command using Cloud API.  Set --no-use_cloud_api to go back to using the API\n",
      "\n",
      "To authorize access needed by Earth Engine, open the following URL in a web browser and follow the instructions. If the web browser does not start automatically, please manually browse the URL below.\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=U9wC2k_xmN2goO9j2Vsf3DfaX-8cnNQDBhX306c9AuM&code_challenge_method=S256\n",
      "\n",
      "The authorization workflow will generate a code, which you should paste in the box below. \n",
      "Enter verification code: 4/1AY0e-g6DGOKcurxaEiVWM35kFKcyik_Ckw-HieUbp8OXKQvF1sbX30Hgymk\n",
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "# PLEASE USE YOUR INDIVIDUAL GEE ACCOUNT\n",
    "\n",
    "!earthengine authenticate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pYNY3YLn9zJ"
   },
   "outputs": [],
   "source": [
    "# PLEASE USE YOUR INDIVIDUAL GEE ACCOUNT\n",
    "# authenticate to Google Colab\n",
    "\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkMz8_N5n92p",
    "outputId": "ad283336-f2f1-42b0-b738-f1ec2bccac5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# USE MIDSCWA@gmail.com/cleanwater\n",
    "# to access csv file\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOhZOzOln96G"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import ee\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62SEig6EPDZ7"
   },
   "source": [
    "# 3. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_aMcWg6_13U"
   },
   "outputs": [],
   "source": [
    "# Sentinel Level 2A surface reflectances\n",
    "# Note: 2A is a processed file whereby Level 1A top-of-atmosphere TOA \n",
    "# reflectance is converted to surface reflectance\n",
    "\n",
    "\n",
    "# Use the latest Sentinel-2 Cloud Masking with s2cloudless\n",
    "# https://developers.google.com/earth-engine/tutorials/community/sentinel-2-s2cloudless\n",
    "# Parameter | Type\t| Description\n",
    "# AOI\tee.Geometry\tArea of interest\n",
    "# START_DATE\tstring\tImage collection start date (inclusive)\n",
    "# END_DATE\tstring\tImage collection end date (exclusive)\n",
    "# CLOUD_FILTER\tinteger\tMaximum image cloud cover percent allowed in image \n",
    "# collection\n",
    "# CLD_PRB_THRESH\tinteger\tCloud probability (%); values greater than are \n",
    "# considered cloud\n",
    "# NIR_DRK_THRESH\tfloat\tNear-infrared reflectance; values less than are considered \n",
    "# potential cloud shadow\n",
    "# CLD_PRJ_DIST\tfloat\tMaximum distance (km) to search for cloud shadows from \n",
    "# cloud edges\n",
    "# BUFFER\tinteger\tDistance (m) to dilate the edge of cloud-identified objects\n",
    "\n",
    "\n",
    "START_DATE = '2018-08-01'\n",
    "END_DATE = '2020-04-01'\n",
    "CLOUD_FILTER = 60\n",
    "CLD_PRB_THRESH = 40\n",
    "NIR_DRK_THRESH = 0.15\n",
    "CLD_PRJ_DIST = 2\n",
    "BUFFER = 100\n",
    "\n",
    "# # Build a Sentinel-2 collection\n",
    "# # Sentinel-2 surface reflectance and Sentinel-2 cloud probability are two \n",
    "# different image collections. Each collection must be filtered similarly \n",
    "# (e.g., by date and bounds) and then the two filtered collections must \n",
    "# be joined.\n",
    "\n",
    "# # Define a function to filter the SR and s2cloudless collections according \n",
    "# to area of interest and date parameters, then join them on the system:index \n",
    "# property. The result is a copy of the SR collection where each image has a \n",
    "# new 's2cloudless' property whose value is the corresponding s2cloudless image.\n",
    "\n",
    "def get_s2_sr_cld_col(aoi, start_date, end_date):\n",
    "    # Import and filter S2 SR.\n",
    "    s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date)\n",
    "        .filter(ee.Filter.lte('CLOUDY_PIXEL_PERCENTAGE', CLOUD_FILTER)))\n",
    "\n",
    "    # Import and filter s2cloudless.\n",
    "    s2_cloudless_col = (ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "        .filterBounds(aoi)\n",
    "        .filterDate(start_date, end_date))\n",
    "\n",
    "    # Join the filtered s2cloudless collection to the SR collection by \n",
    "    # the 'system:index' property.\n",
    "    return ee.ImageCollection(ee.Join.saveFirst('s2cloudless').apply(**{\n",
    "        'primary': s2_sr_col,\n",
    "        'secondary': s2_cloudless_col,\n",
    "        'condition': ee.Filter.equals(**{\n",
    "            'leftField': 'system:index',\n",
    "            'rightField': 'system:index'\n",
    "        })\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F9bVlKz2ka6E"
   },
   "outputs": [],
   "source": [
    "def add_cloud_bands(img):\n",
    "    # Get s2cloudless image, subset the probability band.\n",
    "    cld_prb = ee.Image(img.get('s2cloudless')).select('probability')\n",
    "\n",
    "    # Condition s2cloudless by the probability threshold value.\n",
    "    is_cloud = cld_prb.gt(CLD_PRB_THRESH).rename('clouds')\n",
    "\n",
    "    # Add the cloud probability layer and cloud mask as image bands.\n",
    "    return img.addBands(ee.Image([cld_prb, is_cloud]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqexfNCnkeuA"
   },
   "outputs": [],
   "source": [
    "def add_shadow_bands(img):\n",
    "    # Identify water pixels from the SCL band.\n",
    "    not_water = img.select('SCL').neq(6)\n",
    "\n",
    "    # Identify dark NIR pixels that are not water (potential cloud shadow pixels).\n",
    "    SR_BAND_SCALE = 1e4\n",
    "    dark_pixels = img.select('B8').lt(NIR_DRK_THRESH*SR_BAND_SCALE).multiply(not_water).rename('dark_pixels')\n",
    "\n",
    "    # Determine the direction to project cloud shadow from clouds (assumes UTM projection).\n",
    "    shadow_azimuth = ee.Number(90).subtract(ee.Number(img.get('MEAN_SOLAR_AZIMUTH_ANGLE')));\n",
    "\n",
    "    # Project shadows from clouds for the distance specified by the CLD_PRJ_DIST input.\n",
    "    cld_proj = (img.select('clouds').directionalDistanceTransform(shadow_azimuth, CLD_PRJ_DIST*10)\n",
    "        .reproject(**{'crs': img.select(0).projection(), 'scale': 100})\n",
    "        .select('distance')\n",
    "        .mask()\n",
    "        .rename('cloud_transform'))\n",
    "\n",
    "    # Identify the intersection of dark pixels with cloud shadow projection.\n",
    "    shadows = cld_proj.multiply(dark_pixels).rename('shadows')\n",
    "\n",
    "    # Add dark pixels, cloud projection, and identified shadows as image bands.\n",
    "    return img.addBands(ee.Image([dark_pixels, cld_proj, shadows]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ej9_E_r2kNuX"
   },
   "outputs": [],
   "source": [
    "def add_cld_shdw_mask(img):\n",
    "    # Add cloud component bands.\n",
    "    img_cloud = add_cloud_bands(img)\n",
    "\n",
    "    # Add cloud shadow component bands.\n",
    "    img_cloud_shadow = add_shadow_bands(img_cloud)\n",
    "\n",
    "    # Combine cloud and shadow mask, set cloud and shadow as value 1, else 0.\n",
    "    is_cld_shdw = img_cloud_shadow.select('clouds').add(img_cloud_shadow.select('shadows')).gt(0)\n",
    "\n",
    "    # Remove small cloud-shadow patches and dilate remaining pixels by BUFFER input.\n",
    "    # 20 m scale is for speed, and assumes clouds don't require 10 m precision.\n",
    "    is_cld_shdw = (is_cld_shdw.focal_min(2).focal_max(BUFFER*2/20)\n",
    "        .reproject(**{'crs': img.select([0]).projection(), 'scale': 20})\n",
    "        .rename('cloudmask'))\n",
    "\n",
    "    # Add the final cloud-shadow mask to the image.\n",
    "    return img_cloud_shadow.addBands(is_cld_shdw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EU750KEkN4X"
   },
   "outputs": [],
   "source": [
    "def apply_cld_shdw_mask(img):\n",
    "    # Subset the cloudmask band and invert it so clouds/shadow are 0, else 1.\n",
    "    not_cld_shdw = img.select('cloudmask').Not()\n",
    "\n",
    "    # Subset reflectance bands and update their masks, return the result.\n",
    "    return img.select('B.*').updateMask(not_cld_shdw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3v_crqO313l"
   },
   "outputs": [],
   "source": [
    "def s2_sr_median_func(lat, lon, buffer_m):\n",
    "  AOI = ee.Geometry.Point([lon, lat])#.buffer(res).bounds()\n",
    "  s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "  s2_sr_median = (s2_sr_cld_col.map(add_cld_shdw_mask)\n",
    "                             .map(apply_cld_shdw_mask)\n",
    "                             .median())\n",
    "  return s2_sr_median\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggwS5nwCFiHw"
   },
   "source": [
    "# Set up bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSFWBtG4Fv9X"
   },
   "outputs": [],
   "source": [
    "def square(lat, lon, size):\n",
    "  crs_proj = \"EPSG:4326\"  \n",
    "  return ee.Geometry.Point([lon, lat], proj=crs_proj).buffer(size).bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B47yUe4SATbx"
   },
   "source": [
    "# SRTM, JRC and NDVI etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjQ9sH-Gn-Cq"
   },
   "outputs": [],
   "source": [
    "# SRTM for elevation\n",
    "srtm = ee.Image('USGS/SRTMGL1_003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NuSIR949X2GG"
   },
   "outputs": [],
   "source": [
    "# slope of terrain\n",
    "slope = ee.Terrain.slope(srtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9VmW0Whdnq5b",
    "outputId": "8320f260-cf8b-4451-91c0-57e6039004c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seasonality', 'transition', 'max_extent']"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add JRC bands of interest\n",
    "jrc = ee.Image(\"JRC/GSW1_2/GlobalSurfaceWater\")\n",
    "jrc_bands = jrc.select(\"seasonality\", \"transition\", \"max_extent\")\\\n",
    "                .bandNames().getInfo()\n",
    "jrc = jrc.select(jrc_bands)\n",
    "jrc.bandNames().getInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7BzVImhV_nh"
   },
   "outputs": [],
   "source": [
    "def make_ndvi(image, red='B4', nir='B8'):\n",
    "  return image.normalizedDifference([nir, red])  \n",
    "\n",
    "def make_ndwi(image, green='B3', nir='B8'):\n",
    "  return image.normalizedDifference([green, nir])  \n",
    "\n",
    "\n",
    "def make_mndvi(image, red='B4', nir='B8'):\n",
    "  nir = image.select('B8')\n",
    "  red = image.select('B4')\n",
    "  aerosols = image.select('B1')  \n",
    "  mndvi = (nir.subtract(red)\n",
    "                      .divide(\n",
    "                          nir.add(red)\n",
    "                          .subtract(aerosols.add(aerosols))\n",
    "                          )\n",
    "                      .rename('mndvi'))\n",
    "  return mndvi\n",
    "\n",
    "def make_mndwi(image, green='B3', swir='B11'):\n",
    "  green = image.select('B3')\n",
    "  swir = image.select('B11')\n",
    "  mndwi = (green.subtract(swir)\n",
    "                .divide(\n",
    "                    green.add(swir))\n",
    "                .rename('mndwi'))\n",
    "  return mndwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSM5VUj7nynK"
   },
   "outputs": [],
   "source": [
    "# choose median of mndwi image collection\n",
    "def make_med_mndwi(lat, lon, buffer_m):\n",
    "  AOI = ee.Geometry.Point([lon, lat])#.buffer(res).bounds()\n",
    "  s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "  med_mndwi = (s2_sr_cld_col.map(add_cld_shdw_mask)\n",
    "                             .map(apply_cld_shdw_mask)\n",
    "                             .map(make_mndwi)                            \n",
    "                             .median()\n",
    "                             )\n",
    "  return med_mndwi # returns the image with median mndwi in the date range\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4nXxi5um8bZ"
   },
   "outputs": [],
   "source": [
    "# choose the max water pixel from mndwi image collection\n",
    "def s2_sr_greenestpixel_func2(lat, lon, buffer_m):\n",
    "  AOI = ee.Geometry.Point([lon, lat])#.buffer(res).bounds()\n",
    "  s2_sr_cld_col = get_s2_sr_cld_col(AOI, START_DATE, END_DATE)\n",
    "  s2_sr_greenestpixel = (s2_sr_cld_col.map(add_cld_shdw_mask)\n",
    "                             .map(apply_cld_shdw_mask)\n",
    "                             .map(make_mndwi)                            \n",
    "                             .qualityMosaic('mndwi')\n",
    "                             )\n",
    "  # returns the image with highest mndwi in the date range\n",
    "  return s2_sr_greenestpixel \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tt96ySPAYIn"
   },
   "source": [
    "# Read in Rapanos dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_t_J2SataRZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# datapath = \"drive/MyDrive/Data/combined_v2.csv\"\n",
    "datapath = \"/content/drive/MyDrive/Data/combined_regular_clean.csv\"\n",
    "df = pd.read_csv(datapath, encoding = \"ISO-8859-1\")\n",
    "\n",
    "# column name 'index' is conflicting with the native index of dataframe\n",
    "# hence, creating a new column named \"Index\"\n",
    "df[\"Index\"] = df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMZyUm24AcWU"
   },
   "source": [
    "# Set up global variables for Export/Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPov83g1tiGH"
   },
   "outputs": [],
   "source": [
    "# INSERT YOUR BUCKET HERE:\n",
    "BUCKET = 'pollutemenot-ai'\n",
    "# FOLDER = 'test_final'\n",
    "FOLDER = \"GEE_images_final2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9IYvD7NLC5T"
   },
   "source": [
    "# Exporting Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9KsSAfHL4nj"
   },
   "outputs": [],
   "source": [
    "def doExport_RBG2(lat, lon, index_danum, band, size=1000):\n",
    "  image = s2_sr_median_func(lat, lon, size)\n",
    "  image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "  imageRGB = image.visualize(**{'bands': ['B4', 'B3', 'B2'], 'max': 9000, 'min': 0.5})\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  else:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = imageRGB, \n",
    "      description = index_danum,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,      \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,            \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uu0ei3jQLBBm"
   },
   "outputs": [],
   "source": [
    "def doExport_index2(lat, lon, index_danum, band, size=1000, func=make_ndvi):\n",
    "  image = s2_sr_median_func(lat, lon, size)\n",
    "  # image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = func(image), \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sLq5PXm_o2Wy"
   },
   "outputs": [],
   "source": [
    "def doExport_mmndwi(lat, lon, index_danum, band=\"gmndwi\", size=1000, func=None):\n",
    "  image = make_med_mndwi(lat, lon, size)\n",
    "  # image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kak-wbbWpOH2"
   },
   "outputs": [],
   "source": [
    "def doExport_gmndwi(lat, lon, index_danum, band=\"gmndwi\", size=1000, func=None):\n",
    "  image = s2_sr_greenestpixel_func2(lat, lon, size)\n",
    "  # image = image.select('B4', 'B3', 'B2', 'B8')\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fciKiExpLBBm"
   },
   "outputs": [],
   "source": [
    "def doExport_srtm2(lat, lon, index_danum, band, size=1000, func=None):\n",
    "  image = ee.Image('USGS/SRTMGL1_003')\n",
    "  # image = ee.Terrain.slope(srtm)\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tewPrA5LBBn"
   },
   "outputs": [],
   "source": [
    "def doExport_slope2(lat, lon, index_danum, band, size=1000, func=None):\n",
    "  srtm = ee.Image('USGS/SRTMGL1_003')\n",
    "  image = ee.Terrain.slope(srtm)\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = image, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5DAwuVxLBBn"
   },
   "outputs": [],
   "source": [
    "def doExport_jrc2(lat, lon, index_danum, band, size=1000, func=None, res='hires'):\n",
    "  jrc = ee.Image(\"JRC/GSW1_2/GlobalSurfaceWater\")\n",
    "  # jrc_bands = jrc.select(\"seasonality\", \"transition\", \"max_extent\")\\\n",
    "                # .bandNames().getInfo()\n",
    "  if band == \"transition\":\n",
    "    jrc = jrc.select(\"transition\")\n",
    "  elif band == \"max_extent\":\n",
    "    jrc = jrc.select(\"max_extent\")\n",
    "  else:\n",
    "    jrc = jrc.select(\"seasonality\")\n",
    "\n",
    "  if size == 1000:\n",
    "    size_ = \"hires\"\n",
    "  elif size == 10000:\n",
    "    size_ = 'lores'\n",
    "  folder = FOLDER\n",
    "  \n",
    "  task = ee.batch.Export.image.toCloudStorage(\n",
    "      image = jrc, \n",
    "      description = index_danum + '_' + size_,\n",
    "      bucket = BUCKET,\n",
    "      # fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum  + band + size_,            \n",
    "      fileNamePrefix = folder + '/' + size_ + '/' + band + '/' + index_danum + '_' + band + '_' + size_,      \n",
    "      region = square(lat, lon, size).getInfo().get('coordinates'),\n",
    "      crs = 'EPSG:4326',\n",
    "      # crs_transform = crs_transform,\n",
    "      dimensions = \"256x256\",\n",
    "      maxPixels = 1E13,\n",
    "      fileFormat = \"GeoTIFF\",\n",
    "      formatOptions = {\n",
    "      \"cloudOptimized\" : True\n",
    "      }\n",
    "      )\n",
    "  task.start()\n",
    "    # Block until the task completes.\n",
    "  # print('Running image export to Cloud Storage...')\n",
    "  import time\n",
    "  while task.active():\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUJSFiJ5KSwV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def export_data2(index_danum, lat, lon):\n",
    "  for size in [1000, 10000]:\n",
    "    # doExport_RBG2(lat, lon, index_danum, 'RBG', size)\n",
    "    # doExport_index2(lat, lon, index_danum, 'ndvi', size, make_ndvi)\n",
    "    # doExport_index2(lat, lon, index_danum, 'ndwi', size, make_ndwi)\n",
    "    doExport_index2(lat, lon, index_danum, 'mndvi', size, make_mndvi)\n",
    "    doExport_index2(lat, lon, index_danum, 'mndwi', size, make_mndwi)\n",
    "    doExport_gmndwi(lat, lon, index_danum, 'gmndwi', size, None)\n",
    "    # doExport_gmndwi(lat, lon, index_danum, 'mmndwi', size, None)\n",
    "    doExport_srtm2(lat, lon, index_danum, 'srtm', size, None)\n",
    "    # doExport_slope2(lat, lon, index_danum, 'slope', size, None)\n",
    "    if size == 1000:\n",
    "      doExport_jrc2(lat, lon, index_danum, 'seasonality', size, None)\n",
    "      doExport_jrc2(lat, lon, index_danum, 'transition', size, None)\n",
    "    # doExport_jrc2(lat, lon, index_danum, 'max_extent', size, None, hires)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9XprqTJDAaRo"
   },
   "source": [
    "# Exporting Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W4n0N8A573Je"
   },
   "outputs": [],
   "source": [
    "# Assigned begin and end of records for each person\n",
    "# MADHUKAR: records 1 - 4000\n",
    "# SHOBHA: records 4000 - 8000\n",
    "# RADHIKA: records 8000 - 12000\n",
    "# JOE: 12000 - last\n",
    "\n",
    "names = [\"MADHUKAR\", 'SHOBHA', 'RADHIKA', 'JOE']\n",
    "start = [1, 4000, 8000, 12000]\n",
    "end = [4000, 8000, 12000, df.shape[0]]\n",
    "MY_NAME = \"MADHUKAR\"\n",
    "\n",
    "start_dict = dict(zip(names, start))\n",
    "end_dict = dict(zip(names, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "8kJySS6YgN9p",
    "outputId": "fbe5e472-de04-43f3-93b3-370e32026e49"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijL-clnf74GG"
   },
   "outputs": [],
   "source": [
    "# index_begin = 1\n",
    "# index_end = 300\n",
    "\n",
    "# if index_begin >= start_dict[MY_NAME] and index_end <= end_dict[MY_NAME]:\n",
    "#   for count in range(index_begin, index_end):\n",
    "#     if count == index_begin: print(\"exporting index =\", count)\n",
    "#     da_number = df.iloc[count-1][\"da_number\"]\n",
    "#     latitude = df.iloc[count-1][\"latitude\"]\n",
    "#     longitude = df.iloc[count-1][\"longitude\"]\n",
    "#     index = count\n",
    "#     export_data2(str(index) + '_' + da_number, latitude, longitude)\n",
    "#     print(\"Done uploading hires and lores of index =\", index)\n",
    "# else:\n",
    "#   print(\"Please ensure the begin and end is within the interval assigned to you\")\n",
    "\n",
    "# print(\"Woohoo all done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULTYN5stCZXl"
   },
   "outputs": [],
   "source": [
    "stop # stope execution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7Y_Ux_ggTCy"
   },
   "source": [
    "# SSURGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GxvR5sNlgUrB",
    "outputId": "4fc7ff05-7013-4bda-b36c-805e1c635ec8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ssurgo_path = \"/content/drive/MyDrive/GeoSpatialData/SSURGO/muaggatt.zip\"\n",
    "df_s = pd.read_csv(ssurgo_path, compression='zip', header=0, sep='\\t', quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6F2QAB0cmMdv"
   },
   "outputs": [],
   "source": [
    "# def find_percent_hydric(lat=None, lon=None):\n",
    "#   ssurgo = ee.Image(\"users/madhukarreddy/gSSURGO\")\n",
    "#   pt = ee.Geometry.Point([lon, lat])\n",
    "#   mukey = ssurgo.select('b1').clip(pt).sample(pt).getInfo()[\"features\"][0]['properties']['b1'] \n",
    "#   # print(type(mukey))\n",
    "#   try:\n",
    "#     hydclprs = df_s[df_s.mukey == mukey][\"hydclprs\"]\n",
    "#   except:\n",
    "#     print(mukey)\n",
    "#     return np.nan\n",
    "#   return int(hydclprs)\n",
    "\n",
    "# # lat = float(df[df.index == 100].latitude)\n",
    "# # lon = float(df[df.index == 100].longitude)\n",
    "# find_percent_hydric(37.4811, -121.9641) # this is known wetland, so should read 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYyafNxS2ZjK"
   },
   "outputs": [],
   "source": [
    "# def find_percent_hydric2(mukey):\n",
    "#   try:\n",
    "#     hydclprs = df_s[df_s.mukey == mukey][\"hydclprs\"]\n",
    "#   except:\n",
    "#     print(\"Can not find this mukey in SSURGO\", mukey)\n",
    "#     return np.nan\n",
    "#   return hydclprs\n",
    "\n",
    "# # lat = float(df[df.index == 100].latitude)\n",
    "# # lon = float(df[df.index == 100].longitude)\n",
    "# # find_percent_hydric(37.4811, -121.9641) # this is known wetland, so should read 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNoNaYgspAgb"
   },
   "outputs": [],
   "source": [
    "# def find_percent_hydric3(mukey):\n",
    "#   if mukey != np.nan:\n",
    "#     # hydclprs = df_s[df_s.mukey == mukey][\"hydclprs\"]\n",
    "#     hydclprs = df_s.loc[df_s.mukey == mukey,\"hydclprs\"]\n",
    "#   else:\n",
    "#     return np.nan\n",
    "#   return hydclprs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxG_hwjoSVQ7"
   },
   "source": [
    "# Find mukeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "-7pFTKUFivoj",
    "outputId": "27b81885-bcf2-4c57-b6d8-8d5d13a1dc7a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-21ede0c3ce38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# lat = float(df[df.index == 100].latitude)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# lon = float(df[df.index == 100].longitude)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfind_mukey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m37.4811\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m121.9641\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this is known wetland, so should read 100%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-21ede0c3ce38>\u001b[0m in \u001b[0;36mfind_mukey\u001b[0;34m(lat, lon)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_mukey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mssurgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"users/madhukarreddy/gSSURGO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeometry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmukey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssurgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'properties'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ee' is not defined"
     ]
    }
   ],
   "source": [
    "def find_mukey(lat=None, lon=None):\n",
    "  ssurgo = ee.Image(\"users/madhukarreddy/gSSURGO\")\n",
    "  pt = ee.Geometry.Point([lon, lat])\n",
    "  try:\n",
    "    mukey = ssurgo.select('b1').clip(pt).sample(pt).getInfo()[\"features\"][0]['properties']['b1'] \n",
    "    return mukey\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    return np.nan\n",
    "  \n",
    "# lat = float(df[df.index == 100].latitude)\n",
    "# lon = float(df[df.index == 100].longitude)\n",
    "find_mukey(37.4811, -121.9641) # this is known wetland, so should read 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKFCpSt9ycgp"
   },
   "outputs": [],
   "source": [
    "# for i in range(5):\n",
    "#   # Index = 14000\n",
    "#   Index = i + 1\n",
    "#   lat = df.iloc[Index-1].get(\"latitude\")\n",
    "#   lon = df.iloc[Index-1].get(\"longitude\")\n",
    "#   print(find_mukey(lat, lon), find_percent_hydric(lat, lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ye-wpoo6JSQQ"
   },
   "outputs": [],
   "source": [
    "# # df_s.hydclprs.describe()\n",
    "# df_s.hydclprs.plot.hist(bins=100)\n",
    "# # df_s.mukey.plot.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVoZOBLMwoBB"
   },
   "outputs": [],
   "source": [
    "# temp = df.head().copy()\n",
    "# # temp[\"percent_hydric\"] = find_percent_hydric(temp.latitude, temp.longitude)\n",
    "# temp[\"mukey\"] = temp.apply(lambda x: find_mukey(x.latitude, x.longitude), axis=1)\n",
    "# temp[\"percent_hydric\"] = temp.apply(lambda x: find_percent_hydric2(x.mukey), axis=1)\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y-r6T2Bd-6RM"
   },
   "outputs": [],
   "source": [
    "# # extract mukey from GEE asset\n",
    "# df[\"mukey\"] = df.apply(lambda x: find_mukey(x.latitude, x.longitude), axis=1)\n",
    "\n",
    "# # join to SSURGO to find the percent hydricity for a given mukey\n",
    "# df[\"percent_hydric\"] = df.apply(lambda x: find_percent_hydric2(x.mukey), axis=1)\n",
    "\n",
    "# # saving the dataframe \n",
    "# df.to_csv('/content/drive/MyDrive/Data/combined_regular_clean_with_hydclprs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2l7BUfPFn3Q"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# datapath = \"/content/drive/MyDrive/Data/combined_regular_clean_with_hydclprs.csv\"\n",
    "# temp = pd.read_csv(datapath, encoding = \"ISO-8859-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wP58hOlc85v"
   },
   "outputs": [],
   "source": [
    "df[\"mukey\"] = np.nan\n",
    "df[\"hydclprs\"] = np.nan\n",
    "\n",
    "# True if you want to recalculate mukeys\n",
    "calculate_mukeys = False\n",
    "if calculate_mukeys:\n",
    "  for i in range(df.shape[0]):\n",
    "    lat = df.iloc[i].get(\"latitude\")\n",
    "    lon = df.iloc[i].get(\"longitude\")\n",
    "    mukey_ = find_mukey(lat, lon)\n",
    "    df.loc[i, \"mukey\"] = mukey_\n",
    "    # df.loc[i, \"hydclprs\"] = find_percent_hydric2(mukey_)\n",
    "    if i % 1000 == 0:\n",
    "      print(\"On index\", i)\n",
    "\n",
    "# saving the dataframe \n",
    "# df.to_csv('/content/drive/MyDrive/Data/combined_regular_clean_with_mukey.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuLtE4SdNc0p"
   },
   "source": [
    "# Convert mukey to ssurgo variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOLV1hqEaWFe"
   },
   "outputs": [],
   "source": [
    "mukey_path = \"/content/drive/MyDrive/Data/combined_regular_clean_with_mukey.csv\"\n",
    "df_mukey = pd.read_csv(mukey_path)\n",
    "# df_mukey.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQTL3bnsjwfZ",
    "outputId": "43a4b56b-4142-4d47-fbc5-5a854865d18c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(560, 26)"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are 560 records without a valid mukey (Alaska and Hawaii)\n",
    "df_mukey[df_mukey.mukey.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMLvq88hsaT8"
   },
   "outputs": [],
   "source": [
    "def find_percent_hydric3(mukey):\n",
    "  if mukey != np.nan:\n",
    "    hydclprs = df_s[df_s.mukey == mukey][\"hydclprs\"]\n",
    "    # hydclprs = df_s.loc[df_s.mukey == mukey,\"hydclprs\"]\n",
    "  else:\n",
    "    return np.nan\n",
    "    # return np.array(np.nan)\n",
    "  return hydclprs.mean()\n",
    "  # return hydclprs.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gURrcy_XhENG"
   },
   "outputs": [],
   "source": [
    "def find_aws025wta(mukey):\n",
    "  if mukey != np.nan:\n",
    "    aws025wta = df_s[df_s.mukey == mukey][\"aws025wta\"]\n",
    "  else:\n",
    "    return np.nan\n",
    "    # return np.array(np.nan)\n",
    "  return aws025wta.mean()\n",
    "  # return aws025wta.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohlX0ul4ghFQ"
   },
   "outputs": [],
   "source": [
    "def find_drclassdcd(mukey):\n",
    "  if mukey != np.nan:\n",
    "    drclassdcd = df_s[df_s.mukey == mukey][\"drclassdcd\"]\n",
    "  else:\n",
    "    return np.nan\n",
    "  try:\n",
    "    result = drclassdcd.values[0]\n",
    "  except:\n",
    "    result = np.nan\n",
    "  return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "c4emlpJcYFdo",
    "outputId": "476a3e9e-d799-4522-e7fd-572556a56377"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-3f21172dbcb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmukey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmukey_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hydclprs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmukey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmukey_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"aws025wta\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmukey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmukey_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"drclassdcd\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 8 different text categories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"hydclprs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aws025wta\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"drclassdcd\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#.dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mukey_' is not defined"
     ]
    }
   ],
   "source": [
    "df_s[df_s.mukey == mukey_][\"hydclprs\"] # int\n",
    "df_s[df_s.mukey == mukey_][\"aws025wta\"] # float\n",
    "df_s[df_s.mukey == mukey_][\"drclassdcd\"] # 8 different text categories\n",
    "\n",
    "df_s[[\"hydclprs\", \"aws025wta\", \"drclassdcd\"]]#.dtypes\n",
    "# df_s.columns\n",
    "\n",
    "df_s.drclassdcd.describe() # 8 different text categories\n",
    "df_s[df_s.drclassdcd.isna()].groupby(\"drclassdcd\").count()[\"mukey\"].sum()# - df_s.shape[0]\n",
    "df_s.groupby(\"drclassdcd\").count()[\"mukey\"].sum()# - df_s.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2zqjrfnldf7"
   },
   "outputs": [],
   "source": [
    "# df_s[df_s.drclassdcd.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iBilWbm7trpW",
    "outputId": "28b769aa-8580-4edc-faa3-8482ab95f9e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'jurisdiction_type', 'da_number', 'district',\n",
       "       'project_name', 'longitude', 'latitude', 'date_issued_or_denied',\n",
       "       'rha_determination', 'cwa_determination', 'rha1', 'rha2', 'cwa1',\n",
       "       'cwa2', 'cwa3', 'cwa4', 'cwa5', 'cwa6', 'cwa7', 'cwa8', 'cwa9',\n",
       "       'potential_wetland', 'index', 'Index', 'mukey', 'hydclprs'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mukey.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHrQpBvltyAl",
    "outputId": "0465a104-1a53-40ca-8ae8-bf17aacd4f59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['musym', 'muname', 'mustatus', 'slopegradd', 'slopegradw', 'brockdepmi',\n",
       "       'wtdepannmi', 'wtdepaprju', 'flodfreqdc', 'flodfreqma', 'pondfreqpr',\n",
       "       'aws025wta', 'aws050wta', 'aws0100wta', 'aws0150wta', 'drclassdcd',\n",
       "       'drclasswet', 'hydgrpdcd', 'iccdcd', 'iccdcdpct', 'niccdcd',\n",
       "       'niccdcdpct', 'engdwobdcd', 'engdwbdcd', 'engdwbll', 'engdwbml',\n",
       "       'engstafdcd', 'engstafll', 'engstafml', 'engsldcd', 'engsldcp',\n",
       "       'englrsdcd', 'engcmssdcd', 'engcmssmp', 'urbrecptdc', 'urbrecptwt',\n",
       "       'forpehrtdc', 'hydclprs', 'awmmfpwwta', 'mukey'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOFoTOu0u_qq"
   },
   "outputs": [],
   "source": [
    "def find_slopegradd(mukey):\n",
    "  if mukey != np.nan:\n",
    "    slopegradd = df_s[df_s.mukey == mukey][\"slopegradd\"]\n",
    "  else:\n",
    "    return np.nan\n",
    "  try:\n",
    "    result = slopegradd.values[0]\n",
    "  except:\n",
    "    result = np.nan\n",
    "  return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6FJ4f62gvj2I"
   },
   "outputs": [],
   "source": [
    "def find_wtdepannmi(mukey):\n",
    "  if mukey != np.nan:\n",
    "    wtdepannmi = df_s[df_s.mukey == mukey][\"wtdepannmi\"]\n",
    "  else:\n",
    "    return np.nan\n",
    "  try:\n",
    "    result = wtdepannmi.values[0]\n",
    "  except:\n",
    "    result = np.nan\n",
    "  return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EahXLnIXvt1c"
   },
   "outputs": [],
   "source": [
    "def find_flodfreqdc(mukey):\n",
    "  if mukey != np.nan:\n",
    "    flodfreqdc = df_s[df_s.mukey == mukey][\"flodfreqdc\"]\n",
    "  else:\n",
    "    return np.nan\n",
    "  try:\n",
    "    result = flodfreqdc.values[0]\n",
    "  except:\n",
    "    result = np.nan\n",
    "  return result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCS22n2Pv5jT"
   },
   "outputs": [],
   "source": [
    "def find_wtdepaprju(mukey):\n",
    "  if mukey != np.nan:\n",
    "    wtdepaprju = df_s[df_s.mukey == mukey][\"wtdepaprju\"]\n",
    "  else:\n",
    "    return np.nan\n",
    "  try:\n",
    "    result = wtdepaprju.values[0]\n",
    "  except:\n",
    "    result = np.nan\n",
    "  return result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PCORVJKUvXRr",
    "outputId": "a28667ab-bd34-486b-d912-0b6c1a13f423"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "mukey = 289404.0\n",
    "find_wtdepaprju(mukey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvtXwyj9l3_c"
   },
   "outputs": [],
   "source": [
    "# join to SSURGO to find the percent hydricity for a given mukey\n",
    "calculate_ssurgo_variable = True\n",
    "if calculate_ssurgo_variable:\n",
    "  df_mukey[\"hydclprs\"] = df_mukey.apply(lambda x: find_percent_hydric3(x.mukey), axis=1)\n",
    "  df_mukey[\"aws025wta\"] = df_mukey.apply(lambda x: find_aws025wta(x.mukey), axis=1)\n",
    "  df_mukey[\"drclassdcd\"] = df_mukey.apply(lambda x: find_drclassdcd(x.mukey), axis=1)\n",
    "  df_mukey[\"slopegradd\"] = df_mukey.apply(lambda x: find_slopegradd(x.mukey), axis=1)\n",
    "  df_mukey[\"wtdepannmi\"] = df_mukey.apply(lambda x: find_wtdepannmi(x.mukey), axis=1)\n",
    "  df_mukey[\"flodfreqdc\"] = df_mukey.apply(lambda x: find_flodfreqdc(x.mukey), axis=1)\n",
    "  df_mukey[\"pondfreqpr\"] = df_mukey.apply(lambda x: find_pondfreqpr(x.mukey), axis=1)\n",
    "  df_mukey[\"wtdepaprju\"] = df_mukey.apply(lambda x: find_wtdepaprju(x.mukey), axis=1)\n",
    "  \n",
    "\n",
    "# saving the dataframe \n",
    "df_mukey.to_csv('/content/drive/MyDrive/Data/combined_regular_clean_with_ssurgo_variables_updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zcZp9vIjsGLf",
    "outputId": "7b9ab211-6f9a-4cfc-94d7-0c51aacb44d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///content/drive/MyDrive/Data/combined_regular_clean_with_ssurgo_variables_updated.csv [Content-Type=text/csv]...\n",
      "/ [1/1 files][  3.2 MiB/  3.2 MiB] 100% Done                                    \n",
      "Operation completed over 1 objects/3.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# push to GCS\n",
    "!gsutil -m cp -r  \"/content/drive/MyDrive/Data/combined_regular_clean_with_ssurgo_variables_updated.csv\" \"gs://pollutemenot-ai/SSURGO/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "J-17POuWNHKk",
    "outputId": "1e1a08f3-a554-494e-f45b-f4a9eb566472"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>jurisdiction_type</th>\n",
       "      <th>da_number</th>\n",
       "      <th>district</th>\n",
       "      <th>project_name</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>date_issued_or_denied</th>\n",
       "      <th>rha_determination</th>\n",
       "      <th>cwa_determination</th>\n",
       "      <th>rha1</th>\n",
       "      <th>rha2</th>\n",
       "      <th>cwa1</th>\n",
       "      <th>cwa2</th>\n",
       "      <th>cwa3</th>\n",
       "      <th>cwa4</th>\n",
       "      <th>cwa5</th>\n",
       "      <th>cwa6</th>\n",
       "      <th>cwa7</th>\n",
       "      <th>cwa8</th>\n",
       "      <th>cwa9</th>\n",
       "      <th>potential_wetland</th>\n",
       "      <th>index</th>\n",
       "      <th>Index</th>\n",
       "      <th>mukey</th>\n",
       "      <th>hydclprs</th>\n",
       "      <th>aws025wta</th>\n",
       "      <th>drclassdcd</th>\n",
       "      <th>slopegradd</th>\n",
       "      <th>wtdepannmi</th>\n",
       "      <th>flodfreqdc</th>\n",
       "      <th>pondfreqpr</th>\n",
       "      <th>wtdepaprju</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>LRB-1983-10120</td>\n",
       "      <td>Buffalo</td>\n",
       "      <td>Trade-A-Yacht (Hibiscus Harbor - Union Springs...</td>\n",
       "      <td>-76.70773</td>\n",
       "      <td>42.85821</td>\n",
       "      <td>06/19/2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>289404.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>7.40</td>\n",
       "      <td>Very poorly drained</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>LRB-1985-69031</td>\n",
       "      <td>Buffalo</td>\n",
       "      <td>POOLEY, MARK A.</td>\n",
       "      <td>-75.85524</td>\n",
       "      <td>43.15230</td>\n",
       "      <td>07/07/2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>293511.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>LRB-1986-99614</td>\n",
       "      <td>Buffalo</td>\n",
       "      <td>Bellamy, Michael (previous: MACKO, JOHN)</td>\n",
       "      <td>-78.04046</td>\n",
       "      <td>42.68911</td>\n",
       "      <td>10/12/2017</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>295436.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>LRB-1990-97632</td>\n",
       "      <td>Buffalo</td>\n",
       "      <td>WESTWOOD COUNTRY CLUB</td>\n",
       "      <td>-78.77134</td>\n",
       "      <td>42.97994</td>\n",
       "      <td>06/28/2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>290780.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>Moderately well drained</td>\n",
       "      <td>2.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>RAPANOS</td>\n",
       "      <td>LRB-1991-98611</td>\n",
       "      <td>Buffalo</td>\n",
       "      <td>MODERN LANDFILL INCORPORATED</td>\n",
       "      <td>-78.97142</td>\n",
       "      <td>43.21616</td>\n",
       "      <td>03/22/2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>293019.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>5.20</td>\n",
       "      <td>Poorly drained</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  ... pondfreqpr wtdepaprju\n",
       "0           0             0  ...       80.0        0.0\n",
       "1           1             1  ...        0.0        0.0\n",
       "2           2             2  ...        0.0        0.0\n",
       "3           3             3  ...        0.0       54.0\n",
       "4           4             4  ...        4.0        0.0\n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m = pd.read_csv(\"/content/drive/MyDrive/Data/combined_regular_clean_with_ssurgo_variables_updated.csv\")\n",
    "df_m.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVpXFsD2UxXy"
   },
   "source": [
    "# Opening GeoTIFF images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4p7GDYFtC0L"
   },
   "source": [
    "## Download GCS contents to GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jzvTSAcF4BR"
   },
   "outputs": [],
   "source": [
    "# create relevant folder for download using %cd and %mkdir\n",
    "# %cd drive/MyDrive/Madhukar/images\n",
    "# %mkdir /content/drive/MyDrive/Madhukar/images3\n",
    "\n",
    "# https://philipplies.medium.com/transferring-data-from-google-drive-to-google-cloud-storage-using-google-colab-96e088a8c041\n",
    "# !gsutil -m cp -r gs://pollutemenot-ai/test3/hires/gmndwi /content/drive/MyDrive/Madhukar/images/test3_1/hires/gmndwi\n",
    "\n",
    "# !gsutil -m cp -r \"gs://pollutemenot-ai/test3/\" \"/content/drive/MyDrive/Madhukar/images3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQNZ1xBzj-Rp"
   },
   "outputs": [],
   "source": [
    "# use !pwd to get local path\n",
    "# dont forget the / at the end!\n",
    "local_download_path = r\"/content/drive/MyDrive/Madhukar/test_final/lores/srtm/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZVPulc6yW-g"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from osgeo import gdal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = [None] * 3\n",
    "\n",
    "# https://howtothink.readthedocs.io/en/latest/PvL_H.html\n",
    "for i in range(1,4):\n",
    "  ax[i-1] = fig.add_subplot(2, 2, i) \n",
    "\n",
    "count = 0\n",
    "for filename in os.listdir(local_download_path):\n",
    "    \n",
    "  if filename.endswith(\"tif\"): \n",
    "      print(filename)\n",
    "      try:\n",
    "        dataset = gdal.Open(local_download_path+filename, gdal.GA_ReadOnly) \n",
    "        # Note GetRasterBand() takes band no. starting from 1 not 0\n",
    "        band = dataset.GetRasterBand(1)\n",
    "        arr = band.ReadAsArray()\n",
    "        colors = [(1, 0, 0), (0, 1, 0), (0, 0, 1)]  # R -> G -> B correct form\n",
    "        # colors = [(0, 0, 1), (0, 1, 0), (1, 0, 0)]  # B -> G -> R\n",
    "        n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "        cmap_name = 'my_list'\n",
    "        cm = LinearSegmentedColormap.from_list(cmap_name, colors)\n",
    "\n",
    "        ax[count].imshow(arr, cmap=cm)\n",
    "        ax[count].set_title(filename.split(\"_\")[-2])\n",
    "        # plt.imshow(arr)\n",
    "        # dataset.GetGeoTransform()\n",
    "        print(\"({},{})\".format(dataset.GetRasterBand(1).XSize, dataset.GetRasterBand(1).YSize))\n",
    "      except Exception as e:\n",
    "        print(e)\n",
    "  count += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-TjA3PuBurs"
   },
   "source": [
    "# NHDPlus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPc0WsleS7vA"
   },
   "source": [
    "## mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUnoHOPtS4Iz"
   },
   "outputs": [],
   "source": [
    "# np18w = ee.FeatureCollection(\"users/madhukarreddy/NHDPlus18_Waterbodies\")#.filterBounds(square(41.638, -122.0048, 3000))\n",
    "# np18f = ee.FeatureCollection(\"users/madhukarreddy/NHDPlus18_Flowlines\")#.filterBounds(square(41.638, -122.0048, 3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeoRx8gGWQEZ"
   },
   "outputs": [],
   "source": [
    "# # this code does not work in its scaled up form\n",
    "# merged_wb = ee.FeatureCollection(\"users/madhukarreddy/NHDPlus01_Waterbodies\")\n",
    "# for i in range(1,18):\n",
    "#   if i < 9:\n",
    "#     num = \"0\" + str(i+1)\n",
    "#   else:\n",
    "#     num = str(i+1)\n",
    "#   if i == 2:\n",
    "#     for direction in ['N', 'S', 'W']:\n",
    "#       print(\"users/madhukarreddy/NHDPlus\" + num + direction + \"_Waterbodies\")\n",
    "#       merged_wb = merged_wb.merge(ee.FeatureCollection(\"users/madhukarreddy/NHDPlus\" + num + direction + \"_Waterbodies\"))\n",
    "#   elif i == 9:\n",
    "#     for direction in ['L', 'U']:\n",
    "#       print(\"users/madhukarreddy/NHDPlus\" + num + direction + \"_Waterbodies\")\n",
    "#       merged_wb = merged_wb.merge(ee.FeatureCollection(\"users/madhukarreddy/NHDPlus\" + num + direction + \"_Waterbodies\"))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UuN_jp02af17"
   },
   "outputs": [],
   "source": [
    "# for i in range(18,18):\n",
    "#   if i < 9:\n",
    "#     num = \"0\" + str(i+1)\n",
    "#   else:\n",
    "#     num = str(i+1)\n",
    "#   if i == 2:\n",
    "#     for direction in ['N', 'S', 'W']:\n",
    "#       print(\"users/madhukarreddy/NHDPlus\" + num + direction + \"_Waterbodies\")\n",
    "#       merged_wb = merged_wb.merge(ee.FeatureCollection(\"users/madhukarreddy/NHDPlus\" + num + direction + \"_Waterbodies\"))\n",
    "#   elif i == 9:\n",
    "#     for direction in ['L', 'U']:\n",
    "#       print(\"users/madhukarreddy/NHDPlus\" + num + direction + \"_Waterbodies\")\n",
    "#       merged_wb = merged_wb.merge(ee.FeatureCollection(\"users/madhukarreddy/NHDPlus\" + num + direction + \"_Waterbodies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQcFBF2RbSPm"
   },
   "outputs": [],
   "source": [
    "# def fc_wb(nhd_num):\n",
    "#   merged_wb = ee.FeatureCollection([None])\n",
    "#   if nhd_num < 9:\n",
    "#     num = \"0\" + str(i+1)\n",
    "#   else:\n",
    "#     num = str(i+1)\n",
    "#   if nhd_num == 2:\n",
    "#     for direction in ['N', 'S', 'W']:\n",
    "#       print(\"users/madhukarreddy/NHDPlus\" + num + direction + \"_Waterbodies\")\n",
    "#       merged_wb = merged_wb.merge(ee.FeatureCollection(\"users/madhukarreddy/NHDPlus\" + num + direction + \"_Waterbodies\"))\n",
    "#   elif i == 9:\n",
    "#     for direction in ['L', 'U']:\n",
    "#       print(\"users/madhukarreddy/NHDPlus\" + num + direction + \"_Waterbodies\")\n",
    "#       merged_wb = merged_wb.merge(ee.FeatureCollection(\"users/madhukarreddy/NHDPlus\" + num + direction + \"_Waterbodies\"))\n",
    "#   else:\n",
    "#     merged_wb = merged_wb.merge(ee.FeatureCollection(\"users/madhukarreddy/NHDPlus\" + num + \"_Waterbodies\"))\n",
    "#   return merged_wb\n",
    "\n",
    "# def fc_wb2(num):\n",
    "#   merged_wb = ee.FeatureCollection([None])\n",
    "#   # return ee.FeatureCollection(\"users/madhukarreddy/NHDPlus\" + str(num) + \"_Waterbodies\")\n",
    "#   return merged_wb.merge(ee.FeatureCollection(\"users/madhukarreddy/NHDPlus\" + str(num) + \"_Waterbodies\"))\n",
    "\n",
    "# len(fc_wb(18).filterBounds(square(41.638, -122.0048, 3000)).getInfo().get('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5HrUlzVe5wR"
   },
   "outputs": [],
   "source": [
    "def fc(nhd_folder_num, feature): # fc for feature collection\n",
    "  \"\"\"\n",
    "\n",
    "  Function calls the individual GEE asset correspondiing to shapefiles in \n",
    "  subfolder of NHDPlus V2 dataset\n",
    "\n",
    "  nhd_num: number indicating which subfolder in the NHDPlus dataset\n",
    "  \"\"\"\n",
    "\n",
    "  # start with empty feature collection\n",
    "  merged = ee.FeatureCollection([None])\n",
    "\n",
    "  # convert '1' into '01' etc\n",
    "  if nhd_folder_num < 10:\n",
    "    num = \"0\" + str(nhd_folder_num)\n",
    "  else:\n",
    "    num = str(nhd_folder_num)\n",
    "\n",
    "  # add suffix corresponding to how the subfolders were named\n",
    "  if nhd_folder_num == 3:\n",
    "    for direction in ['N', 'S', 'W']:\n",
    "      merged = merged.merge((ee.FeatureCollection(\"users/madhukarreddy/NHDPlus\" \n",
    "                                                  + num + direction \n",
    "                                                  + \"_\" + feature)))\n",
    "  elif nhd_folder_num == 10:\n",
    "    for direction in ['L', 'U']:\n",
    "      merged = merged.merge((ee.FeatureCollection(\"users/madhukarreddy/NHDPlus\" \n",
    "                                                  + num + direction \n",
    "                                                  + \"_\" + feature)))\n",
    "  else:\n",
    "    merged = merged.merge((ee.FeatureCollection(\"users/madhukarreddy/NHDPlus\" \n",
    "                                                + num + \"_\" \n",
    "                                                + feature)))\n",
    "  return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvBSoW1i-Yr8"
   },
   "outputs": [],
   "source": [
    "def merge_fc(feature):\n",
    "  \"\"\"\n",
    "\n",
    "  Joins all the shapefiles across the US\n",
    "\n",
    "  feature: \"Waterbodies\" or \"Flowlines\"\n",
    "  \"\"\"\n",
    "  merged_fc_ = ee.FeatureCollection([None])\n",
    "  for i in range(1,19):\n",
    "    merged_fc_ = merged_fc_.merge(fc(i, feature))\n",
    "  return merged_fc_\n",
    "\n",
    "fc_wb = merge_fc(\"Waterbodies\")\n",
    "fc_fl = merge_fc(\"Flowlines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgRrwWVVS9qJ"
   },
   "source": [
    "## NHD parameter retrieval - trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "6QAst5rRk2hD",
    "outputId": "60b33ee1-fc6a-48ea-eb44-29b69b95f376"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-040ba097acc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnp18w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatureCollection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"users/madhukarreddy/NHDPlus18_Waterbodies\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterBounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m41.638\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m122.0048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnp18f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatureCollection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"users/madhukarreddy/NHDPlus18_Flowlines\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterBounds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m41.638\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m122.0048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnhd_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/GeoSpatialData/NHD/nhd_stats_AI.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# nhd_stats.dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \"\"\"\n\u001b[1;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# np18w = ee.FeatureCollection(\"users/madhukarreddy/NHDPlus18_Waterbodies\").filterBounds(square(41.638, -122.0048, 3000))\n",
    "# np18f = ee.FeatureCollection(\"users/madhukarreddy/NHDPlus18_Flowlines\").filterBounds(square(41.638, -122.0048, 3000))\n",
    "# nhd_stats = pd.read_csv(\"/content/drive/MyDrive/GeoSpatialData/NHD/nhd_stats_AI.csv\")\n",
    "# # nhd_stats.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7jTfJeq6aSo7"
   },
   "outputs": [],
   "source": [
    "# # number of water bodies and flowlines(COMID's) present within the bounding box\n",
    "# wb_count = len(np18w.getInfo().get('features'))\n",
    "# fl_count = len(np18f.getInfo().get('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9NLKpczHzhH"
   },
   "outputs": [],
   "source": [
    "# # this retrives the properties of each waterbody\n",
    "# np18w.getInfo().get('features')[2].get('properties')\n",
    "# # imp properties: COMID, FCODE, FTYPE, GNIS_NAME, AREASQKM\n",
    "# # total_area = sum of all comids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEVgNaYXH9q_"
   },
   "outputs": [],
   "source": [
    "# nhd_stats[nhd_stats.comid == 8265274]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFMy6ddzHxyy"
   },
   "outputs": [],
   "source": [
    "# flowlines\n",
    "# np18f.getInfo().get('features')[0].get('properties')\n",
    "# imp properties: COMID, FCODE, FTYPE, GNIS_NAME, LENGTHKM\n",
    "# total_len = sum of all comids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Og9_MSS0HT5D"
   },
   "outputs": [],
   "source": [
    "# nhd_stats[nhd_stats.comid == 948010065]\n",
    "# key parameters in nhd_stats: startflag, intephem, lengthkm (same), gnis_name_ind (OHE), areasqkm, totdasqkm, flow_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hC-WJkMN24L"
   },
   "outputs": [],
   "source": [
    "# nhd_stats.gnis_name_ind.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJ25HijxChrp"
   },
   "source": [
    "# NHD parameter retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drKWGsUbPeK4"
   },
   "outputs": [],
   "source": [
    "# waterbodies\n",
    "# key imp properties: COMID, FTYPE, GNIS_NAME, AREASQKM\n",
    "\n",
    "# How do you combine these?:\n",
    "# FTYPE: join the strings for later usage (OHE)\n",
    "# GNIS_NAME: sum all of the OHE name present vs absent\n",
    "# AREASQKM: sum of all\n",
    "\n",
    "# Given a lat, lon, find the above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2wyb_ZyGTkS"
   },
   "outputs": [],
   "source": [
    "def find_ftype(feature=\"Waterbodies\", lat=41.638, lon=-122.0048, size=3000):\n",
    "  \"\"\"\n",
    "\n",
    "  For a given lat, lon, find all te ftypes and join them into one long string\n",
    "  \"\"\"\n",
    "\n",
    "  ftype_str = \"\"\n",
    "  try:\n",
    "    if feature == \"Waterbodies\":\n",
    "      fc = fc_wb.filterBounds(square(lat, lon, size))\n",
    "    else:\n",
    "      fc = fc_fl.filterBounds(square(lat, lon, size))\n",
    "      \n",
    "    num_of_features = len(fc.getInfo().get('features'))  \n",
    "      \n",
    "    for feat in range(num_of_features):\n",
    "      ftype_str += fc.getInfo().get('features')[feat]\\\n",
    "                               .get('properties')\\\n",
    "                               .get('FTYPE') + \"+\"\n",
    "    return ftype_str\n",
    "  except:\n",
    "    print(\"Issue with {0} at lat={1}, lon={2}\".format(feature, lat, lon))\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzQBDwj6JQPx"
   },
   "outputs": [],
   "source": [
    "def gnis_name_ind(feature=\"Waterbodies\", lat=41.6575, lon=-122.1802, size=3000):\n",
    "  gnis_count = 0\n",
    "  try:\n",
    "    if feature == \"Waterbodies\":\n",
    "      fc = fc_wb.filterBounds(square(lat, lon, size))\n",
    "    else:\n",
    "      fc = fc_fl.filterBounds(square(lat, lon, size))\n",
    "      \n",
    "    num_of_features = len(fc.getInfo().get('features'))  \n",
    "      \n",
    "    for feat in range(num_of_features):\n",
    "      gnis_count += len(fc.getInfo().get('features')[feat]\\\n",
    "                               .get('properties')\\\n",
    "                               .get('GNIS_ID')) > 0\n",
    "    return gnis_count\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Issue with {0} at lat={1}, lon={2}\".format(feature, lat, lon))\n",
    "    return np.nan  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f4wDkKaMZ0G"
   },
   "outputs": [],
   "source": [
    "def total_wb_area(feature=\"Waterbodies\", lat=41.6575, lon=-122.1802, size=3000):\n",
    "  \"\"\"\n",
    "\n",
    "  Return only Waterbody area as Flowlines do not have area in GEE\n",
    "  \"\"\"\n",
    "\n",
    "  wb_area = 0\n",
    "  try:\n",
    "    if feature == \"Waterbodies\":\n",
    "      fc = fc_wb.filterBounds(square(lat, lon, size))\n",
    "    else:\n",
    "      print(\"Available only for waterbodies\")\n",
    "      \n",
    "    num_of_features = len(fc.getInfo().get('features'))  \n",
    "      \n",
    "    for feat in range(num_of_features):\n",
    "      wb_area += fc.getInfo().get('features')[feat]\\\n",
    "                               .get('properties')\\\n",
    "                               .get('AREASQKM')\n",
    "    return wb_area\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Issue with {0} at lat={1}, lon={2}\".format(feature, lat, lon))\n",
    "    return np.nan  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tv-d1L3MO2V5"
   },
   "outputs": [],
   "source": [
    "# return multiple fields\n",
    "def nhd_vars(feature=\"Waterbodies\", lat=41.638, lon=-122.0048, size=1000):\n",
    "  \"\"\"\n",
    "\n",
    "  For a given lat, lon, return multiple variables\n",
    "  \"\"\"\n",
    "  \n",
    "  comid_list = []\n",
    "  ftype_str = \"\"\n",
    "  gnis_count = 0\n",
    "  wb_area = 0\n",
    "  fl_length = 0\n",
    "\n",
    "  try:\n",
    "    if feature == \"Waterbodies\":\n",
    "      fc = fc_wb.filterBounds(square(lat, lon, size))\n",
    "    else:\n",
    "      fc = fc_fl.filterBounds(square(lat, lon, size))\n",
    "      \n",
    "    num_of_features = len(fc.getInfo().get('features'))  \n",
    "      \n",
    "    for feat in range(num_of_features):\n",
    "\n",
    "      comid_list.append(fc.getInfo().get('features')[feat]\\\n",
    "                               .get('properties')\\\n",
    "                               .get('COMID'))\n",
    "\n",
    "      ftype_str += fc.getInfo().get('features')[feat]\\\n",
    "                               .get('properties')\\\n",
    "                               .get('FTYPE') + \"+\"\n",
    "\n",
    "      gnis_count += len(fc.getInfo().get('features')[feat]\\\n",
    "                               .get('properties')\\\n",
    "                               .get('GNIS_ID')) > 0\n",
    "\n",
    "      if feature == \"Waterbodies\":\n",
    "        wb_area += fc.getInfo().get('features')[feat]\\\n",
    "                               .get('properties')\\\n",
    "                               .get('AREASQKM')\n",
    "      else:\n",
    "        wb_area = np.nan                                     \n",
    "      \n",
    "      if feature == \"Flowlines\":\n",
    "        fl_length += fc.getInfo().get('features')[feat]\\\n",
    "                               .get('properties')\\\n",
    "                               .get('LENGTHKM')\n",
    "      else:\n",
    "        fl_length = np.nan\n",
    "\n",
    "    return comid_list, ftype_str, gnis_count, wb_area, fl_length\n",
    "  except:\n",
    "    print(\"Issue with {0} at lat={1}, lon={2}\".format(feature, lat, lon))\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FbXPXHt8KV3e"
   },
   "outputs": [],
   "source": [
    "# (fc_wb.filterBounds(square(41.6483, -122.1741, 3000)).getInfo().get('features')[0].get('properties').get(\"AREASQKM\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDTz6zAM-C0N",
    "outputId": "d5ae4760-e5e8-4985-fbf0-a31004547c09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], '', 0, 0, 0)"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find_ftype(feature=\"Flowlines\", lat=np.nan, lon=np.nan)\n",
    "nhd_vars(feature=\"Waterbodies\")#, lat=np.nan, lon=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bcRIItG4bu7"
   },
   "outputs": [],
   "source": [
    "# read in the nhd addendum file\n",
    "nhd_stats = pd.read_csv(\"/content/drive/MyDrive/GeoSpatialData/NHD/nhd_stats_AI.csv\")\n",
    "\n",
    "# read in csv file with SSURGO variables\n",
    "df_m = pd.read_csv(\"/content/drive/MyDrive/Data/combined_regular_clean_with_ssurgo_variables.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wwUp3-I42f1"
   },
   "outputs": [],
   "source": [
    "def find_percent_hydric3(mukey):\n",
    "  if mukey != np.nan:\n",
    "    hydclprs = df_s[df_s.mukey == mukey][\"hydclprs\"]\n",
    "    # hydclprs = df_s.loc[df_s.mukey == mukey,\"hydclprs\"]\n",
    "  else:\n",
    "    return np.nan\n",
    "    # return np.array(np.nan)\n",
    "  return hydclprs.mean()\n",
    "  # return hydclprs.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyUX9Hp51CVR"
   },
   "outputs": [],
   "source": [
    "# for i in range(df.shape[0]):\n",
    "#   lat = df.iloc[i].get(\"latitude\")\n",
    "#   lon = df.iloc[i].get(\"longitude\")\n",
    "#   size = 1000\n",
    "#   comid_list, ftype_str, gnis_count, wb_area, fl_length = nhd_vars(feature=\"Waterbodies\", lat, lon, size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxe3xJLh4RNL",
    "outputId": "680f8844-080c-48fc-f194-9045519528c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue with Waterbodies at lat=34.257259999999995, lon=-86.21438000000002\n",
      "Issue with Waterbodies at lat=32.31653, lon=-90.22429\n"
     ]
    }
   ],
   "source": [
    "# df_mukey[\"hydclprs\"] = df_mukey.apply(lambda x: find_percent_hydric3(x.mukey), axis=1)\n",
    "df_m_ = df_m#.head(2).copy()\n",
    "\n",
    "df_m_[\"nhd_vars_wb\"] = df_m_.apply(lambda x: nhd_vars(feature=\"Waterbodies\", lat=x.latitude, lon=x.longitude), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqCa0mfY7YOK"
   },
   "outputs": [],
   "source": [
    "df_m_[\"nhd_vars_fl\"] = df_m_.apply(lambda x: nhd_vars(feature=\"Flowlines\", lat=x.latitude, lon=x.longitude), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qJlJOh27hLx"
   },
   "outputs": [],
   "source": [
    "# parse out the COMID's from nhd_vars_wb and nhd_vars_fl columns and find populate them across columns\n",
    "# comid_list, ftype_str, gnis_count, wb_area, fl_length\n",
    "df_m_[\"wb_comids\"] = df_m_.apply(lambda x: \" \".join([str(i) for i in x.nhd_vars_wb[0]]), axis=1)\n",
    "df_m_[\"wb_ftype\"] = df_m_.apply(lambda x: x.nhd_vars_wb[1], axis=1)\n",
    "df_m_[\"wb_gnis_count\"] = df_m_.apply(lambda x: x.nhd_vars_wb[2], axis=1)\n",
    "df_m_[\"wb_area\"] = df_m_.apply(lambda x: x.nhd_vars_wb[3], axis=1)\n",
    "\n",
    "df_m_[\"fl_comids\"] = df_m_.apply(lambda x: \" \".join([str(i) for i in x.nhd_vars_fl[0]]), axis=1)\n",
    "df_m_[\"fl_ftype\"] = df_m_.apply(lambda x: x.nhd_vars_fl[1], axis=1)\n",
    "df_m_[\"fl_gnis_count\"] = df_m_.apply(lambda x: x.nhd_vars_fl[2], axis=1)\n",
    "df_m_[\"fl_length\"] = df_m_.apply(lambda x: x.nhd_vars_fl[4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQ4jP2P-BgM4"
   },
   "outputs": [],
   "source": [
    "# read in fl_comid_list, pull out comids and match with nhd_stats to sum \n",
    "# areasqkm, todasqkm and flow_type\n",
    "\n",
    "df_m_[\"fl_comid_list\"] = df_m_.apply(lambda x: x.nhd_vars_fl[0], axis=1)\n",
    "\n",
    "\n",
    "def fl_areasqkm(comid):\n",
    "  return nhd_stats[nhd_stats[\"comid\"] == comid][\"areasqkm\"]\n",
    "\n",
    "def fl_totdasqkm(comid):\n",
    "  return nhd_stats[nhd_stats[\"comid\"] == comid][\"totdasqkm\"]\n",
    "\n",
    "def fl_flow_type(comid):\n",
    "  return nhd_stats[nhd_stats[\"comid\"] == comid][\"flow_type\"]\n",
    "\n",
    "df_m_[\"fl_areasqkm\"] = df_m_.apply(lambda x: np.sum([fl_areasqkm(comid) for comid in x.fl_comid_list]), axis=1)\n",
    "df_m_[\"fl_totdasqkm\"] = df_m_.apply(lambda x: np.sum([fl_totdasqkm(comid) for comid in x.fl_comid_list]), axis=1)\n",
    "df_m_[\"fl_flow_type\"] = df_m_.apply(lambda x: np.sum([fl_flow_type(comid) for comid in x.fl_comid_list]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJm1vgMbKJis"
   },
   "outputs": [],
   "source": [
    "# saving the dataframe \n",
    "df_m_.to_csv('/content/drive/MyDrive/Data/combined_regular_clean_with_ssurgo_nhd_variables.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wyr-kwm5PDnq"
   },
   "outputs": [],
   "source": [
    "#read the file\n",
    "df_m_test = pd.read_csv('/content/drive/MyDrive/Data/combined_regular_clean_with_ssurgo_nhd_variables.csv')\n",
    "df_m_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTyxq3-7J37F"
   },
   "outputs": [],
   "source": [
    "# flowlines\n",
    "# imp properties: COMID, FCODE, FTYPE, GNIS_NAME, LENGTHKM\n",
    "# total_len = sum of all LENGTHKM\n",
    "# key parameters in nhd_stats: startflag, intephem, lengthkm (same), gnis_name_ind (OHE), areasqkm, totdasqkm, flow_type\n",
    "# final key params:\n",
    "# COMID, FTYPE, gnis_name_ind (0,1), LENGTHKM, areasqkm, totdasqkm, flow_type (1,0)\n",
    "\n",
    "# How do you combine these?: \n",
    "# FTYPE: join the strings for later usage (OHE)\n",
    "# gnis_name_ind: sum of all\n",
    "# LENGTHKM: sum of all \n",
    "# areasqkm: sum of all\n",
    "# todasqkm: sum of all\n",
    "# flow_type: sum of all?\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2021.03.06_SSURGO extraction.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
